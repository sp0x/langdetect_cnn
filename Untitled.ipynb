{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:\n",
      "     checkpoint_step 200\n",
      "          vocab_size 4090\n",
      "           train_dir 'models'\n",
      "          min_window 3\n",
      "      tolerance_step 500\n",
      "            sent_len 257\n",
      "        use_pretrain False\n",
      "        summary_step 200\n",
      "            emb_size 300\n",
      "           num_epoch 50\n",
      "            log_step 10\n",
      "            lr_decay 0.95\n",
      "          num_kernel 100\n",
      "           optimizer 'adam'\n",
      "          batch_size 100\n",
      "          max_window 5\n",
      "            data_dir '/home/v_g_vasilev94/cnn/langdetect_cnn/data/ted500'\n",
      "              l2_reg 1e-05\n",
      "             dropout 0.5\n",
      "             init_lr 0.01\n",
      "Preparing train data ...\n",
      "Loaded target classes (length 65).\n",
      "Loaded data with 29250 examples. 100 examples per batch will be used.\n",
      "Preparing test data ...\n",
      "Loaded target classes (length 65).\n",
      "Loaded data with 3250 examples. 100 examples per batch will be used.\n",
      "WARNING:tensorflow:From /home/v_g_vasilev94/cnn/langdetect_cnn/cnn.py:115: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "\n",
      "Start training, 293 batches needed, with 100 examples per batch.\n",
      "2018-11-29 14:27:13.097702: step 10/14650 (epoch 1/50), acc = 0.29, loss = 3.51 (827.1 examples/sec; 0.121 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:27:14.541273: step 20/14650 (epoch 1/50), acc = 0.45, loss = 2.14 (797.2 examples/sec; 0.125 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:27:16.011723: step 30/14650 (epoch 1/50), acc = 0.57, loss = 1.55 (821.2 examples/sec; 0.122 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:27:17.478430: step 40/14650 (epoch 1/50), acc = 0.57, loss = 1.44 (812.4 examples/sec; 0.123 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:27:18.974194: step 50/14650 (epoch 1/50), acc = 0.65, loss = 1.31 (816.3 examples/sec; 0.123 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:27:20.483265: step 60/14650 (epoch 1/50), acc = 0.64, loss = 1.08 (822.3 examples/sec; 0.122 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:27:21.991926: step 70/14650 (epoch 1/50), acc = 0.68, loss = 1.16 (820.9 examples/sec; 0.122 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:27:23.509675: step 80/14650 (epoch 1/50), acc = 0.76, loss = 0.87 (813.3 examples/sec; 0.123 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:27:25.052763: step 90/14650 (epoch 1/50), acc = 0.77, loss = 0.77 (820.8 examples/sec; 0.122 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:27:26.588923: step 100/14650 (epoch 1/50), acc = 0.70, loss = 0.95 (818.1 examples/sec; 0.122 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:27:28.128280: step 110/14650 (epoch 1/50), acc = 0.77, loss = 0.87 (816.6 examples/sec; 0.122 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:27:29.698043: step 120/14650 (epoch 1/50), acc = 0.78, loss = 0.74 (806.6 examples/sec; 0.124 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:27:31.267760: step 130/14650 (epoch 1/50), acc = 0.73, loss = 1.11 (814.6 examples/sec; 0.123 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:27:32.841711: step 140/14650 (epoch 1/50), acc = 0.71, loss = 1.19 (810.7 examples/sec; 0.123 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:27:34.411518: step 150/14650 (epoch 1/50), acc = 0.75, loss = 1.11 (812.7 examples/sec; 0.123 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:27:36.011351: step 160/14650 (epoch 1/50), acc = 0.74, loss = 0.77 (817.8 examples/sec; 0.122 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:27:37.623136: step 170/14650 (epoch 1/50), acc = 0.65, loss = 1.48 (808.3 examples/sec; 0.124 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:27:39.256531: step 180/14650 (epoch 1/50), acc = 0.75, loss = 0.81 (812.2 examples/sec; 0.123 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:27:40.879868: step 190/14650 (epoch 1/50), acc = 0.67, loss = 1.20 (810.4 examples/sec; 0.123 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:27:42.508405: step 200/14650 (epoch 1/50), acc = 0.74, loss = 1.09 (810.8 examples/sec; 0.123 sec/batch), lr: 0.010000\n",
      "\n",
      "Step 200: train_loss = 1.433868, train_accuracy = 0.645\n",
      "Step 200:  test_loss = 0.607899,  test_accuracy = 0.804\n",
      "\n",
      "2018-11-29 14:27:45.472770: step 210/14650 (epoch 1/50), acc = 0.71, loss = 1.29 (811.1 examples/sec; 0.123 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:27:47.105155: step 220/14650 (epoch 1/50), acc = 0.72, loss = 1.18 (803.2 examples/sec; 0.125 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:27:48.808305: step 230/14650 (epoch 1/50), acc = 0.67, loss = 1.07 (811.7 examples/sec; 0.123 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:27:50.445114: step 240/14650 (epoch 1/50), acc = 0.73, loss = 1.01 (811.6 examples/sec; 0.123 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:27:52.111579: step 250/14650 (epoch 1/50), acc = 0.63, loss = 1.72 (811.9 examples/sec; 0.123 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:27:53.787987: step 260/14650 (epoch 1/50), acc = 0.70, loss = 1.16 (815.4 examples/sec; 0.123 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:27:55.449971: step 270/14650 (epoch 1/50), acc = 0.77, loss = 0.77 (816.0 examples/sec; 0.123 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:27:57.111780: step 280/14650 (epoch 1/50), acc = 0.72, loss = 0.76 (818.3 examples/sec; 0.122 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:27:58.798331: step 290/14650 (epoch 1/50), acc = 0.79, loss = 0.79 (808.5 examples/sec; 0.124 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:28:00.642277: step 300/14650 (epoch 2/50), acc = 0.79, loss = 1.02 (812.5 examples/sec; 0.123 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:28:02.358358: step 310/14650 (epoch 2/50), acc = 0.69, loss = 0.92 (808.8 examples/sec; 0.124 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:28:04.069380: step 320/14650 (epoch 2/50), acc = 0.73, loss = 0.96 (811.9 examples/sec; 0.123 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:28:05.809809: step 330/14650 (epoch 2/50), acc = 0.75, loss = 1.38 (813.5 examples/sec; 0.123 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:28:07.561050: step 340/14650 (epoch 2/50), acc = 0.81, loss = 0.71 (801.0 examples/sec; 0.125 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:28:09.392641: step 350/14650 (epoch 2/50), acc = 0.70, loss = 0.93 (802.7 examples/sec; 0.125 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:28:11.204429: step 360/14650 (epoch 2/50), acc = 0.81, loss = 0.79 (809.0 examples/sec; 0.124 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:28:13.013065: step 370/14650 (epoch 2/50), acc = 0.84, loss = 0.82 (807.0 examples/sec; 0.124 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:28:14.848738: step 380/14650 (epoch 2/50), acc = 0.70, loss = 1.06 (813.9 examples/sec; 0.123 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:28:16.675098: step 390/14650 (epoch 2/50), acc = 0.75, loss = 0.76 (782.4 examples/sec; 0.128 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:28:18.559137: step 400/14650 (epoch 2/50), acc = 0.79, loss = 1.12 (797.3 examples/sec; 0.125 sec/batch), lr: 0.010000\n",
      "\n",
      "Step 400: train_loss = 0.920866, train_accuracy = 0.762\n",
      "Step 400:  test_loss = 0.593692,  test_accuracy = 0.828\n",
      "\n",
      "2018-11-29 14:28:21.673131: step 410/14650 (epoch 2/50), acc = 0.72, loss = 0.99 (795.5 examples/sec; 0.126 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:28:23.592131: step 420/14650 (epoch 2/50), acc = 0.76, loss = 0.72 (777.0 examples/sec; 0.129 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:28:25.514001: step 430/14650 (epoch 2/50), acc = 0.70, loss = 0.96 (782.9 examples/sec; 0.128 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:28:27.495207: step 440/14650 (epoch 2/50), acc = 0.82, loss = 0.61 (768.1 examples/sec; 0.130 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:28:29.508005: step 450/14650 (epoch 2/50), acc = 0.73, loss = 0.90 (776.5 examples/sec; 0.129 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:28:31.447924: step 460/14650 (epoch 2/50), acc = 0.88, loss = 0.53 (767.2 examples/sec; 0.130 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:28:33.445654: step 470/14650 (epoch 2/50), acc = 0.76, loss = 0.95 (765.5 examples/sec; 0.131 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:28:35.562557: step 480/14650 (epoch 2/50), acc = 0.81, loss = 0.81 (748.7 examples/sec; 0.134 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:28:37.630530: step 490/14650 (epoch 2/50), acc = 0.71, loss = 1.05 (737.9 examples/sec; 0.136 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:28:39.721826: step 500/14650 (epoch 2/50), acc = 0.77, loss = 0.98 (728.6 examples/sec; 0.137 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:28:41.881642: step 510/14650 (epoch 2/50), acc = 0.82, loss = 0.53 (727.7 examples/sec; 0.137 sec/batch), lr: 0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-29 14:28:43.990518: step 520/14650 (epoch 2/50), acc = 0.75, loss = 1.00 (726.0 examples/sec; 0.138 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:28:46.055069: step 530/14650 (epoch 2/50), acc = 0.75, loss = 1.04 (721.5 examples/sec; 0.139 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:28:48.227152: step 540/14650 (epoch 2/50), acc = 0.76, loss = 0.95 (710.8 examples/sec; 0.141 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:28:50.360514: step 550/14650 (epoch 2/50), acc = 0.82, loss = 0.78 (709.2 examples/sec; 0.141 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:28:52.635151: step 560/14650 (epoch 2/50), acc = 0.70, loss = 1.25 (696.3 examples/sec; 0.144 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:28:54.864579: step 570/14650 (epoch 2/50), acc = 0.84, loss = 0.60 (698.3 examples/sec; 0.143 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:28:57.138898: step 580/14650 (epoch 2/50), acc = 0.78, loss = 0.79 (688.3 examples/sec; 0.145 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:28:59.429182: step 590/14650 (epoch 3/50), acc = 0.80, loss = 0.67 (694.0 examples/sec; 0.144 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:29:01.753094: step 600/14650 (epoch 3/50), acc = 0.83, loss = 0.73 (678.8 examples/sec; 0.147 sec/batch), lr: 0.010000\n",
      "\n",
      "Step 600: train_loss = 0.898400, train_accuracy = 0.774\n",
      "Step 600:  test_loss = 0.662406,  test_accuracy = 0.818\n",
      "\n",
      "2018-11-29 14:29:05.245752: step 610/14650 (epoch 3/50), acc = 0.83, loss = 0.81 (675.8 examples/sec; 0.148 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:29:07.629171: step 620/14650 (epoch 3/50), acc = 0.71, loss = 1.01 (675.0 examples/sec; 0.148 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:29:09.943415: step 630/14650 (epoch 3/50), acc = 0.77, loss = 1.01 (677.2 examples/sec; 0.148 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:29:12.264328: step 640/14650 (epoch 3/50), acc = 0.85, loss = 0.72 (655.3 examples/sec; 0.153 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:29:14.674551: step 650/14650 (epoch 3/50), acc = 0.77, loss = 1.07 (656.6 examples/sec; 0.152 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:29:17.032775: step 660/14650 (epoch 3/50), acc = 0.83, loss = 0.58 (640.1 examples/sec; 0.156 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:29:19.447291: step 670/14650 (epoch 3/50), acc = 0.76, loss = 1.00 (647.0 examples/sec; 0.155 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:29:21.861593: step 680/14650 (epoch 3/50), acc = 0.82, loss = 0.73 (639.1 examples/sec; 0.156 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:29:24.322170: step 690/14650 (epoch 3/50), acc = 0.76, loss = 1.01 (640.0 examples/sec; 0.156 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:29:26.790030: step 700/14650 (epoch 3/50), acc = 0.70, loss = 0.99 (633.4 examples/sec; 0.158 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:29:29.278047: step 710/14650 (epoch 3/50), acc = 0.87, loss = 0.82 (629.3 examples/sec; 0.159 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:29:31.854639: step 720/14650 (epoch 3/50), acc = 0.86, loss = 0.55 (621.1 examples/sec; 0.161 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:29:34.357184: step 730/14650 (epoch 3/50), acc = 0.70, loss = 1.32 (616.4 examples/sec; 0.162 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:29:36.951085: step 740/14650 (epoch 3/50), acc = 0.74, loss = 0.91 (625.9 examples/sec; 0.160 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:29:39.539318: step 750/14650 (epoch 3/50), acc = 0.82, loss = 0.85 (619.2 examples/sec; 0.161 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:29:42.152176: step 760/14650 (epoch 3/50), acc = 0.82, loss = 0.82 (609.1 examples/sec; 0.164 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:29:44.807356: step 770/14650 (epoch 3/50), acc = 0.77, loss = 1.03 (612.4 examples/sec; 0.163 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:29:47.462369: step 780/14650 (epoch 3/50), acc = 0.72, loss = 0.85 (606.1 examples/sec; 0.165 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:29:50.268549: step 790/14650 (epoch 3/50), acc = 0.79, loss = 1.13 (595.0 examples/sec; 0.168 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:29:53.085774: step 800/14650 (epoch 3/50), acc = 0.79, loss = 0.83 (595.8 examples/sec; 0.168 sec/batch), lr: 0.010000\n",
      "\n",
      "Step 800: train_loss = 0.950366, train_accuracy = 0.773\n",
      "Step 800:  test_loss = 0.679473,  test_accuracy = 0.824\n",
      "\n",
      "2018-11-29 14:29:57.229104: step 810/14650 (epoch 3/50), acc = 0.79, loss = 1.01 (585.8 examples/sec; 0.171 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:30:00.080608: step 820/14650 (epoch 3/50), acc = 0.80, loss = 0.68 (588.5 examples/sec; 0.170 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:30:03.171944: step 830/14650 (epoch 3/50), acc = 0.77, loss = 1.05 (577.2 examples/sec; 0.173 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:30:06.295613: step 840/14650 (epoch 3/50), acc = 0.70, loss = 1.20 (569.6 examples/sec; 0.176 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:30:09.411121: step 850/14650 (epoch 3/50), acc = 0.79, loss = 0.85 (571.8 examples/sec; 0.175 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:30:12.470869: step 860/14650 (epoch 3/50), acc = 0.81, loss = 0.78 (565.7 examples/sec; 0.177 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:30:15.499908: step 870/14650 (epoch 3/50), acc = 0.77, loss = 1.57 (560.6 examples/sec; 0.178 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:30:18.532433: step 880/14650 (epoch 4/50), acc = 0.80, loss = 0.69 (551.2 examples/sec; 0.181 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:30:21.569938: step 890/14650 (epoch 4/50), acc = 0.77, loss = 0.99 (561.6 examples/sec; 0.178 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:30:24.543651: step 900/14650 (epoch 4/50), acc = 0.77, loss = 0.80 (554.4 examples/sec; 0.180 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:30:27.616596: step 910/14650 (epoch 4/50), acc = 0.81, loss = 0.84 (559.6 examples/sec; 0.179 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:30:30.563028: step 920/14650 (epoch 4/50), acc = 0.71, loss = 1.07 (559.1 examples/sec; 0.179 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:30:33.625375: step 930/14650 (epoch 4/50), acc = 0.72, loss = 0.87 (547.1 examples/sec; 0.183 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:30:36.645899: step 940/14650 (epoch 4/50), acc = 0.74, loss = 1.12 (544.6 examples/sec; 0.184 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:30:39.808386: step 950/14650 (epoch 4/50), acc = 0.78, loss = 0.85 (539.9 examples/sec; 0.185 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:30:43.037734: step 960/14650 (epoch 4/50), acc = 0.68, loss = 1.35 (534.1 examples/sec; 0.187 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:30:46.273506: step 970/14650 (epoch 4/50), acc = 0.78, loss = 0.75 (530.3 examples/sec; 0.189 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:30:49.536440: step 980/14650 (epoch 4/50), acc = 0.85, loss = 0.66 (528.9 examples/sec; 0.189 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:30:52.865088: step 990/14650 (epoch 4/50), acc = 0.66, loss = 1.27 (527.2 examples/sec; 0.190 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:30:56.215207: step 1000/14650 (epoch 4/50), acc = 0.79, loss = 0.96 (522.7 examples/sec; 0.191 sec/batch), lr: 0.010000\n",
      "\n",
      "Step 1000: train_loss = 0.945888, train_accuracy = 0.773\n",
      "Step 1000:  test_loss = 0.684522,  test_accuracy = 0.830\n",
      "\n",
      "2018-11-29 14:31:00.870587: step 1010/14650 (epoch 4/50), acc = 0.85, loss = 0.56 (523.4 examples/sec; 0.191 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:31:04.152937: step 1020/14650 (epoch 4/50), acc = 0.79, loss = 1.09 (527.6 examples/sec; 0.190 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:31:07.450601: step 1030/14650 (epoch 4/50), acc = 0.83, loss = 0.82 (514.4 examples/sec; 0.194 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:31:10.863638: step 1040/14650 (epoch 4/50), acc = 0.80, loss = 0.70 (511.8 examples/sec; 0.195 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:31:14.167594: step 1050/14650 (epoch 4/50), acc = 0.76, loss = 1.04 (518.2 examples/sec; 0.193 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:31:17.690913: step 1060/14650 (epoch 4/50), acc = 0.74, loss = 1.06 (515.1 examples/sec; 0.194 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:31:21.291541: step 1070/14650 (epoch 4/50), acc = 0.79, loss = 0.82 (497.4 examples/sec; 0.201 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:31:24.983297: step 1080/14650 (epoch 4/50), acc = 0.72, loss = 1.07 (502.0 examples/sec; 0.199 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:31:28.658401: step 1090/14650 (epoch 4/50), acc = 0.74, loss = 1.20 (498.3 examples/sec; 0.201 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:31:32.249915: step 1100/14650 (epoch 4/50), acc = 0.85, loss = 0.71 (497.3 examples/sec; 0.201 sec/batch), lr: 0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-29 14:31:35.944097: step 1110/14650 (epoch 4/50), acc = 0.81, loss = 1.11 (503.0 examples/sec; 0.199 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:31:39.614617: step 1120/14650 (epoch 4/50), acc = 0.80, loss = 0.76 (496.0 examples/sec; 0.202 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:31:43.408018: step 1130/14650 (epoch 4/50), acc = 0.71, loss = 1.05 (482.6 examples/sec; 0.207 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:31:47.296339: step 1140/14650 (epoch 4/50), acc = 0.76, loss = 1.49 (417.1 examples/sec; 0.240 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:31:51.242879: step 1150/14650 (epoch 4/50), acc = 0.78, loss = 0.76 (478.3 examples/sec; 0.209 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:31:55.145654: step 1160/14650 (epoch 4/50), acc = 0.69, loss = 1.07 (405.9 examples/sec; 0.246 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:31:58.809453: step 1170/14650 (epoch 4/50), acc = 0.82, loss = 0.96 (484.1 examples/sec; 0.207 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:32:02.816561: step 1180/14650 (epoch 5/50), acc = 0.81, loss = 0.89 (478.1 examples/sec; 0.209 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:32:06.872679: step 1190/14650 (epoch 5/50), acc = 0.79, loss = 0.86 (470.9 examples/sec; 0.212 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:32:11.227041: step 1200/14650 (epoch 5/50), acc = 0.81, loss = 1.28 (477.8 examples/sec; 0.209 sec/batch), lr: 0.010000\n",
      "\n",
      "Step 1200: train_loss = 0.922783, train_accuracy = 0.788\n",
      "Step 1200:  test_loss = 0.718184,  test_accuracy = 0.828\n",
      "\n",
      "2018-11-29 14:32:17.176313: step 1210/14650 (epoch 5/50), acc = 0.78, loss = 1.03 (468.1 examples/sec; 0.214 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:32:21.278214: step 1220/14650 (epoch 5/50), acc = 0.78, loss = 0.91 (464.1 examples/sec; 0.215 sec/batch), lr: 0.010000\n",
      "2018-11-29 14:32:22.069256: step 1222/14650 (epoch 5/50), Learning rate decays to 0.00950\n",
      "2018-11-29 14:32:25.237216: step 1230/14650 (epoch 5/50), acc = 0.69, loss = 1.20 (469.4 examples/sec; 0.213 sec/batch), lr: 0.009500\n",
      "2018-11-29 14:32:29.422307: step 1240/14650 (epoch 5/50), acc = 0.71, loss = 1.28 (462.3 examples/sec; 0.216 sec/batch), lr: 0.009500\n",
      "2018-11-29 14:32:33.441751: step 1250/14650 (epoch 5/50), acc = 0.84, loss = 0.89 (468.2 examples/sec; 0.214 sec/batch), lr: 0.009500\n",
      "2018-11-29 14:32:37.682521: step 1260/14650 (epoch 5/50), acc = 0.87, loss = 0.75 (464.8 examples/sec; 0.215 sec/batch), lr: 0.009500\n",
      "2018-11-29 14:32:42.150919: step 1270/14650 (epoch 5/50), acc = 0.78, loss = 0.89 (451.3 examples/sec; 0.222 sec/batch), lr: 0.009500\n",
      "2018-11-29 14:32:46.402641: step 1280/14650 (epoch 5/50), acc = 0.74, loss = 1.02 (457.4 examples/sec; 0.219 sec/batch), lr: 0.009500\n",
      "2018-11-29 14:32:50.694504: step 1290/14650 (epoch 5/50), acc = 0.79, loss = 0.77 (450.8 examples/sec; 0.222 sec/batch), lr: 0.009500\n",
      "2018-11-29 14:32:55.050421: step 1300/14650 (epoch 5/50), acc = 0.82, loss = 0.75 (457.1 examples/sec; 0.219 sec/batch), lr: 0.009500\n",
      "2018-11-29 14:32:59.530581: step 1310/14650 (epoch 5/50), acc = 0.80, loss = 0.79 (443.5 examples/sec; 0.226 sec/batch), lr: 0.009500\n",
      "2018-11-29 14:33:03.923687: step 1320/14650 (epoch 5/50), acc = 0.72, loss = 1.14 (451.8 examples/sec; 0.221 sec/batch), lr: 0.009500\n",
      "2018-11-29 14:33:08.512780: step 1330/14650 (epoch 5/50), acc = 0.74, loss = 1.02 (446.3 examples/sec; 0.224 sec/batch), lr: 0.009500\n",
      "2018-11-29 14:33:13.046117: step 1340/14650 (epoch 5/50), acc = 0.76, loss = 1.33 (446.0 examples/sec; 0.224 sec/batch), lr: 0.009500\n",
      "2018-11-29 14:33:17.577839: step 1350/14650 (epoch 5/50), acc = 0.80, loss = 1.68 (438.3 examples/sec; 0.228 sec/batch), lr: 0.009500\n",
      "2018-11-29 14:33:22.175464: step 1360/14650 (epoch 5/50), acc = 0.82, loss = 0.82 (433.8 examples/sec; 0.231 sec/batch), lr: 0.009500\n",
      "2018-11-29 14:33:27.045243: step 1370/14650 (epoch 5/50), acc = 0.85, loss = 0.63 (439.6 examples/sec; 0.228 sec/batch), lr: 0.009500\n",
      "2018-11-29 14:33:31.584733: step 1380/14650 (epoch 5/50), acc = 0.80, loss = 0.95 (439.0 examples/sec; 0.228 sec/batch), lr: 0.009500\n",
      "2018-11-29 14:33:36.108179: step 1390/14650 (epoch 5/50), acc = 0.77, loss = 0.88 (431.4 examples/sec; 0.232 sec/batch), lr: 0.009500\n",
      "2018-11-29 14:33:40.747883: step 1400/14650 (epoch 5/50), acc = 0.80, loss = 1.19 (433.6 examples/sec; 0.231 sec/batch), lr: 0.009500\n",
      "\n",
      "Step 1400: train_loss = 0.949152, train_accuracy = 0.778\n",
      "Step 1400:  test_loss = 0.727171,  test_accuracy = 0.821\n",
      "\n",
      "2018-11-29 14:33:46.970119: step 1410/14650 (epoch 5/50), acc = 0.85, loss = 0.69 (433.5 examples/sec; 0.231 sec/batch), lr: 0.009500\n",
      "2018-11-29 14:33:51.770776: step 1420/14650 (epoch 5/50), acc = 0.79, loss = 0.77 (423.6 examples/sec; 0.236 sec/batch), lr: 0.009500\n",
      "2018-11-29 14:33:56.876354: step 1430/14650 (epoch 5/50), acc = 0.80, loss = 0.67 (430.8 examples/sec; 0.232 sec/batch), lr: 0.009500\n",
      "2018-11-29 14:34:01.603084: step 1440/14650 (epoch 5/50), acc = 0.81, loss = 0.76 (424.6 examples/sec; 0.236 sec/batch), lr: 0.009500\n",
      "2018-11-29 14:34:06.425798: step 1450/14650 (epoch 5/50), acc = 0.76, loss = 1.14 (420.4 examples/sec; 0.238 sec/batch), lr: 0.009500\n",
      "2018-11-29 14:34:11.029543: step 1460/14650 (epoch 5/50), acc = 0.77, loss = 1.34 (416.0 examples/sec; 0.240 sec/batch), lr: 0.009500\n",
      "2018-11-29 14:34:15.684160: step 1470/14650 (epoch 6/50), acc = 0.72, loss = 1.57 (419.6 examples/sec; 0.238 sec/batch), lr: 0.009500\n",
      "2018-11-29 14:34:20.740478: step 1480/14650 (epoch 6/50), acc = 0.72, loss = 1.10 (414.7 examples/sec; 0.241 sec/batch), lr: 0.009500\n",
      "2018-11-29 14:34:25.716799: step 1490/14650 (epoch 6/50), acc = 0.73, loss = 1.39 (411.4 examples/sec; 0.243 sec/batch), lr: 0.009500\n",
      "2018-11-29 14:34:30.800159: step 1500/14650 (epoch 6/50), acc = 0.74, loss = 0.99 (406.5 examples/sec; 0.246 sec/batch), lr: 0.009500\n",
      "2018-11-29 14:34:36.001156: step 1510/14650 (epoch 6/50), acc = 0.76, loss = 0.89 (407.0 examples/sec; 0.246 sec/batch), lr: 0.009500\n",
      "2018-11-29 14:34:41.151780: step 1520/14650 (epoch 6/50), acc = 0.75, loss = 0.85 (404.7 examples/sec; 0.247 sec/batch), lr: 0.009500\n",
      "2018-11-29 14:34:47.006738: step 1530/14650 (epoch 6/50), acc = 0.84, loss = 0.69 (401.5 examples/sec; 0.249 sec/batch), lr: 0.009500\n",
      "2018-11-29 14:34:52.666659: step 1540/14650 (epoch 6/50), acc = 0.83, loss = 0.85 (316.9 examples/sec; 0.316 sec/batch), lr: 0.009500\n",
      "2018-11-29 14:34:57.562669: step 1550/14650 (epoch 6/50), acc = 0.88, loss = 0.54 (405.3 examples/sec; 0.247 sec/batch), lr: 0.009500\n",
      "2018-11-29 14:35:02.589470: step 1560/14650 (epoch 6/50), acc = 0.78, loss = 0.94 (397.8 examples/sec; 0.251 sec/batch), lr: 0.009500\n",
      "2018-11-29 14:35:07.974964: step 1570/14650 (epoch 6/50), acc = 0.84, loss = 0.53 (395.6 examples/sec; 0.253 sec/batch), lr: 0.009500\n",
      "2018-11-29 14:35:13.215541: step 1580/14650 (epoch 6/50), acc = 0.73, loss = 1.00 (392.7 examples/sec; 0.255 sec/batch), lr: 0.009500\n",
      "2018-11-29 14:35:18.642319: step 1590/14650 (epoch 6/50), acc = 0.71, loss = 1.08 (392.7 examples/sec; 0.255 sec/batch), lr: 0.009500\n",
      "2018-11-29 14:35:23.894615: step 1600/14650 (epoch 6/50), acc = 0.78, loss = 1.50 (400.4 examples/sec; 0.250 sec/batch), lr: 0.009500\n",
      "\n",
      "Step 1600: train_loss = 0.981693, train_accuracy = 0.782\n",
      "Step 1600:  test_loss = 0.930298,  test_accuracy = 0.823\n",
      "\n",
      "2018-11-29 14:35:31.249929: step 1610/14650 (epoch 6/50), acc = 0.79, loss = 0.94 (385.4 examples/sec; 0.259 sec/batch), lr: 0.009500\n",
      "2018-11-29 14:35:36.939250: step 1620/14650 (epoch 6/50), acc = 0.71, loss = 0.93 (394.1 examples/sec; 0.254 sec/batch), lr: 0.009500\n",
      "2018-11-29 14:35:42.593605: step 1630/14650 (epoch 6/50), acc = 0.80, loss = 0.90 (385.2 examples/sec; 0.260 sec/batch), lr: 0.009500\n",
      "2018-11-29 14:35:48.308643: step 1640/14650 (epoch 6/50), acc = 0.77, loss = 1.11 (380.8 examples/sec; 0.263 sec/batch), lr: 0.009500\n",
      "2018-11-29 14:35:53.831684: step 1650/14650 (epoch 6/50), acc = 0.74, loss = 0.99 (384.7 examples/sec; 0.260 sec/batch), lr: 0.009500\n",
      "2018-11-29 14:35:59.720185: step 1660/14650 (epoch 6/50), acc = 0.75, loss = 0.96 (271.7 examples/sec; 0.368 sec/batch), lr: 0.009500\n",
      "2018-11-29 14:36:06.287781: step 1670/14650 (epoch 6/50), acc = 0.83, loss = 0.75 (377.3 examples/sec; 0.265 sec/batch), lr: 0.009500\n",
      "2018-11-29 14:36:11.993591: step 1680/14650 (epoch 6/50), acc = 0.75, loss = 0.85 (383.6 examples/sec; 0.261 sec/batch), lr: 0.009500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-29 14:36:17.661664: step 1690/14650 (epoch 6/50), acc = 0.80, loss = 0.77 (381.7 examples/sec; 0.262 sec/batch), lr: 0.009500\n",
      "2018-11-29 14:36:23.808935: step 1700/14650 (epoch 6/50), acc = 0.69, loss = 1.09 (314.6 examples/sec; 0.318 sec/batch), lr: 0.009500\n",
      "2018-11-29 14:36:29.850629: step 1710/14650 (epoch 6/50), acc = 0.77, loss = 0.83 (373.9 examples/sec; 0.267 sec/batch), lr: 0.009500\n",
      "2018-11-29 14:36:35.929874: step 1720/14650 (epoch 6/50), acc = 0.81, loss = 0.95 (376.3 examples/sec; 0.266 sec/batch), lr: 0.009500\n",
      "2018-11-29 14:36:37.180476: step 1722/14650 (epoch 6/50), Learning rate decays to 0.00903\n",
      "2018-11-29 14:36:41.814712: step 1730/14650 (epoch 6/50), acc = 0.71, loss = 1.34 (352.6 examples/sec; 0.284 sec/batch), lr: 0.009025\n",
      "2018-11-29 14:36:47.821496: step 1740/14650 (epoch 6/50), acc = 0.77, loss = 1.20 (372.6 examples/sec; 0.268 sec/batch), lr: 0.009025\n",
      "2018-11-29 14:36:54.193135: step 1750/14650 (epoch 6/50), acc = 0.77, loss = 0.93 (377.1 examples/sec; 0.265 sec/batch), lr: 0.009025\n",
      "2018-11-29 14:37:00.842423: step 1760/14650 (epoch 7/50), acc = 0.79, loss = 1.07 (366.8 examples/sec; 0.273 sec/batch), lr: 0.009025\n",
      "2018-11-29 14:37:06.872986: step 1770/14650 (epoch 7/50), acc = 0.79, loss = 0.81 (368.7 examples/sec; 0.271 sec/batch), lr: 0.009025\n",
      "2018-11-29 14:37:12.787292: step 1780/14650 (epoch 7/50), acc = 0.80, loss = 1.24 (362.0 examples/sec; 0.276 sec/batch), lr: 0.009025\n",
      "2018-11-29 14:37:19.083010: step 1790/14650 (epoch 7/50), acc = 0.78, loss = 0.81 (366.2 examples/sec; 0.273 sec/batch), lr: 0.009025\n",
      "2018-11-29 14:37:25.364343: step 1800/14650 (epoch 7/50), acc = 0.79, loss = 0.87 (363.3 examples/sec; 0.275 sec/batch), lr: 0.009025\n",
      "\n",
      "Step 1800: train_loss = 1.020635, train_accuracy = 0.780\n",
      "Step 1800:  test_loss = 0.941035,  test_accuracy = 0.820\n",
      "\n",
      "2018-11-29 14:37:33.336229: step 1810/14650 (epoch 7/50), acc = 0.74, loss = 0.89 (359.5 examples/sec; 0.278 sec/batch), lr: 0.009025\n",
      "2018-11-29 14:37:39.914482: step 1820/14650 (epoch 7/50), acc = 0.83, loss = 0.75 (361.0 examples/sec; 0.277 sec/batch), lr: 0.009025\n",
      "2018-11-29 14:37:46.156048: step 1830/14650 (epoch 7/50), acc = 0.64, loss = 1.44 (362.1 examples/sec; 0.276 sec/batch), lr: 0.009025\n",
      "2018-11-29 14:37:53.037713: step 1840/14650 (epoch 7/50), acc = 0.82, loss = 0.74 (364.4 examples/sec; 0.274 sec/batch), lr: 0.009025\n",
      "2018-11-29 14:37:59.067630: step 1850/14650 (epoch 7/50), acc = 0.75, loss = 1.11 (361.1 examples/sec; 0.277 sec/batch), lr: 0.009025\n",
      "2018-11-29 14:38:05.429913: step 1860/14650 (epoch 7/50), acc = 0.80, loss = 0.89 (250.4 examples/sec; 0.399 sec/batch), lr: 0.009025\n",
      "2018-11-29 14:38:11.634622: step 1870/14650 (epoch 7/50), acc = 0.81, loss = 0.87 (352.7 examples/sec; 0.284 sec/batch), lr: 0.009025\n",
      "2018-11-29 14:38:18.077502: step 1880/14650 (epoch 7/50), acc = 0.77, loss = 1.02 (246.1 examples/sec; 0.406 sec/batch), lr: 0.009025\n",
      "2018-11-29 14:38:24.418243: step 1890/14650 (epoch 7/50), acc = 0.80, loss = 0.77 (354.2 examples/sec; 0.282 sec/batch), lr: 0.009025\n",
      "2018-11-29 14:38:31.087386: step 1900/14650 (epoch 7/50), acc = 0.73, loss = 1.18 (243.7 examples/sec; 0.410 sec/batch), lr: 0.009025\n",
      "2018-11-29 14:38:37.790296: step 1910/14650 (epoch 7/50), acc = 0.70, loss = 1.38 (353.8 examples/sec; 0.283 sec/batch), lr: 0.009025\n",
      "2018-11-29 14:38:44.223252: step 1920/14650 (epoch 7/50), acc = 0.77, loss = 1.03 (253.0 examples/sec; 0.395 sec/batch), lr: 0.009025\n",
      "2018-11-29 14:38:50.859939: step 1930/14650 (epoch 7/50), acc = 0.68, loss = 1.17 (343.4 examples/sec; 0.291 sec/batch), lr: 0.009025\n",
      "2018-11-29 14:38:57.582159: step 1940/14650 (epoch 7/50), acc = 0.80, loss = 0.90 (349.7 examples/sec; 0.286 sec/batch), lr: 0.009025\n",
      "2018-11-29 14:39:04.612057: step 1950/14650 (epoch 7/50), acc = 0.81, loss = 0.67 (342.0 examples/sec; 0.292 sec/batch), lr: 0.009025\n",
      "2018-11-29 14:39:11.899656: step 1960/14650 (epoch 7/50), acc = 0.78, loss = 1.05 (342.0 examples/sec; 0.292 sec/batch), lr: 0.009025\n",
      "2018-11-29 14:39:19.560222: step 1970/14650 (epoch 7/50), acc = 0.79, loss = 0.73 (333.9 examples/sec; 0.299 sec/batch), lr: 0.009025\n",
      "2018-11-29 14:39:26.669128: step 1980/14650 (epoch 7/50), acc = 0.73, loss = 0.90 (337.7 examples/sec; 0.296 sec/batch), lr: 0.009025\n",
      "2018-11-29 14:39:33.990470: step 1990/14650 (epoch 7/50), acc = 0.72, loss = 1.12 (331.6 examples/sec; 0.302 sec/batch), lr: 0.009025\n",
      "2018-11-29 14:39:41.168486: step 2000/14650 (epoch 7/50), acc = 0.76, loss = 0.98 (338.0 examples/sec; 0.296 sec/batch), lr: 0.009025\n",
      "\n",
      "Step 2000: train_loss = 0.987043, train_accuracy = 0.783\n",
      "Step 2000:  test_loss = 0.924093,  test_accuracy = 0.835\n",
      "\n",
      "2018-11-29 14:39:50.062814: step 2010/14650 (epoch 7/50), acc = 0.76, loss = 1.10 (330.6 examples/sec; 0.302 sec/batch), lr: 0.009025\n",
      "2018-11-29 14:39:57.936661: step 2020/14650 (epoch 7/50), acc = 0.76, loss = 1.04 (341.8 examples/sec; 0.293 sec/batch), lr: 0.009025\n",
      "2018-11-29 14:40:04.951394: step 2030/14650 (epoch 7/50), acc = 0.84, loss = 0.83 (333.5 examples/sec; 0.300 sec/batch), lr: 0.009025\n",
      "2018-11-29 14:40:11.864658: step 2040/14650 (epoch 7/50), acc = 0.84, loss = 0.72 (334.7 examples/sec; 0.299 sec/batch), lr: 0.009025\n",
      "2018-11-29 14:40:19.128085: step 2050/14650 (epoch 7/50), acc = 0.76, loss = 1.53 (328.5 examples/sec; 0.304 sec/batch), lr: 0.009025\n",
      "2018-11-29 14:40:26.083196: step 2060/14650 (epoch 8/50), acc = 0.84, loss = 0.77 (322.2 examples/sec; 0.310 sec/batch), lr: 0.009025\n",
      "2018-11-29 14:40:33.482877: step 2070/14650 (epoch 8/50), acc = 0.79, loss = 0.97 (328.5 examples/sec; 0.304 sec/batch), lr: 0.009025\n",
      "2018-11-29 14:40:41.300417: step 2080/14650 (epoch 8/50), acc = 0.86, loss = 0.91 (324.0 examples/sec; 0.309 sec/batch), lr: 0.009025\n",
      "2018-11-29 14:40:48.873314: step 2090/14650 (epoch 8/50), acc = 0.81, loss = 1.00 (217.7 examples/sec; 0.459 sec/batch), lr: 0.009025\n",
      "2018-11-29 14:40:56.125020: step 2100/14650 (epoch 8/50), acc = 0.80, loss = 0.88 (324.9 examples/sec; 0.308 sec/batch), lr: 0.009025\n",
      "2018-11-29 14:41:03.523161: step 2110/14650 (epoch 8/50), acc = 0.80, loss = 0.93 (322.2 examples/sec; 0.310 sec/batch), lr: 0.009025\n",
      "2018-11-29 14:41:10.732459: step 2120/14650 (epoch 8/50), acc = 0.77, loss = 0.83 (317.6 examples/sec; 0.315 sec/batch), lr: 0.009025\n",
      "2018-11-29 14:41:18.262394: step 2130/14650 (epoch 8/50), acc = 0.76, loss = 2.51 (320.3 examples/sec; 0.312 sec/batch), lr: 0.009025\n",
      "2018-11-29 14:41:25.351289: step 2140/14650 (epoch 8/50), acc = 0.77, loss = 0.91 (323.4 examples/sec; 0.309 sec/batch), lr: 0.009025\n",
      "2018-11-29 14:41:32.797973: step 2150/14650 (epoch 8/50), acc = 0.74, loss = 1.04 (326.5 examples/sec; 0.306 sec/batch), lr: 0.009025\n",
      "2018-11-29 14:41:40.536551: step 2160/14650 (epoch 8/50), acc = 0.73, loss = 1.16 (316.5 examples/sec; 0.316 sec/batch), lr: 0.009025\n",
      "2018-11-29 14:41:47.771568: step 2170/14650 (epoch 8/50), acc = 0.75, loss = 1.85 (318.3 examples/sec; 0.314 sec/batch), lr: 0.009025\n",
      "2018-11-29 14:41:55.365010: step 2180/14650 (epoch 8/50), acc = 0.78, loss = 0.99 (312.2 examples/sec; 0.320 sec/batch), lr: 0.009025\n",
      "2018-11-29 14:42:02.940532: step 2190/14650 (epoch 8/50), acc = 0.80, loss = 0.84 (308.9 examples/sec; 0.324 sec/batch), lr: 0.009025\n",
      "2018-11-29 14:42:10.722546: step 2200/14650 (epoch 8/50), acc = 0.69, loss = 1.28 (205.5 examples/sec; 0.487 sec/batch), lr: 0.009025\n",
      "\n",
      "Step 2200: train_loss = 1.057750, train_accuracy = 0.783\n",
      "Step 2200:  test_loss = 0.914140,  test_accuracy = 0.822\n",
      "\n",
      "2018-11-29 14:42:20.074216: step 2210/14650 (epoch 8/50), acc = 0.77, loss = 0.86 (314.7 examples/sec; 0.318 sec/batch), lr: 0.009025\n",
      "2018-11-29 14:42:27.531035: step 2220/14650 (epoch 8/50), acc = 0.77, loss = 0.90 (316.9 examples/sec; 0.316 sec/batch), lr: 0.009025\n",
      "2018-11-29 14:42:35.281165: step 2230/14650 (epoch 8/50), acc = 0.70, loss = 1.87 (226.5 examples/sec; 0.442 sec/batch), lr: 0.009025\n",
      "2018-11-29 14:42:42.761026: step 2240/14650 (epoch 8/50), acc = 0.78, loss = 0.92 (308.2 examples/sec; 0.324 sec/batch), lr: 0.009025\n",
      "2018-11-29 14:42:50.621916: step 2250/14650 (epoch 8/50), acc = 0.78, loss = 0.97 (308.7 examples/sec; 0.324 sec/batch), lr: 0.009025\n",
      "2018-11-29 14:42:58.515820: step 2260/14650 (epoch 8/50), acc = 0.85, loss = 0.87 (307.8 examples/sec; 0.325 sec/batch), lr: 0.009025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-29 14:43:06.735663: step 2270/14650 (epoch 8/50), acc = 0.83, loss = 0.96 (304.0 examples/sec; 0.329 sec/batch), lr: 0.009025\n",
      "2018-11-29 14:43:14.496548: step 2280/14650 (epoch 8/50), acc = 0.70, loss = 1.26 (306.4 examples/sec; 0.326 sec/batch), lr: 0.009025\n",
      "2018-11-29 14:43:22.620143: step 2290/14650 (epoch 8/50), acc = 0.83, loss = 0.67 (303.5 examples/sec; 0.329 sec/batch), lr: 0.009025\n",
      "2018-11-29 14:43:30.819073: step 2300/14650 (epoch 8/50), acc = 0.71, loss = 1.46 (302.5 examples/sec; 0.331 sec/batch), lr: 0.009025\n",
      "2018-11-29 14:43:39.641416: step 2310/14650 (epoch 8/50), acc = 0.73, loss = 0.97 (300.7 examples/sec; 0.333 sec/batch), lr: 0.009025\n",
      "2018-11-29 14:43:46.438966: step 2318/14650 (epoch 8/50), Learning rate decays to 0.00857\n",
      "2018-11-29 14:43:47.962719: step 2320/14650 (epoch 8/50), acc = 0.79, loss = 0.94 (300.9 examples/sec; 0.332 sec/batch), lr: 0.008574\n",
      "2018-11-29 14:43:56.774759: step 2330/14650 (epoch 8/50), acc = 0.76, loss = 1.80 (287.6 examples/sec; 0.348 sec/batch), lr: 0.008574\n",
      "2018-11-29 14:44:05.093519: step 2340/14650 (epoch 8/50), acc = 0.78, loss = 0.98 (296.9 examples/sec; 0.337 sec/batch), lr: 0.008574\n",
      "2018-11-29 14:44:14.288280: step 2350/14650 (epoch 9/50), acc = 0.77, loss = 0.80 (293.9 examples/sec; 0.340 sec/batch), lr: 0.008574\n",
      "2018-11-29 14:44:23.132160: step 2360/14650 (epoch 9/50), acc = 0.83, loss = 0.84 (291.5 examples/sec; 0.343 sec/batch), lr: 0.008574\n",
      "2018-11-29 14:44:32.311421: step 2370/14650 (epoch 9/50), acc = 0.86, loss = 0.64 (286.8 examples/sec; 0.349 sec/batch), lr: 0.008574\n",
      "2018-11-29 14:44:40.919309: step 2380/14650 (epoch 9/50), acc = 0.80, loss = 0.93 (295.4 examples/sec; 0.339 sec/batch), lr: 0.008574\n",
      "2018-11-29 14:44:49.634887: step 2390/14650 (epoch 9/50), acc = 0.79, loss = 0.78 (295.8 examples/sec; 0.338 sec/batch), lr: 0.008574\n",
      "2018-11-29 14:44:58.388916: step 2400/14650 (epoch 9/50), acc = 0.81, loss = 0.97 (290.2 examples/sec; 0.345 sec/batch), lr: 0.008574\n",
      "\n",
      "Step 2400: train_loss = 0.937834, train_accuracy = 0.796\n",
      "Step 2400:  test_loss = 0.907476,  test_accuracy = 0.839\n",
      "\n",
      "2018-11-29 14:45:08.930744: step 2410/14650 (epoch 9/50), acc = 0.82, loss = 0.74 (296.6 examples/sec; 0.337 sec/batch), lr: 0.008574\n",
      "2018-11-29 14:45:17.913327: step 2420/14650 (epoch 9/50), acc = 0.71, loss = 1.18 (292.8 examples/sec; 0.341 sec/batch), lr: 0.008574\n",
      "2018-11-29 14:45:26.778800: step 2430/14650 (epoch 9/50), acc = 0.82, loss = 0.91 (294.9 examples/sec; 0.339 sec/batch), lr: 0.008574\n",
      "2018-11-29 14:45:36.296715: step 2440/14650 (epoch 9/50), acc = 0.74, loss = 1.02 (294.6 examples/sec; 0.339 sec/batch), lr: 0.008574\n",
      "2018-11-29 14:45:45.427986: step 2450/14650 (epoch 9/50), acc = 0.78, loss = 0.92 (289.7 examples/sec; 0.345 sec/batch), lr: 0.008574\n",
      "2018-11-29 14:45:55.151724: step 2460/14650 (epoch 9/50), acc = 0.78, loss = 0.90 (286.5 examples/sec; 0.349 sec/batch), lr: 0.008574\n",
      "2018-11-29 14:46:04.368892: step 2470/14650 (epoch 9/50), acc = 0.84, loss = 1.94 (286.4 examples/sec; 0.349 sec/batch), lr: 0.008574\n",
      "2018-11-29 14:46:13.560781: step 2480/14650 (epoch 9/50), acc = 0.77, loss = 1.00 (285.3 examples/sec; 0.350 sec/batch), lr: 0.008574\n",
      "2018-11-29 14:46:22.994837: step 2490/14650 (epoch 9/50), acc = 0.79, loss = 1.61 (277.7 examples/sec; 0.360 sec/batch), lr: 0.008574\n",
      "2018-11-29 14:46:32.497187: step 2500/14650 (epoch 9/50), acc = 0.84, loss = 0.72 (284.8 examples/sec; 0.351 sec/batch), lr: 0.008574\n",
      "2018-11-29 14:46:41.817385: step 2510/14650 (epoch 9/50), acc = 0.75, loss = 1.18 (279.5 examples/sec; 0.358 sec/batch), lr: 0.008574\n",
      "2018-11-29 14:46:51.374929: step 2520/14650 (epoch 9/50), acc = 0.79, loss = 1.05 (283.1 examples/sec; 0.353 sec/batch), lr: 0.008574\n",
      "2018-11-29 14:47:00.964414: step 2530/14650 (epoch 9/50), acc = 0.76, loss = 0.99 (283.7 examples/sec; 0.352 sec/batch), lr: 0.008574\n",
      "2018-11-29 14:47:09.901980: step 2540/14650 (epoch 9/50), acc = 0.73, loss = 1.12 (285.2 examples/sec; 0.351 sec/batch), lr: 0.008574\n",
      "2018-11-29 14:47:19.447992: step 2550/14650 (epoch 9/50), acc = 0.75, loss = 1.18 (273.3 examples/sec; 0.366 sec/batch), lr: 0.008574\n",
      "2018-11-29 14:47:29.254432: step 2560/14650 (epoch 9/50), acc = 0.79, loss = 1.20 (281.8 examples/sec; 0.355 sec/batch), lr: 0.008574\n",
      "2018-11-29 14:47:39.506111: step 2570/14650 (epoch 9/50), acc = 0.72, loss = 1.46 (271.5 examples/sec; 0.368 sec/batch), lr: 0.008574\n",
      "2018-11-29 14:47:49.173838: step 2580/14650 (epoch 9/50), acc = 0.72, loss = 1.18 (283.1 examples/sec; 0.353 sec/batch), lr: 0.008574\n",
      "2018-11-29 14:47:59.149118: step 2590/14650 (epoch 9/50), acc = 0.70, loss = 1.47 (276.1 examples/sec; 0.362 sec/batch), lr: 0.008574\n",
      "2018-11-29 14:48:09.487558: step 2600/14650 (epoch 9/50), acc = 0.73, loss = 1.04 (280.6 examples/sec; 0.356 sec/batch), lr: 0.008574\n",
      "\n",
      "Step 2600: train_loss = 1.012828, train_accuracy = 0.786\n",
      "Step 2600:  test_loss = 1.082822,  test_accuracy = 0.824\n",
      "\n",
      "2018-11-29 14:48:21.691398: step 2610/14650 (epoch 9/50), acc = 0.79, loss = 0.84 (275.3 examples/sec; 0.363 sec/batch), lr: 0.008574\n",
      "2018-11-29 14:48:31.462762: step 2620/14650 (epoch 9/50), acc = 0.77, loss = 2.34 (275.9 examples/sec; 0.363 sec/batch), lr: 0.008574\n",
      "2018-11-29 14:48:41.723742: step 2630/14650 (epoch 9/50), acc = 0.81, loss = 1.76 (277.0 examples/sec; 0.361 sec/batch), lr: 0.008574\n",
      "2018-11-29 14:48:52.351969: step 2640/14650 (epoch 10/50), acc = 0.79, loss = 1.47 (279.6 examples/sec; 0.358 sec/batch), lr: 0.008574\n",
      "2018-11-29 14:49:02.243748: step 2650/14650 (epoch 10/50), acc = 0.83, loss = 0.86 (273.2 examples/sec; 0.366 sec/batch), lr: 0.008574\n",
      "2018-11-29 14:49:12.734323: step 2660/14650 (epoch 10/50), acc = 0.82, loss = 0.91 (273.3 examples/sec; 0.366 sec/batch), lr: 0.008574\n",
      "2018-11-29 14:49:23.092584: step 2670/14650 (epoch 10/50), acc = 0.76, loss = 1.24 (277.3 examples/sec; 0.361 sec/batch), lr: 0.008574\n",
      "2018-11-29 14:49:33.619194: step 2680/14650 (epoch 10/50), acc = 0.74, loss = 1.17 (277.0 examples/sec; 0.361 sec/batch), lr: 0.008574\n",
      "2018-11-29 14:49:44.441020: step 2690/14650 (epoch 10/50), acc = 0.73, loss = 0.97 (272.8 examples/sec; 0.367 sec/batch), lr: 0.008574\n",
      "2018-11-29 14:49:55.231356: step 2700/14650 (epoch 10/50), acc = 0.74, loss = 3.10 (275.3 examples/sec; 0.363 sec/batch), lr: 0.008574\n",
      "2018-11-29 14:50:05.932099: step 2710/14650 (epoch 10/50), acc = 0.78, loss = 1.43 (177.0 examples/sec; 0.565 sec/batch), lr: 0.008574\n",
      "2018-11-29 14:50:15.992132: step 2720/14650 (epoch 10/50), acc = 0.76, loss = 0.97 (266.5 examples/sec; 0.375 sec/batch), lr: 0.008574\n",
      "2018-11-29 14:50:27.029107: step 2730/14650 (epoch 10/50), acc = 0.78, loss = 0.84 (265.9 examples/sec; 0.376 sec/batch), lr: 0.008574\n",
      "2018-11-29 14:50:38.184675: step 2740/14650 (epoch 10/50), acc = 0.79, loss = 0.95 (262.7 examples/sec; 0.381 sec/batch), lr: 0.008574\n",
      "2018-11-29 14:50:49.708066: step 2750/14650 (epoch 10/50), acc = 0.77, loss = 0.92 (158.7 examples/sec; 0.630 sec/batch), lr: 0.008574\n",
      "2018-11-29 14:51:02.732322: step 2760/14650 (epoch 10/50), acc = 0.83, loss = 1.60 (263.1 examples/sec; 0.380 sec/batch), lr: 0.008574\n",
      "2018-11-29 14:51:13.915895: step 2770/14650 (epoch 10/50), acc = 0.84, loss = 0.73 (268.4 examples/sec; 0.373 sec/batch), lr: 0.008574\n",
      "2018-11-29 14:51:24.773196: step 2780/14650 (epoch 10/50), acc = 0.78, loss = 0.93 (267.5 examples/sec; 0.374 sec/batch), lr: 0.008574\n",
      "2018-11-29 14:51:35.745616: step 2790/14650 (epoch 10/50), acc = 0.72, loss = 1.12 (266.4 examples/sec; 0.375 sec/batch), lr: 0.008574\n",
      "2018-11-29 14:51:46.805743: step 2800/14650 (epoch 10/50), acc = 0.75, loss = 1.45 (267.0 examples/sec; 0.375 sec/batch), lr: 0.008574\n",
      "\n",
      "Step 2800: train_loss = 1.040716, train_accuracy = 0.783\n",
      "Step 2800:  test_loss = 1.005475,  test_accuracy = 0.830\n",
      "\n",
      "2018-11-29 14:51:59.915307: step 2810/14650 (epoch 10/50), acc = 0.87, loss = 0.58 (266.9 examples/sec; 0.375 sec/batch), lr: 0.008574\n",
      "2018-11-29 14:52:08.873341: step 2818/14650 (epoch 10/50), Learning rate decays to 0.00815\n",
      "2018-11-29 14:52:11.173461: step 2820/14650 (epoch 10/50), acc = 0.75, loss = 1.10 (266.6 examples/sec; 0.375 sec/batch), lr: 0.008145\n",
      "2018-11-29 14:52:22.346985: step 2830/14650 (epoch 10/50), acc = 0.75, loss = 1.00 (263.4 examples/sec; 0.380 sec/batch), lr: 0.008145\n",
      "2018-11-29 14:52:33.553326: step 2840/14650 (epoch 10/50), acc = 0.66, loss = 1.48 (263.9 examples/sec; 0.379 sec/batch), lr: 0.008145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-29 14:52:45.028626: step 2850/14650 (epoch 10/50), acc = 0.79, loss = 0.81 (263.4 examples/sec; 0.380 sec/batch), lr: 0.008145\n",
      "2018-11-29 14:52:56.419074: step 2860/14650 (epoch 10/50), acc = 0.78, loss = 1.06 (261.9 examples/sec; 0.382 sec/batch), lr: 0.008145\n",
      "2018-11-29 14:53:07.950150: step 2870/14650 (epoch 10/50), acc = 0.75, loss = 1.00 (262.1 examples/sec; 0.382 sec/batch), lr: 0.008145\n",
      "2018-11-29 14:53:19.905769: step 2880/14650 (epoch 10/50), acc = 0.81, loss = 1.16 (160.1 examples/sec; 0.625 sec/batch), lr: 0.008145\n",
      "2018-11-29 14:53:31.536071: step 2890/14650 (epoch 10/50), acc = 0.84, loss = 0.72 (245.2 examples/sec; 0.408 sec/batch), lr: 0.008145\n",
      "2018-11-29 14:53:42.867666: step 2900/14650 (epoch 10/50), acc = 0.83, loss = 0.78 (213.2 examples/sec; 0.469 sec/batch), lr: 0.008145\n",
      "2018-11-29 14:53:54.493658: step 2910/14650 (epoch 10/50), acc = 0.78, loss = 0.87 (258.1 examples/sec; 0.387 sec/batch), lr: 0.008145\n",
      "2018-11-29 14:54:06.060038: step 2920/14650 (epoch 10/50), acc = 0.76, loss = 1.03 (258.9 examples/sec; 0.386 sec/batch), lr: 0.008145\n",
      "2018-11-29 14:54:18.186103: step 2930/14650 (epoch 10/50), acc = 0.43, loss = 0.99 (287.7 examples/sec; 0.348 sec/batch), lr: 0.008145\n",
      "2018-11-29 14:54:30.361116: step 2940/14650 (epoch 11/50), acc = 0.72, loss = 1.20 (256.3 examples/sec; 0.390 sec/batch), lr: 0.008145\n",
      "2018-11-29 14:54:42.050543: step 2950/14650 (epoch 11/50), acc = 0.81, loss = 0.84 (255.8 examples/sec; 0.391 sec/batch), lr: 0.008145\n",
      "2018-11-29 14:54:53.833390: step 2960/14650 (epoch 11/50), acc = 0.76, loss = 1.18 (255.4 examples/sec; 0.392 sec/batch), lr: 0.008145\n",
      "2018-11-29 14:55:05.862339: step 2970/14650 (epoch 11/50), acc = 0.75, loss = 1.15 (159.4 examples/sec; 0.627 sec/batch), lr: 0.008145\n",
      "2018-11-29 14:55:18.092174: step 2980/14650 (epoch 11/50), acc = 0.81, loss = 0.89 (162.2 examples/sec; 0.617 sec/batch), lr: 0.008145\n",
      "2018-11-29 14:55:29.938587: step 2990/14650 (epoch 11/50), acc = 0.79, loss = 0.96 (253.5 examples/sec; 0.394 sec/batch), lr: 0.008145\n",
      "2018-11-29 14:55:42.068828: step 3000/14650 (epoch 11/50), acc = 0.77, loss = 0.85 (253.0 examples/sec; 0.395 sec/batch), lr: 0.008145\n",
      "\n",
      "Step 3000: train_loss = 0.958027, train_accuracy = 0.792\n",
      "Step 3000:  test_loss = 1.063274,  test_accuracy = 0.824\n",
      "\n",
      "2018-11-29 14:55:56.410380: step 3010/14650 (epoch 11/50), acc = 0.83, loss = 0.68 (248.9 examples/sec; 0.402 sec/batch), lr: 0.008145\n",
      "2018-11-29 14:56:08.782634: step 3020/14650 (epoch 11/50), acc = 0.82, loss = 0.74 (251.9 examples/sec; 0.397 sec/batch), lr: 0.008145\n",
      "2018-11-29 14:56:21.264420: step 3030/14650 (epoch 11/50), acc = 0.88, loss = 0.68 (250.9 examples/sec; 0.399 sec/batch), lr: 0.008145\n",
      "2018-11-29 14:56:33.756651: step 3040/14650 (epoch 11/50), acc = 0.80, loss = 0.95 (247.9 examples/sec; 0.403 sec/batch), lr: 0.008145\n",
      "2018-11-29 14:56:46.301208: step 3050/14650 (epoch 11/50), acc = 0.75, loss = 0.99 (249.2 examples/sec; 0.401 sec/batch), lr: 0.008145\n",
      "2018-11-29 14:56:58.919084: step 3060/14650 (epoch 11/50), acc = 0.80, loss = 1.21 (249.1 examples/sec; 0.402 sec/batch), lr: 0.008145\n",
      "2018-11-29 14:57:11.452022: step 3070/14650 (epoch 11/50), acc = 0.77, loss = 1.05 (248.0 examples/sec; 0.403 sec/batch), lr: 0.008145\n",
      "2018-11-29 14:57:24.214315: step 3080/14650 (epoch 11/50), acc = 0.76, loss = 0.88 (247.6 examples/sec; 0.404 sec/batch), lr: 0.008145\n",
      "2018-11-29 14:57:37.279915: step 3090/14650 (epoch 11/50), acc = 0.76, loss = 1.39 (246.3 examples/sec; 0.406 sec/batch), lr: 0.008145\n",
      "2018-11-29 14:57:50.305879: step 3100/14650 (epoch 11/50), acc = 0.82, loss = 0.82 (240.5 examples/sec; 0.416 sec/batch), lr: 0.008145\n",
      "2018-11-29 14:58:02.812362: step 3110/14650 (epoch 11/50), acc = 0.79, loss = 1.02 (244.2 examples/sec; 0.409 sec/batch), lr: 0.008145\n",
      "2018-11-29 14:58:15.804345: step 3120/14650 (epoch 11/50), acc = 0.79, loss = 0.83 (242.5 examples/sec; 0.412 sec/batch), lr: 0.008145\n",
      "2018-11-29 14:58:28.784216: step 3130/14650 (epoch 11/50), acc = 0.78, loss = 0.89 (240.1 examples/sec; 0.417 sec/batch), lr: 0.008145\n",
      "2018-11-29 14:58:41.802411: step 3140/14650 (epoch 11/50), acc = 0.81, loss = 1.01 (244.0 examples/sec; 0.410 sec/batch), lr: 0.008145\n",
      "2018-11-29 14:58:54.941493: step 3150/14650 (epoch 11/50), acc = 0.82, loss = 0.85 (184.3 examples/sec; 0.543 sec/batch), lr: 0.008145\n",
      "2018-11-29 14:59:08.112396: step 3160/14650 (epoch 11/50), acc = 0.73, loss = 1.67 (145.9 examples/sec; 0.685 sec/batch), lr: 0.008145\n",
      "2018-11-29 14:59:21.239483: step 3170/14650 (epoch 11/50), acc = 0.72, loss = 1.26 (178.6 examples/sec; 0.560 sec/batch), lr: 0.008145\n",
      "2018-11-29 14:59:35.473090: step 3180/14650 (epoch 11/50), acc = 0.77, loss = 1.10 (234.6 examples/sec; 0.426 sec/batch), lr: 0.008145\n",
      "2018-11-29 14:59:49.155601: step 3190/14650 (epoch 11/50), acc = 0.81, loss = 0.78 (165.5 examples/sec; 0.604 sec/batch), lr: 0.008145\n",
      "2018-11-29 15:00:02.594558: step 3200/14650 (epoch 11/50), acc = 0.77, loss = 1.11 (142.5 examples/sec; 0.702 sec/batch), lr: 0.008145\n",
      "\n",
      "Step 3200: train_loss = 0.975851, train_accuracy = 0.788\n",
      "Step 3200:  test_loss = 1.107173,  test_accuracy = 0.832\n",
      "\n",
      "2018-11-29 15:00:18.263155: step 3210/14650 (epoch 11/50), acc = 0.81, loss = 1.04 (204.8 examples/sec; 0.488 sec/batch), lr: 0.008145\n",
      "2018-11-29 15:00:32.129387: step 3220/14650 (epoch 11/50), acc = 0.85, loss = 0.71 (236.3 examples/sec; 0.423 sec/batch), lr: 0.008145\n",
      "2018-11-29 15:00:45.732509: step 3230/14650 (epoch 12/50), acc = 0.80, loss = 0.76 (155.0 examples/sec; 0.645 sec/batch), lr: 0.008145\n",
      "2018-11-29 15:00:59.235859: step 3240/14650 (epoch 12/50), acc = 0.84, loss = 0.69 (233.3 examples/sec; 0.429 sec/batch), lr: 0.008145\n",
      "2018-11-29 15:01:13.108768: step 3250/14650 (epoch 12/50), acc = 0.84, loss = 0.78 (228.4 examples/sec; 0.438 sec/batch), lr: 0.008145\n",
      "2018-11-29 15:01:27.169986: step 3260/14650 (epoch 12/50), acc = 0.81, loss = 0.73 (201.9 examples/sec; 0.495 sec/batch), lr: 0.008145\n",
      "2018-11-29 15:01:41.331073: step 3270/14650 (epoch 12/50), acc = 0.78, loss = 1.08 (199.9 examples/sec; 0.500 sec/batch), lr: 0.008145\n",
      "2018-11-29 15:01:55.853877: step 3280/14650 (epoch 12/50), acc = 0.83, loss = 0.88 (231.1 examples/sec; 0.433 sec/batch), lr: 0.008145\n",
      "2018-11-29 15:02:09.763793: step 3290/14650 (epoch 12/50), acc = 0.77, loss = 1.44 (201.5 examples/sec; 0.496 sec/batch), lr: 0.008145\n",
      "2018-11-29 15:02:24.012530: step 3300/14650 (epoch 12/50), acc = 0.82, loss = 0.84 (224.5 examples/sec; 0.445 sec/batch), lr: 0.008145\n",
      "2018-11-29 15:02:38.159449: step 3310/14650 (epoch 12/50), acc = 0.81, loss = 0.94 (201.6 examples/sec; 0.496 sec/batch), lr: 0.008145\n",
      "2018-11-29 15:02:49.440219: step 3318/14650 (epoch 12/50), Learning rate decays to 0.00774\n",
      "2018-11-29 15:02:52.466669: step 3320/14650 (epoch 12/50), acc = 0.82, loss = 1.09 (225.5 examples/sec; 0.444 sec/batch), lr: 0.007738\n",
      "2018-11-29 15:03:06.904731: step 3330/14650 (epoch 12/50), acc = 0.73, loss = 1.29 (223.7 examples/sec; 0.447 sec/batch), lr: 0.007738\n",
      "2018-11-29 15:03:21.190756: step 3340/14650 (epoch 12/50), acc = 0.72, loss = 0.97 (219.3 examples/sec; 0.456 sec/batch), lr: 0.007738\n",
      "2018-11-29 15:03:35.596014: step 3350/14650 (epoch 12/50), acc = 0.82, loss = 0.78 (222.8 examples/sec; 0.449 sec/batch), lr: 0.007738\n",
      "2018-11-29 15:03:50.463537: step 3360/14650 (epoch 12/50), acc = 0.77, loss = 0.87 (226.9 examples/sec; 0.441 sec/batch), lr: 0.007738\n",
      "2018-11-29 15:04:04.928517: step 3370/14650 (epoch 12/50), acc = 0.77, loss = 0.92 (211.6 examples/sec; 0.473 sec/batch), lr: 0.007738\n",
      "2018-11-29 15:04:19.378481: step 3380/14650 (epoch 12/50), acc = 0.74, loss = 0.95 (220.9 examples/sec; 0.453 sec/batch), lr: 0.007738\n",
      "2018-11-29 15:04:33.960945: step 3390/14650 (epoch 12/50), acc = 0.76, loss = 1.03 (209.3 examples/sec; 0.478 sec/batch), lr: 0.007738\n",
      "2018-11-29 15:04:48.906097: step 3400/14650 (epoch 12/50), acc = 0.81, loss = 0.74 (223.0 examples/sec; 0.448 sec/batch), lr: 0.007738\n",
      "\n",
      "Step 3400: train_loss = 0.959963, train_accuracy = 0.792\n",
      "Step 3400:  test_loss = 1.190428,  test_accuracy = 0.836\n",
      "\n",
      "2018-11-29 15:05:05.915655: step 3410/14650 (epoch 12/50), acc = 0.76, loss = 1.11 (221.9 examples/sec; 0.451 sec/batch), lr: 0.007738\n",
      "2018-11-29 15:05:20.882657: step 3420/14650 (epoch 12/50), acc = 0.79, loss = 0.88 (219.4 examples/sec; 0.456 sec/batch), lr: 0.007738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-29 15:05:35.793801: step 3430/14650 (epoch 12/50), acc = 0.82, loss = 1.47 (223.2 examples/sec; 0.448 sec/batch), lr: 0.007738\n",
      "2018-11-29 15:05:50.880130: step 3440/14650 (epoch 12/50), acc = 0.87, loss = 0.66 (220.2 examples/sec; 0.454 sec/batch), lr: 0.007738\n",
      "2018-11-29 15:06:05.810830: step 3450/14650 (epoch 12/50), acc = 0.81, loss = 0.89 (222.7 examples/sec; 0.449 sec/batch), lr: 0.007738\n",
      "2018-11-29 15:06:21.379250: step 3460/14650 (epoch 12/50), acc = 0.81, loss = 0.87 (214.2 examples/sec; 0.467 sec/batch), lr: 0.007738\n",
      "2018-11-29 15:06:36.621070: step 3470/14650 (epoch 12/50), acc = 0.77, loss = 0.93 (207.4 examples/sec; 0.482 sec/batch), lr: 0.007738\n",
      "2018-11-29 15:06:52.417409: step 3480/14650 (epoch 12/50), acc = 0.76, loss = 0.99 (209.4 examples/sec; 0.478 sec/batch), lr: 0.007738\n",
      "2018-11-29 15:07:08.446604: step 3490/14650 (epoch 12/50), acc = 0.79, loss = 0.94 (214.9 examples/sec; 0.465 sec/batch), lr: 0.007738\n",
      "2018-11-29 15:07:24.298220: step 3500/14650 (epoch 12/50), acc = 0.78, loss = 0.86 (215.9 examples/sec; 0.463 sec/batch), lr: 0.007738\n",
      "2018-11-29 15:07:40.116849: step 3510/14650 (epoch 12/50), acc = 0.82, loss = 0.85 (206.9 examples/sec; 0.483 sec/batch), lr: 0.007738\n",
      "2018-11-29 15:07:56.077247: step 3520/14650 (epoch 13/50), acc = 0.75, loss = 0.94 (211.3 examples/sec; 0.473 sec/batch), lr: 0.007738\n",
      "2018-11-29 15:08:12.408371: step 3530/14650 (epoch 13/50), acc = 0.86, loss = 0.70 (213.8 examples/sec; 0.468 sec/batch), lr: 0.007738\n",
      "2018-11-29 15:08:29.428953: step 3540/14650 (epoch 13/50), acc = 0.77, loss = 1.05 (209.3 examples/sec; 0.478 sec/batch), lr: 0.007738\n",
      "2018-11-29 15:08:45.118741: step 3550/14650 (epoch 13/50), acc = 0.74, loss = 1.12 (215.1 examples/sec; 0.465 sec/batch), lr: 0.007738\n",
      "2018-11-29 15:09:01.446986: step 3560/14650 (epoch 13/50), acc = 0.77, loss = 0.94 (122.4 examples/sec; 0.817 sec/batch), lr: 0.007738\n",
      "2018-11-29 15:09:17.387290: step 3570/14650 (epoch 13/50), acc = 0.80, loss = 0.82 (205.7 examples/sec; 0.486 sec/batch), lr: 0.007738\n",
      "2018-11-29 15:09:33.886011: step 3580/14650 (epoch 13/50), acc = 0.75, loss = 0.89 (124.4 examples/sec; 0.804 sec/batch), lr: 0.007738\n",
      "2018-11-29 15:09:49.994306: step 3590/14650 (epoch 13/50), acc = 0.82, loss = 0.89 (216.9 examples/sec; 0.461 sec/batch), lr: 0.007738\n",
      "2018-11-29 15:10:06.701807: step 3600/14650 (epoch 13/50), acc = 0.81, loss = 0.78 (214.9 examples/sec; 0.465 sec/batch), lr: 0.007738\n",
      "\n",
      "Step 3600: train_loss = 0.908720, train_accuracy = 0.799\n",
      "Step 3600:  test_loss = 0.987265,  test_accuracy = 0.837\n",
      "\n",
      "2018-11-29 15:10:26.020945: step 3610/14650 (epoch 13/50), acc = 0.80, loss = 0.82 (207.3 examples/sec; 0.482 sec/batch), lr: 0.007738\n",
      "2018-11-29 15:10:42.912161: step 3620/14650 (epoch 13/50), acc = 0.76, loss = 0.90 (211.2 examples/sec; 0.474 sec/batch), lr: 0.007738\n",
      "2018-11-29 15:10:59.218149: step 3630/14650 (epoch 13/50), acc = 0.75, loss = 0.96 (204.9 examples/sec; 0.488 sec/batch), lr: 0.007738\n",
      "2018-11-29 15:11:15.803590: step 3640/14650 (epoch 13/50), acc = 0.77, loss = 0.97 (201.4 examples/sec; 0.497 sec/batch), lr: 0.007738\n",
      "2018-11-29 15:11:35.888108: step 3650/14650 (epoch 13/50), acc = 0.75, loss = 1.19 (213.3 examples/sec; 0.469 sec/batch), lr: 0.007738\n",
      "2018-11-29 15:11:52.454549: step 3660/14650 (epoch 13/50), acc = 0.79, loss = 0.86 (207.0 examples/sec; 0.483 sec/batch), lr: 0.007738\n",
      "2018-11-29 15:12:08.988741: step 3670/14650 (epoch 13/50), acc = 0.81, loss = 0.73 (203.8 examples/sec; 0.491 sec/batch), lr: 0.007738\n",
      "2018-11-29 15:12:25.770334: step 3680/14650 (epoch 13/50), acc = 0.78, loss = 0.92 (125.1 examples/sec; 0.799 sec/batch), lr: 0.007738\n",
      "2018-11-29 15:12:42.621257: step 3690/14650 (epoch 13/50), acc = 0.83, loss = 0.84 (123.1 examples/sec; 0.813 sec/batch), lr: 0.007738\n",
      "2018-11-29 15:12:59.026224: step 3700/14650 (epoch 13/50), acc = 0.82, loss = 0.84 (204.5 examples/sec; 0.489 sec/batch), lr: 0.007738\n",
      "2018-11-29 15:13:15.699164: step 3710/14650 (epoch 13/50), acc = 0.81, loss = 0.92 (198.5 examples/sec; 0.504 sec/batch), lr: 0.007738\n",
      "2018-11-29 15:13:32.700624: step 3720/14650 (epoch 13/50), acc = 0.72, loss = 1.16 (212.3 examples/sec; 0.471 sec/batch), lr: 0.007738\n",
      "2018-11-29 15:13:49.868889: step 3730/14650 (epoch 13/50), acc = 0.86, loss = 0.66 (202.9 examples/sec; 0.493 sec/batch), lr: 0.007738\n",
      "2018-11-29 15:14:07.122438: step 3740/14650 (epoch 13/50), acc = 0.82, loss = 0.88 (211.8 examples/sec; 0.472 sec/batch), lr: 0.007738\n",
      "2018-11-29 15:14:24.411123: step 3750/14650 (epoch 13/50), acc = 0.79, loss = 0.86 (206.6 examples/sec; 0.484 sec/batch), lr: 0.007738\n",
      "2018-11-29 15:14:41.813271: step 3760/14650 (epoch 13/50), acc = 0.89, loss = 0.63 (202.2 examples/sec; 0.495 sec/batch), lr: 0.007738\n",
      "2018-11-29 15:14:59.503941: step 3770/14650 (epoch 13/50), acc = 0.82, loss = 0.87 (198.4 examples/sec; 0.504 sec/batch), lr: 0.007738\n",
      "2018-11-29 15:15:17.450654: step 3780/14650 (epoch 13/50), acc = 0.72, loss = 1.24 (133.1 examples/sec; 0.752 sec/batch), lr: 0.007738\n",
      "2018-11-29 15:15:35.093728: step 3790/14650 (epoch 13/50), acc = 0.82, loss = 0.75 (163.9 examples/sec; 0.610 sec/batch), lr: 0.007738\n",
      "2018-11-29 15:15:52.727699: step 3800/14650 (epoch 13/50), acc = 0.79, loss = 0.82 (199.9 examples/sec; 0.500 sec/batch), lr: 0.007738\n",
      "\n",
      "Step 3800: train_loss = 0.912551, train_accuracy = 0.801\n",
      "Step 3800:  test_loss = 1.040111,  test_accuracy = 0.839\n",
      "\n",
      "2018-11-29 15:16:12.892409: step 3810/14650 (epoch 14/50), acc = 0.79, loss = 0.92 (206.9 examples/sec; 0.483 sec/batch), lr: 0.007738\n",
      "2018-11-29 15:16:27.690993: step 3818/14650 (epoch 14/50), Learning rate decays to 0.00735\n",
      "2018-11-29 15:16:31.238452: step 3820/14650 (epoch 14/50), acc = 0.76, loss = 0.80 (198.5 examples/sec; 0.504 sec/batch), lr: 0.007351\n",
      "2018-11-29 15:16:48.929020: step 3830/14650 (epoch 14/50), acc = 0.83, loss = 0.85 (198.4 examples/sec; 0.504 sec/batch), lr: 0.007351\n",
      "2018-11-29 15:17:06.758829: step 3840/14650 (epoch 14/50), acc = 0.80, loss = 1.55 (207.0 examples/sec; 0.483 sec/batch), lr: 0.007351\n",
      "2018-11-29 15:17:24.986114: step 3850/14650 (epoch 14/50), acc = 0.74, loss = 1.02 (197.2 examples/sec; 0.507 sec/batch), lr: 0.007351\n",
      "2018-11-29 15:17:43.152851: step 3860/14650 (epoch 14/50), acc = 0.88, loss = 0.76 (198.1 examples/sec; 0.505 sec/batch), lr: 0.007351\n",
      "2018-11-29 15:18:01.576019: step 3870/14650 (epoch 14/50), acc = 0.83, loss = 0.79 (196.1 examples/sec; 0.510 sec/batch), lr: 0.007351\n",
      "2018-11-29 15:18:19.976181: step 3880/14650 (epoch 14/50), acc = 0.88, loss = 0.69 (151.3 examples/sec; 0.661 sec/batch), lr: 0.007351\n",
      "2018-11-29 15:18:38.094799: step 3890/14650 (epoch 14/50), acc = 0.86, loss = 0.61 (195.4 examples/sec; 0.512 sec/batch), lr: 0.007351\n",
      "2018-11-29 15:18:56.654524: step 3900/14650 (epoch 14/50), acc = 0.86, loss = 0.65 (195.0 examples/sec; 0.513 sec/batch), lr: 0.007351\n",
      "2018-11-29 15:19:15.584080: step 3910/14650 (epoch 14/50), acc = 0.82, loss = 0.96 (117.9 examples/sec; 0.848 sec/batch), lr: 0.007351\n",
      "2018-11-29 15:19:34.275837: step 3920/14650 (epoch 14/50), acc = 0.66, loss = 1.34 (195.2 examples/sec; 0.512 sec/batch), lr: 0.007351\n",
      "2018-11-29 15:19:53.269422: step 3930/14650 (epoch 14/50), acc = 0.80, loss = 1.10 (203.0 examples/sec; 0.493 sec/batch), lr: 0.007351\n",
      "2018-11-29 15:20:12.704080: step 3940/14650 (epoch 14/50), acc = 0.81, loss = 1.20 (147.6 examples/sec; 0.677 sec/batch), lr: 0.007351\n",
      "2018-11-29 15:20:31.764240: step 3950/14650 (epoch 14/50), acc = 0.81, loss = 0.95 (199.8 examples/sec; 0.501 sec/batch), lr: 0.007351\n",
      "2018-11-29 15:20:51.184345: step 3960/14650 (epoch 14/50), acc = 0.75, loss = 0.99 (193.0 examples/sec; 0.518 sec/batch), lr: 0.007351\n",
      "2018-11-29 15:21:10.656747: step 3970/14650 (epoch 14/50), acc = 0.83, loss = 0.73 (130.6 examples/sec; 0.766 sec/batch), lr: 0.007351\n",
      "2018-11-29 15:21:30.047343: step 3980/14650 (epoch 14/50), acc = 0.81, loss = 0.82 (199.0 examples/sec; 0.503 sec/batch), lr: 0.007351\n",
      "2018-11-29 15:21:49.525789: step 3990/14650 (epoch 14/50), acc = 0.78, loss = 0.90 (192.1 examples/sec; 0.521 sec/batch), lr: 0.007351\n",
      "2018-11-29 15:22:09.210680: step 4000/14650 (epoch 14/50), acc = 0.77, loss = 0.99 (147.2 examples/sec; 0.680 sec/batch), lr: 0.007351\n",
      "\n",
      "Step 4000: train_loss = 0.931969, train_accuracy = 0.799\n",
      "Step 4000:  test_loss = 1.306169,  test_accuracy = 0.848\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-29 15:22:30.984530: step 4010/14650 (epoch 14/50), acc = 0.82, loss = 0.88 (190.7 examples/sec; 0.524 sec/batch), lr: 0.007351\n",
      "2018-11-29 15:22:50.819163: step 4020/14650 (epoch 14/50), acc = 0.79, loss = 1.02 (192.1 examples/sec; 0.521 sec/batch), lr: 0.007351\n",
      "2018-11-29 15:23:10.767665: step 4030/14650 (epoch 14/50), acc = 0.83, loss = 0.76 (123.5 examples/sec; 0.810 sec/batch), lr: 0.007351\n",
      "2018-11-29 15:23:30.347924: step 4040/14650 (epoch 14/50), acc = 0.74, loss = 1.10 (189.2 examples/sec; 0.529 sec/batch), lr: 0.007351\n",
      "2018-11-29 15:23:50.363943: step 4050/14650 (epoch 14/50), acc = 0.81, loss = 0.90 (188.4 examples/sec; 0.531 sec/batch), lr: 0.007351\n",
      "2018-11-29 15:24:10.331124: step 4060/14650 (epoch 14/50), acc = 0.85, loss = 0.87 (191.8 examples/sec; 0.521 sec/batch), lr: 0.007351\n",
      "2018-11-29 15:24:29.935164: step 4070/14650 (epoch 14/50), acc = 0.78, loss = 1.01 (188.4 examples/sec; 0.531 sec/batch), lr: 0.007351\n",
      "2018-11-29 15:24:50.318157: step 4080/14650 (epoch 14/50), acc = 0.76, loss = 0.98 (196.3 examples/sec; 0.509 sec/batch), lr: 0.007351\n",
      "2018-11-29 15:25:10.925638: step 4090/14650 (epoch 14/50), acc = 0.83, loss = 0.67 (195.5 examples/sec; 0.512 sec/batch), lr: 0.007351\n",
      "2018-11-29 15:25:32.083914: step 4100/14650 (epoch 14/50), acc = 0.89, loss = 0.69 (115.6 examples/sec; 0.865 sec/batch), lr: 0.007351\n",
      "2018-11-29 15:25:52.243944: step 4110/14650 (epoch 15/50), acc = 0.84, loss = 0.69 (188.2 examples/sec; 0.531 sec/batch), lr: 0.007351\n",
      "2018-11-29 15:26:13.565928: step 4120/14650 (epoch 15/50), acc = 0.86, loss = 0.83 (194.8 examples/sec; 0.513 sec/batch), lr: 0.007351\n",
      "2018-11-29 15:26:34.655208: step 4130/14650 (epoch 15/50), acc = 0.86, loss = 0.71 (186.4 examples/sec; 0.536 sec/batch), lr: 0.007351\n",
      "2018-11-29 15:26:55.974893: step 4140/14650 (epoch 15/50), acc = 0.82, loss = 0.82 (110.7 examples/sec; 0.904 sec/batch), lr: 0.007351\n",
      "2018-11-29 15:27:16.803883: step 4150/14650 (epoch 15/50), acc = 0.80, loss = 0.73 (190.0 examples/sec; 0.526 sec/batch), lr: 0.007351\n",
      "2018-11-29 15:27:37.911828: step 4160/14650 (epoch 15/50), acc = 0.75, loss = 0.96 (194.2 examples/sec; 0.515 sec/batch), lr: 0.007351\n",
      "2018-11-29 15:27:59.345548: step 4170/14650 (epoch 15/50), acc = 0.82, loss = 0.75 (114.8 examples/sec; 0.871 sec/batch), lr: 0.007351\n",
      "2018-11-29 15:28:20.448921: step 4180/14650 (epoch 15/50), acc = 0.84, loss = 0.67 (192.8 examples/sec; 0.519 sec/batch), lr: 0.007351\n",
      "2018-11-29 15:28:41.650585: step 4190/14650 (epoch 15/50), acc = 0.84, loss = 0.69 (186.9 examples/sec; 0.535 sec/batch), lr: 0.007351\n",
      "2018-11-29 15:29:03.067676: step 4200/14650 (epoch 15/50), acc = 0.81, loss = 1.12 (192.4 examples/sec; 0.520 sec/batch), lr: 0.007351\n",
      "\n",
      "Step 4200: train_loss = 0.895112, train_accuracy = 0.807\n",
      "Step 4200:  test_loss = 1.034449,  test_accuracy = 0.837\n",
      "\n",
      "2018-11-29 15:29:27.086936: step 4210/14650 (epoch 15/50), acc = 0.85, loss = 0.68 (189.2 examples/sec; 0.528 sec/batch), lr: 0.007351\n",
      "2018-11-29 15:29:49.209260: step 4220/14650 (epoch 15/50), acc = 0.77, loss = 1.03 (192.1 examples/sec; 0.521 sec/batch), lr: 0.007351\n",
      "2018-11-29 15:30:10.956405: step 4230/14650 (epoch 15/50), acc = 0.78, loss = 0.93 (133.6 examples/sec; 0.749 sec/batch), lr: 0.007351\n",
      "2018-11-29 15:30:32.467117: step 4240/14650 (epoch 15/50), acc = 0.77, loss = 1.15 (187.4 examples/sec; 0.534 sec/batch), lr: 0.007351\n",
      "2018-11-29 15:30:54.284844: step 4250/14650 (epoch 15/50), acc = 0.73, loss = 0.97 (113.3 examples/sec; 0.882 sec/batch), lr: 0.007351\n",
      "2018-11-29 15:31:15.747080: step 4260/14650 (epoch 15/50), acc = 0.80, loss = 0.86 (190.4 examples/sec; 0.525 sec/batch), lr: 0.007351\n",
      "2018-11-29 15:31:37.989292: step 4270/14650 (epoch 15/50), acc = 0.74, loss = 1.12 (179.4 examples/sec; 0.558 sec/batch), lr: 0.007351\n",
      "2018-11-29 15:31:59.943974: step 4280/14650 (epoch 15/50), acc = 0.81, loss = 0.84 (118.9 examples/sec; 0.841 sec/batch), lr: 0.007351\n",
      "2018-11-29 15:32:21.795156: step 4290/14650 (epoch 15/50), acc = 0.88, loss = 0.61 (187.6 examples/sec; 0.533 sec/batch), lr: 0.007351\n",
      "2018-11-29 15:32:44.534619: step 4300/14650 (epoch 15/50), acc = 0.76, loss = 0.92 (180.3 examples/sec; 0.555 sec/batch), lr: 0.007351\n",
      "2018-11-29 15:33:07.281371: step 4310/14650 (epoch 15/50), acc = 0.82, loss = 0.87 (178.5 examples/sec; 0.560 sec/batch), lr: 0.007351\n",
      "2018-11-29 15:33:25.310265: step 4318/14650 (epoch 15/50), Learning rate decays to 0.00698\n",
      "2018-11-29 15:33:29.974448: step 4320/14650 (epoch 15/50), acc = 0.77, loss = 0.94 (186.5 examples/sec; 0.536 sec/batch), lr: 0.006983\n",
      "2018-11-29 15:33:52.932704: step 4330/14650 (epoch 15/50), acc = 0.75, loss = 1.14 (125.5 examples/sec; 0.797 sec/batch), lr: 0.006983\n",
      "2018-11-29 15:34:15.770839: step 4340/14650 (epoch 15/50), acc = 0.87, loss = 0.79 (136.3 examples/sec; 0.733 sec/batch), lr: 0.006983\n",
      "2018-11-29 15:34:38.595171: step 4350/14650 (epoch 15/50), acc = 0.83, loss = 0.87 (141.4 examples/sec; 0.707 sec/batch), lr: 0.006983\n",
      "2018-11-29 15:35:01.576735: step 4360/14650 (epoch 15/50), acc = 0.84, loss = 1.03 (123.0 examples/sec; 0.813 sec/batch), lr: 0.006983\n",
      "2018-11-29 15:35:24.616165: step 4370/14650 (epoch 15/50), acc = 0.82, loss = 0.81 (137.9 examples/sec; 0.725 sec/batch), lr: 0.006983\n",
      "2018-11-29 15:35:47.948770: step 4380/14650 (epoch 15/50), acc = 0.81, loss = 0.82 (179.5 examples/sec; 0.557 sec/batch), lr: 0.006983\n",
      "2018-11-29 15:36:10.905424: step 4390/14650 (epoch 15/50), acc = 0.84, loss = 0.79 (157.9 examples/sec; 0.633 sec/batch), lr: 0.006983\n",
      "2018-11-29 15:36:33.533478: step 4400/14650 (epoch 16/50), acc = 0.77, loss = 1.27 (176.7 examples/sec; 0.566 sec/batch), lr: 0.006983\n",
      "\n",
      "Step 4400: train_loss = 0.950129, train_accuracy = 0.798\n",
      "Step 4400:  test_loss = 1.038291,  test_accuracy = 0.839\n",
      "\n",
      "2018-11-29 15:37:00.171294: step 4410/14650 (epoch 16/50), acc = 0.86, loss = 0.64 (176.3 examples/sec; 0.567 sec/batch), lr: 0.006983\n",
      "2018-11-29 15:37:23.756894: step 4420/14650 (epoch 16/50), acc = 0.84, loss = 0.79 (182.4 examples/sec; 0.548 sec/batch), lr: 0.006983\n",
      "2018-11-29 15:37:47.233370: step 4430/14650 (epoch 16/50), acc = 0.79, loss = 0.86 (175.2 examples/sec; 0.571 sec/batch), lr: 0.006983\n",
      "2018-11-29 15:38:11.625160: step 4440/14650 (epoch 16/50), acc = 0.70, loss = 1.19 (103.7 examples/sec; 0.964 sec/batch), lr: 0.006983\n",
      "2018-11-29 15:38:35.511671: step 4450/14650 (epoch 16/50), acc = 0.75, loss = 1.06 (182.3 examples/sec; 0.549 sec/batch), lr: 0.006983\n",
      "2018-11-29 15:39:00.167775: step 4460/14650 (epoch 16/50), acc = 0.77, loss = 1.20 (103.9 examples/sec; 0.963 sec/batch), lr: 0.006983\n",
      "2018-11-29 15:39:25.180198: step 4470/14650 (epoch 16/50), acc = 0.88, loss = 0.65 (103.4 examples/sec; 0.967 sec/batch), lr: 0.006983\n",
      "2018-11-29 15:39:50.253728: step 4480/14650 (epoch 16/50), acc = 0.81, loss = 1.42 (102.5 examples/sec; 0.975 sec/batch), lr: 0.006983\n",
      "2018-11-29 15:40:15.326653: step 4490/14650 (epoch 16/50), acc = 0.84, loss = 0.73 (102.2 examples/sec; 0.978 sec/batch), lr: 0.006983\n",
      "2018-11-29 15:40:40.107039: step 4500/14650 (epoch 16/50), acc = 0.88, loss = 0.65 (172.6 examples/sec; 0.579 sec/batch), lr: 0.006983\n",
      "2018-11-29 15:41:05.415397: step 4510/14650 (epoch 16/50), acc = 0.76, loss = 1.19 (103.5 examples/sec; 0.966 sec/batch), lr: 0.006983\n",
      "2018-11-29 15:41:29.939021: step 4520/14650 (epoch 16/50), acc = 0.79, loss = 0.74 (101.4 examples/sec; 0.986 sec/batch), lr: 0.006983\n",
      "2018-11-29 15:41:54.660460: step 4530/14650 (epoch 16/50), acc = 0.82, loss = 0.85 (145.0 examples/sec; 0.689 sec/batch), lr: 0.006983\n",
      "2018-11-29 15:42:19.136075: step 4540/14650 (epoch 16/50), acc = 0.75, loss = 1.32 (121.3 examples/sec; 0.824 sec/batch), lr: 0.006983\n",
      "2018-11-29 15:42:43.864894: step 4550/14650 (epoch 16/50), acc = 0.75, loss = 0.86 (172.1 examples/sec; 0.581 sec/batch), lr: 0.006983\n",
      "2018-11-29 15:43:08.554328: step 4560/14650 (epoch 16/50), acc = 0.77, loss = 0.95 (135.5 examples/sec; 0.738 sec/batch), lr: 0.006983\n",
      "2018-11-29 15:43:33.595147: step 4570/14650 (epoch 16/50), acc = 0.84, loss = 0.83 (103.4 examples/sec; 0.967 sec/batch), lr: 0.006983\n",
      "2018-11-29 15:43:58.873852: step 4580/14650 (epoch 16/50), acc = 0.82, loss = 0.83 (102.3 examples/sec; 0.978 sec/batch), lr: 0.006983\n",
      "2018-11-29 15:44:23.451903: step 4590/14650 (epoch 16/50), acc = 0.76, loss = 0.99 (113.0 examples/sec; 0.885 sec/batch), lr: 0.006983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-29 15:44:50.467994: step 4600/14650 (epoch 16/50), acc = 0.84, loss = 0.79 (101.1 examples/sec; 0.989 sec/batch), lr: 0.006983\n",
      "\n",
      "Step 4600: train_loss = 0.923114, train_accuracy = 0.805\n",
      "Step 4600:  test_loss = 0.966694,  test_accuracy = 0.845\n",
      "\n",
      "2018-11-29 15:45:18.437794: step 4610/14650 (epoch 16/50), acc = 0.79, loss = 1.10 (104.2 examples/sec; 0.959 sec/batch), lr: 0.006983\n",
      "2018-11-29 15:45:44.310076: step 4620/14650 (epoch 16/50), acc = 0.83, loss = 0.82 (103.0 examples/sec; 0.971 sec/batch), lr: 0.006983\n",
      "2018-11-29 15:46:10.717627: step 4630/14650 (epoch 16/50), acc = 0.78, loss = 1.05 (109.0 examples/sec; 0.918 sec/batch), lr: 0.006983\n",
      "2018-11-29 15:46:36.363573: step 4640/14650 (epoch 16/50), acc = 0.84, loss = 0.83 (102.7 examples/sec; 0.974 sec/batch), lr: 0.006983\n",
      "2018-11-29 15:47:02.649613: step 4650/14650 (epoch 16/50), acc = 0.79, loss = 0.78 (112.4 examples/sec; 0.890 sec/batch), lr: 0.006983\n",
      "2018-11-29 15:47:28.437390: step 4660/14650 (epoch 16/50), acc = 0.81, loss = 0.78 (106.5 examples/sec; 0.939 sec/batch), lr: 0.006983\n",
      "2018-11-29 15:47:54.503550: step 4670/14650 (epoch 16/50), acc = 0.85, loss = 0.81 (105.5 examples/sec; 0.948 sec/batch), lr: 0.006983\n",
      "2018-11-29 15:48:20.176547: step 4680/14650 (epoch 16/50), acc = 0.85, loss = 0.70 (103.2 examples/sec; 0.969 sec/batch), lr: 0.006983\n",
      "2018-11-29 15:48:46.420396: step 4690/14650 (epoch 17/50), acc = 0.90, loss = 0.61 (100.3 examples/sec; 0.997 sec/batch), lr: 0.006983\n",
      "2018-11-29 15:49:12.847990: step 4700/14650 (epoch 17/50), acc = 0.78, loss = 0.93 (115.1 examples/sec; 0.869 sec/batch), lr: 0.006983\n",
      "2018-11-29 15:49:38.903578: step 4710/14650 (epoch 17/50), acc = 0.82, loss = 0.73 (98.4 examples/sec; 1.016 sec/batch), lr: 0.006983\n",
      "2018-11-29 15:50:05.047773: step 4720/14650 (epoch 17/50), acc = 0.83, loss = 0.72 (106.5 examples/sec; 0.939 sec/batch), lr: 0.006983\n",
      "2018-11-29 15:50:31.600421: step 4730/14650 (epoch 17/50), acc = 0.80, loss = 0.69 (105.9 examples/sec; 0.945 sec/batch), lr: 0.006983\n",
      "2018-11-29 15:50:58.028070: step 4740/14650 (epoch 17/50), acc = 0.79, loss = 1.05 (110.6 examples/sec; 0.904 sec/batch), lr: 0.006983\n",
      "2018-11-29 15:51:24.822079: step 4750/14650 (epoch 17/50), acc = 0.82, loss = 1.00 (113.5 examples/sec; 0.881 sec/batch), lr: 0.006983\n",
      "2018-11-29 15:51:52.012685: step 4760/14650 (epoch 17/50), acc = 0.81, loss = 0.72 (107.2 examples/sec; 0.933 sec/batch), lr: 0.006983\n",
      "2018-11-29 15:52:18.831233: step 4770/14650 (epoch 17/50), acc = 0.84, loss = 0.71 (105.5 examples/sec; 0.947 sec/batch), lr: 0.006983\n",
      "2018-11-29 15:52:45.612924: step 4780/14650 (epoch 17/50), acc = 0.82, loss = 1.22 (119.4 examples/sec; 0.837 sec/batch), lr: 0.006983\n",
      "2018-11-29 15:53:12.613342: step 4790/14650 (epoch 17/50), acc = 0.83, loss = 0.84 (105.4 examples/sec; 0.949 sec/batch), lr: 0.006983\n",
      "2018-11-29 15:53:39.937247: step 4800/14650 (epoch 17/50), acc = 0.77, loss = 1.01 (118.3 examples/sec; 0.846 sec/batch), lr: 0.006983\n",
      "\n",
      "Step 4800: train_loss = 0.900337, train_accuracy = 0.807\n",
      "Step 4800:  test_loss = 0.978773,  test_accuracy = 0.836\n",
      "\n",
      "2018-11-29 15:54:09.903142: step 4810/14650 (epoch 17/50), acc = 0.89, loss = 0.77 (119.7 examples/sec; 0.835 sec/batch), lr: 0.006983\n",
      "2018-11-29 15:54:31.823611: step 4818/14650 (epoch 17/50), Learning rate decays to 0.00663\n",
      "2018-11-29 15:54:37.369610: step 4820/14650 (epoch 17/50), acc = 0.69, loss = 1.16 (126.0 examples/sec; 0.793 sec/batch), lr: 0.006634\n",
      "2018-11-29 15:55:04.780376: step 4830/14650 (epoch 17/50), acc = 0.79, loss = 1.00 (115.1 examples/sec; 0.869 sec/batch), lr: 0.006634\n",
      "2018-11-29 15:55:32.163192: step 4840/14650 (epoch 17/50), acc = 0.79, loss = 0.87 (109.7 examples/sec; 0.911 sec/batch), lr: 0.006634\n",
      "2018-11-29 15:56:00.052990: step 4850/14650 (epoch 17/50), acc = 0.81, loss = 0.73 (126.9 examples/sec; 0.788 sec/batch), lr: 0.006634\n",
      "2018-11-29 15:56:28.188411: step 4860/14650 (epoch 17/50), acc = 0.83, loss = 0.95 (126.8 examples/sec; 0.789 sec/batch), lr: 0.006634\n",
      "2018-11-29 15:56:55.917911: step 4870/14650 (epoch 17/50), acc = 0.79, loss = 0.93 (110.2 examples/sec; 0.907 sec/batch), lr: 0.006634\n",
      "2018-11-29 15:57:23.968112: step 4880/14650 (epoch 17/50), acc = 0.84, loss = 0.71 (120.7 examples/sec; 0.828 sec/batch), lr: 0.006634\n",
      "2018-11-29 15:57:52.122535: step 4890/14650 (epoch 17/50), acc = 0.78, loss = 0.90 (124.6 examples/sec; 0.802 sec/batch), lr: 0.006634\n",
      "2018-11-29 15:58:20.526363: step 4900/14650 (epoch 17/50), acc = 0.83, loss = 0.92 (124.6 examples/sec; 0.802 sec/batch), lr: 0.006634\n",
      "2018-11-29 15:58:48.847555: step 4910/14650 (epoch 17/50), acc = 0.83, loss = 0.68 (127.5 examples/sec; 0.784 sec/batch), lr: 0.006634\n",
      "2018-11-29 15:59:17.036756: step 4920/14650 (epoch 17/50), acc = 0.82, loss = 0.74 (124.2 examples/sec; 0.805 sec/batch), lr: 0.006634\n",
      "2018-11-29 15:59:44.966261: step 4930/14650 (epoch 17/50), acc = 0.75, loss = 0.83 (125.2 examples/sec; 0.798 sec/batch), lr: 0.006634\n",
      "2018-11-29 16:00:13.329424: step 4940/14650 (epoch 17/50), acc = 0.84, loss = 0.82 (140.8 examples/sec; 0.710 sec/batch), lr: 0.006634\n",
      "2018-11-29 16:00:42.170458: step 4950/14650 (epoch 17/50), acc = 0.89, loss = 0.58 (124.0 examples/sec; 0.807 sec/batch), lr: 0.006634\n",
      "2018-11-29 16:01:10.871896: step 4960/14650 (epoch 17/50), acc = 0.83, loss = 0.77 (118.4 examples/sec; 0.845 sec/batch), lr: 0.006634\n",
      "2018-11-29 16:01:40.043239: step 4970/14650 (epoch 17/50), acc = 0.81, loss = 0.72 (150.8 examples/sec; 0.663 sec/batch), lr: 0.006634\n",
      "2018-11-29 16:02:08.846818: step 4980/14650 (epoch 17/50), acc = 0.74, loss = 1.17 (131.6 examples/sec; 0.760 sec/batch), lr: 0.006634\n",
      "2018-11-29 16:02:37.720858: step 4990/14650 (epoch 18/50), acc = 0.77, loss = 0.87 (134.9 examples/sec; 0.742 sec/batch), lr: 0.006634\n",
      "2018-11-29 16:03:06.406630: step 5000/14650 (epoch 18/50), acc = 0.83, loss = 0.68 (130.2 examples/sec; 0.768 sec/batch), lr: 0.006634\n",
      "\n",
      "Step 5000: train_loss = 0.847713, train_accuracy = 0.807\n",
      "Step 5000:  test_loss = 1.007545,  test_accuracy = 0.852\n",
      "\n",
      "2018-11-29 16:03:38.169635: step 5010/14650 (epoch 18/50), acc = 0.80, loss = 0.89 (140.9 examples/sec; 0.710 sec/batch), lr: 0.006634\n",
      "2018-11-29 16:04:06.688263: step 5020/14650 (epoch 18/50), acc = 0.80, loss = 0.80 (125.5 examples/sec; 0.797 sec/batch), lr: 0.006634\n",
      "2018-11-29 16:04:37.008263: step 5030/14650 (epoch 18/50), acc = 0.90, loss = 0.54 (158.9 examples/sec; 0.629 sec/batch), lr: 0.006634\n",
      "2018-11-29 16:05:05.874216: step 5040/14650 (epoch 18/50), acc = 0.82, loss = 0.89 (121.5 examples/sec; 0.823 sec/batch), lr: 0.006634\n",
      "2018-11-29 16:05:34.770976: step 5050/14650 (epoch 18/50), acc = 0.85, loss = 1.34 (133.2 examples/sec; 0.751 sec/batch), lr: 0.006634\n",
      "2018-11-29 16:06:03.868724: step 5060/14650 (epoch 18/50), acc = 0.88, loss = 0.64 (132.8 examples/sec; 0.753 sec/batch), lr: 0.006634\n",
      "2018-11-29 16:06:32.924834: step 5070/14650 (epoch 18/50), acc = 0.87, loss = 0.97 (148.6 examples/sec; 0.673 sec/batch), lr: 0.006634\n",
      "2018-11-29 16:07:02.668078: step 5080/14650 (epoch 18/50), acc = 0.80, loss = 0.88 (130.2 examples/sec; 0.768 sec/batch), lr: 0.006634\n",
      "2018-11-29 16:07:32.147498: step 5090/14650 (epoch 18/50), acc = 0.77, loss = 0.90 (132.3 examples/sec; 0.756 sec/batch), lr: 0.006634\n",
      "2018-11-29 16:08:01.678084: step 5100/14650 (epoch 18/50), acc = 0.82, loss = 0.69 (115.4 examples/sec; 0.867 sec/batch), lr: 0.006634\n",
      "2018-11-29 16:08:31.301210: step 5110/14650 (epoch 18/50), acc = 0.73, loss = 1.05 (125.4 examples/sec; 0.798 sec/batch), lr: 0.006634\n",
      "2018-11-29 16:09:01.233031: step 5120/14650 (epoch 18/50), acc = 0.90, loss = 0.63 (124.8 examples/sec; 0.801 sec/batch), lr: 0.006634\n",
      "2018-11-29 16:09:31.057860: step 5130/14650 (epoch 18/50), acc = 0.80, loss = 0.87 (124.2 examples/sec; 0.805 sec/batch), lr: 0.006634\n",
      "2018-11-29 16:10:01.006285: step 5140/14650 (epoch 18/50), acc = 0.81, loss = 0.73 (129.8 examples/sec; 0.771 sec/batch), lr: 0.006634\n",
      "2018-11-29 16:10:30.994236: step 5150/14650 (epoch 18/50), acc = 0.72, loss = 0.95 (132.5 examples/sec; 0.755 sec/batch), lr: 0.006634\n",
      "2018-11-29 16:11:00.923991: step 5160/14650 (epoch 18/50), acc = 0.81, loss = 0.87 (124.1 examples/sec; 0.806 sec/batch), lr: 0.006634\n",
      "2018-11-29 16:11:31.123148: step 5170/14650 (epoch 18/50), acc = 0.76, loss = 1.02 (122.6 examples/sec; 0.815 sec/batch), lr: 0.006634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-29 16:12:01.428020: step 5180/14650 (epoch 18/50), acc = 0.77, loss = 1.03 (122.6 examples/sec; 0.816 sec/batch), lr: 0.006634\n",
      "2018-11-29 16:12:31.785349: step 5190/14650 (epoch 18/50), acc = 0.77, loss = 1.08 (134.3 examples/sec; 0.745 sec/batch), lr: 0.006634\n",
      "2018-11-29 16:13:01.968078: step 5200/14650 (epoch 18/50), acc = 0.82, loss = 0.92 (131.5 examples/sec; 0.760 sec/batch), lr: 0.006634\n",
      "\n",
      "Step 5200: train_loss = 0.856585, train_accuracy = 0.811\n",
      "Step 5200:  test_loss = 1.067392,  test_accuracy = 0.837\n",
      "\n",
      "2018-11-29 16:13:35.135709: step 5210/14650 (epoch 18/50), acc = 0.82, loss = 0.81 (145.2 examples/sec; 0.689 sec/batch), lr: 0.006634\n",
      "2018-11-29 16:14:05.736934: step 5220/14650 (epoch 18/50), acc = 0.79, loss = 0.94 (146.7 examples/sec; 0.682 sec/batch), lr: 0.006634\n",
      "2018-11-29 16:14:36.462374: step 5230/14650 (epoch 18/50), acc = 0.86, loss = 0.60 (131.0 examples/sec; 0.763 sec/batch), lr: 0.006634\n",
      "2018-11-29 16:15:07.418765: step 5240/14650 (epoch 18/50), acc = 0.80, loss = 1.20 (157.3 examples/sec; 0.636 sec/batch), lr: 0.006634\n",
      "2018-11-29 16:15:38.226162: step 5250/14650 (epoch 18/50), acc = 0.84, loss = 0.83 (141.6 examples/sec; 0.706 sec/batch), lr: 0.006634\n",
      "2018-11-29 16:16:09.247170: step 5260/14650 (epoch 18/50), acc = 0.76, loss = 0.92 (139.7 examples/sec; 0.716 sec/batch), lr: 0.006634\n",
      "2018-11-29 16:16:40.284106: step 5270/14650 (epoch 18/50), acc = 0.77, loss = 0.86 (152.2 examples/sec; 0.657 sec/batch), lr: 0.006634\n",
      "2018-11-29 16:17:11.777538: step 5280/14650 (epoch 19/50), acc = 0.86, loss = 0.66 (151.5 examples/sec; 0.660 sec/batch), lr: 0.006634\n",
      "2018-11-29 16:17:43.219505: step 5290/14650 (epoch 19/50), acc = 0.73, loss = 0.89 (142.1 examples/sec; 0.704 sec/batch), lr: 0.006634\n",
      "2018-11-29 16:18:14.566723: step 5300/14650 (epoch 19/50), acc = 0.80, loss = 1.66 (152.3 examples/sec; 0.656 sec/batch), lr: 0.006634\n",
      "2018-11-29 16:18:45.996348: step 5310/14650 (epoch 19/50), acc = 0.81, loss = 0.71 (147.8 examples/sec; 0.677 sec/batch), lr: 0.006634\n",
      "2018-11-29 16:19:11.313284: step 5318/14650 (epoch 19/50), Learning rate decays to 0.00630\n",
      "2018-11-29 16:19:17.695582: step 5320/14650 (epoch 19/50), acc = 0.84, loss = 0.78 (155.0 examples/sec; 0.645 sec/batch), lr: 0.006302\n",
      "2018-11-29 16:19:49.327081: step 5330/14650 (epoch 19/50), acc = 0.86, loss = 0.87 (150.3 examples/sec; 0.665 sec/batch), lr: 0.006302\n",
      "2018-11-29 16:20:20.921129: step 5340/14650 (epoch 19/50), acc = 0.83, loss = 0.73 (154.9 examples/sec; 0.646 sec/batch), lr: 0.006302\n",
      "2018-11-29 16:20:52.957468: step 5350/14650 (epoch 19/50), acc = 0.79, loss = 0.92 (149.9 examples/sec; 0.667 sec/batch), lr: 0.006302\n",
      "2018-11-29 16:21:25.224503: step 5360/14650 (epoch 19/50), acc = 0.80, loss = 0.86 (148.5 examples/sec; 0.673 sec/batch), lr: 0.006302\n",
      "2018-11-29 16:21:57.559378: step 5370/14650 (epoch 19/50), acc = 0.77, loss = 0.87 (149.5 examples/sec; 0.669 sec/batch), lr: 0.006302\n",
      "2018-11-29 16:22:30.434106: step 5380/14650 (epoch 19/50), acc = 0.84, loss = 0.94 (151.9 examples/sec; 0.658 sec/batch), lr: 0.006302\n",
      "2018-11-29 16:23:02.836281: step 5390/14650 (epoch 19/50), acc = 0.80, loss = 1.11 (153.1 examples/sec; 0.653 sec/batch), lr: 0.006302\n",
      "2018-11-29 16:23:35.134308: step 5400/14650 (epoch 19/50), acc = 0.82, loss = 0.83 (153.3 examples/sec; 0.652 sec/batch), lr: 0.006302\n",
      "\n",
      "Step 5400: train_loss = 0.874307, train_accuracy = 0.810\n",
      "Step 5400:  test_loss = 0.998702,  test_accuracy = 0.846\n",
      "\n",
      "2018-11-29 16:24:10.016584: step 5410/14650 (epoch 19/50), acc = 0.78, loss = 0.76 (148.1 examples/sec; 0.675 sec/batch), lr: 0.006302\n",
      "2018-11-29 16:24:42.964453: step 5420/14650 (epoch 19/50), acc = 0.82, loss = 0.87 (152.9 examples/sec; 0.654 sec/batch), lr: 0.006302\n",
      "2018-11-29 16:25:15.393037: step 5430/14650 (epoch 19/50), acc = 0.77, loss = 1.07 (147.4 examples/sec; 0.678 sec/batch), lr: 0.006302\n",
      "2018-11-29 16:25:47.883534: step 5440/14650 (epoch 19/50), acc = 0.78, loss = 0.81 (147.1 examples/sec; 0.680 sec/batch), lr: 0.006302\n",
      "2018-11-29 16:26:21.274224: step 5450/14650 (epoch 19/50), acc = 0.82, loss = 0.85 (146.4 examples/sec; 0.683 sec/batch), lr: 0.006302\n",
      "2018-11-29 16:26:54.458132: step 5460/14650 (epoch 19/50), acc = 0.82, loss = 0.86 (85.2 examples/sec; 1.173 sec/batch), lr: 0.006302\n",
      "2018-11-29 16:27:27.852972: step 5470/14650 (epoch 19/50), acc = 0.81, loss = 1.07 (145.0 examples/sec; 0.690 sec/batch), lr: 0.006302\n",
      "2018-11-29 16:28:01.390277: step 5480/14650 (epoch 19/50), acc = 0.83, loss = 0.93 (144.4 examples/sec; 0.692 sec/batch), lr: 0.006302\n",
      "2018-11-29 16:28:35.372973: step 5490/14650 (epoch 19/50), acc = 0.79, loss = 0.95 (148.2 examples/sec; 0.675 sec/batch), lr: 0.006302\n",
      "2018-11-29 16:29:08.932489: step 5500/14650 (epoch 19/50), acc = 0.80, loss = 1.01 (146.5 examples/sec; 0.683 sec/batch), lr: 0.006302\n",
      "2018-11-29 16:29:42.832764: step 5510/14650 (epoch 19/50), acc = 0.84, loss = 0.72 (149.8 examples/sec; 0.668 sec/batch), lr: 0.006302\n",
      "2018-11-29 16:30:16.189095: step 5520/14650 (epoch 19/50), acc = 0.77, loss = 0.84 (149.2 examples/sec; 0.670 sec/batch), lr: 0.006302\n",
      "2018-11-29 16:30:50.367250: step 5530/14650 (epoch 19/50), acc = 0.78, loss = 0.89 (136.6 examples/sec; 0.732 sec/batch), lr: 0.006302\n",
      "2018-11-29 16:31:23.940633: step 5540/14650 (epoch 19/50), acc = 0.86, loss = 0.88 (150.0 examples/sec; 0.667 sec/batch), lr: 0.006302\n",
      "2018-11-29 16:31:58.263981: step 5550/14650 (epoch 19/50), acc = 0.87, loss = 0.66 (145.6 examples/sec; 0.687 sec/batch), lr: 0.006302\n",
      "2018-11-29 16:32:32.368079: step 5560/14650 (epoch 19/50), acc = 0.91, loss = 0.62 (143.3 examples/sec; 0.698 sec/batch), lr: 0.006302\n",
      "2018-11-29 16:33:07.438987: step 5570/14650 (epoch 20/50), acc = 0.79, loss = 1.00 (149.2 examples/sec; 0.670 sec/batch), lr: 0.006302\n",
      "2018-11-29 16:33:42.070745: step 5580/14650 (epoch 20/50), acc = 0.83, loss = 0.83 (143.9 examples/sec; 0.695 sec/batch), lr: 0.006302\n",
      "2018-11-29 16:34:16.687760: step 5590/14650 (epoch 20/50), acc = 0.78, loss = 0.92 (142.9 examples/sec; 0.700 sec/batch), lr: 0.006302\n",
      "2018-11-29 16:34:51.747486: step 5600/14650 (epoch 20/50), acc = 0.90, loss = 0.66 (148.1 examples/sec; 0.675 sec/batch), lr: 0.006302\n",
      "\n",
      "Step 5600: train_loss = 0.792481, train_accuracy = 0.828\n",
      "Step 5600:  test_loss = 1.006732,  test_accuracy = 0.848\n",
      "\n",
      "2018-11-29 16:35:30.123979: step 5610/14650 (epoch 20/50), acc = 0.85, loss = 0.86 (147.7 examples/sec; 0.677 sec/batch), lr: 0.006302\n",
      "2018-11-29 16:36:05.418657: step 5620/14650 (epoch 20/50), acc = 0.83, loss = 0.81 (143.8 examples/sec; 0.695 sec/batch), lr: 0.006302\n",
      "2018-11-29 16:36:40.172626: step 5630/14650 (epoch 20/50), acc = 0.83, loss = 0.75 (147.9 examples/sec; 0.676 sec/batch), lr: 0.006302\n",
      "2018-11-29 16:37:15.173672: step 5640/14650 (epoch 20/50), acc = 0.81, loss = 0.91 (143.8 examples/sec; 0.696 sec/batch), lr: 0.006302\n",
      "2018-11-29 16:37:50.529365: step 5650/14650 (epoch 20/50), acc = 0.77, loss = 0.89 (138.5 examples/sec; 0.722 sec/batch), lr: 0.006302\n",
      "2018-11-29 16:38:25.681168: step 5660/14650 (epoch 20/50), acc = 0.78, loss = 1.07 (143.3 examples/sec; 0.698 sec/batch), lr: 0.006302\n",
      "2018-11-29 16:39:00.540620: step 5670/14650 (epoch 20/50), acc = 0.84, loss = 1.01 (144.2 examples/sec; 0.694 sec/batch), lr: 0.006302\n",
      "2018-11-29 16:39:35.560755: step 5680/14650 (epoch 20/50), acc = 0.82, loss = 0.87 (140.4 examples/sec; 0.712 sec/batch), lr: 0.006302\n",
      "2018-11-29 16:40:10.867215: step 5690/14650 (epoch 20/50), acc = 0.82, loss = 0.84 (140.3 examples/sec; 0.713 sec/batch), lr: 0.006302\n",
      "2018-11-29 16:40:46.461411: step 5700/14650 (epoch 20/50), acc = 0.76, loss = 1.12 (140.4 examples/sec; 0.712 sec/batch), lr: 0.006302\n",
      "2018-11-29 16:41:21.703181: step 5710/14650 (epoch 20/50), acc = 0.83, loss = 0.68 (145.4 examples/sec; 0.688 sec/batch), lr: 0.006302\n",
      "2018-11-29 16:41:58.318410: step 5720/14650 (epoch 20/50), acc = 0.77, loss = 0.90 (138.1 examples/sec; 0.724 sec/batch), lr: 0.006302\n",
      "2018-11-29 16:42:34.882152: step 5730/14650 (epoch 20/50), acc = 0.83, loss = 0.87 (80.4 examples/sec; 1.243 sec/batch), lr: 0.006302\n",
      "2018-11-29 16:43:11.097283: step 5740/14650 (epoch 20/50), acc = 0.79, loss = 0.95 (143.4 examples/sec; 0.697 sec/batch), lr: 0.006302\n",
      "2018-11-29 16:43:46.724901: step 5750/14650 (epoch 20/50), acc = 0.76, loss = 0.90 (136.0 examples/sec; 0.735 sec/batch), lr: 0.006302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-29 16:44:22.967779: step 5760/14650 (epoch 20/50), acc = 0.76, loss = 0.94 (80.1 examples/sec; 1.248 sec/batch), lr: 0.006302\n",
      "2018-11-29 16:44:59.039087: step 5770/14650 (epoch 20/50), acc = 0.83, loss = 0.77 (138.1 examples/sec; 0.724 sec/batch), lr: 0.006302\n",
      "2018-11-29 16:45:34.948814: step 5780/14650 (epoch 20/50), acc = 0.76, loss = 1.07 (142.3 examples/sec; 0.703 sec/batch), lr: 0.006302\n",
      "2018-11-29 16:46:11.794847: step 5790/14650 (epoch 20/50), acc = 0.76, loss = 1.11 (137.2 examples/sec; 0.729 sec/batch), lr: 0.006302\n",
      "2018-11-29 16:46:48.935673: step 5800/14650 (epoch 20/50), acc = 0.82, loss = 0.87 (142.1 examples/sec; 0.704 sec/batch), lr: 0.006302\n",
      "\n",
      "Step 5800: train_loss = 0.836932, train_accuracy = 0.816\n",
      "Step 5800:  test_loss = 1.021511,  test_accuracy = 0.844\n",
      "\n",
      "2018-11-29 16:47:29.197409: step 5810/14650 (epoch 20/50), acc = 0.83, loss = 0.95 (136.9 examples/sec; 0.730 sec/batch), lr: 0.006302\n",
      "2018-11-29 16:47:58.940514: step 5818/14650 (epoch 20/50), Learning rate decays to 0.00599\n",
      "2018-11-29 16:48:07.032373: step 5820/14650 (epoch 20/50), acc = 0.81, loss = 0.76 (139.3 examples/sec; 0.718 sec/batch), lr: 0.005987\n",
      "2018-11-29 16:48:44.616871: step 5830/14650 (epoch 20/50), acc = 0.89, loss = 0.57 (142.8 examples/sec; 0.700 sec/batch), lr: 0.005987\n",
      "2018-11-29 16:49:21.950937: step 5840/14650 (epoch 20/50), acc = 0.80, loss = 0.94 (137.3 examples/sec; 0.728 sec/batch), lr: 0.005987\n",
      "2018-11-29 16:49:59.207329: step 5850/14650 (epoch 20/50), acc = 0.82, loss = 0.73 (80.3 examples/sec; 1.245 sec/batch), lr: 0.005987\n",
      "2018-11-29 16:50:36.323308: step 5860/14650 (epoch 20/50), acc = 0.44, loss = 0.77 (151.3 examples/sec; 0.661 sec/batch), lr: 0.005987\n",
      "2018-11-29 16:51:14.734643: step 5870/14650 (epoch 21/50), acc = 0.81, loss = 0.85 (141.4 examples/sec; 0.707 sec/batch), lr: 0.005987\n",
      "2018-11-29 16:51:52.738070: step 5880/14650 (epoch 21/50), acc = 0.81, loss = 0.83 (136.7 examples/sec; 0.731 sec/batch), lr: 0.005987\n",
      "2018-11-29 16:52:30.765697: step 5890/14650 (epoch 21/50), acc = 0.81, loss = 0.75 (138.6 examples/sec; 0.721 sec/batch), lr: 0.005987\n",
      "2018-11-29 16:53:09.025939: step 5900/14650 (epoch 21/50), acc = 0.81, loss = 0.84 (77.7 examples/sec; 1.286 sec/batch), lr: 0.005987\n",
      "2018-11-29 16:53:46.771684: step 5910/14650 (epoch 21/50), acc = 0.82, loss = 0.83 (135.4 examples/sec; 0.738 sec/batch), lr: 0.005987\n",
      "2018-11-29 16:54:25.194709: step 5920/14650 (epoch 21/50), acc = 0.79, loss = 0.81 (135.9 examples/sec; 0.736 sec/batch), lr: 0.005987\n",
      "2018-11-29 16:55:03.512115: step 5930/14650 (epoch 21/50), acc = 0.82, loss = 0.75 (133.5 examples/sec; 0.749 sec/batch), lr: 0.005987\n",
      "2018-11-29 16:55:41.958937: step 5940/14650 (epoch 21/50), acc = 0.85, loss = 0.82 (134.8 examples/sec; 0.742 sec/batch), lr: 0.005987\n",
      "2018-11-29 16:56:20.144934: step 5950/14650 (epoch 21/50), acc = 0.76, loss = 0.93 (137.8 examples/sec; 0.726 sec/batch), lr: 0.005987\n",
      "2018-11-29 16:56:58.990647: step 5960/14650 (epoch 21/50), acc = 0.86, loss = 0.75 (139.2 examples/sec; 0.718 sec/batch), lr: 0.005987\n",
      "2018-11-29 16:57:37.858170: step 5970/14650 (epoch 21/50), acc = 0.85, loss = 0.69 (133.8 examples/sec; 0.748 sec/batch), lr: 0.005987\n",
      "2018-11-29 16:58:17.564824: step 5980/14650 (epoch 21/50), acc = 0.79, loss = 1.00 (80.8 examples/sec; 1.238 sec/batch), lr: 0.005987\n",
      "2018-11-29 16:58:56.915317: step 5990/14650 (epoch 21/50), acc = 0.83, loss = 0.70 (133.7 examples/sec; 0.748 sec/batch), lr: 0.005987\n",
      "2018-11-29 16:59:36.216174: step 6000/14650 (epoch 21/50), acc = 0.80, loss = 0.91 (137.7 examples/sec; 0.726 sec/batch), lr: 0.005987\n",
      "\n",
      "Step 6000: train_loss = 0.789567, train_accuracy = 0.820\n",
      "Step 6000:  test_loss = 0.991866,  test_accuracy = 0.854\n",
      "\n",
      "2018-11-29 17:00:19.042121: step 6010/14650 (epoch 21/50), acc = 0.82, loss = 0.71 (76.5 examples/sec; 1.307 sec/batch), lr: 0.005987\n",
      "2018-11-29 17:00:58.357525: step 6020/14650 (epoch 21/50), acc = 0.83, loss = 0.64 (133.3 examples/sec; 0.750 sec/batch), lr: 0.005987\n",
      "2018-11-29 17:01:39.486821: step 6030/14650 (epoch 21/50), acc = 0.87, loss = 0.65 (79.7 examples/sec; 1.255 sec/batch), lr: 0.005987\n",
      "2018-11-29 17:02:19.363637: step 6040/14650 (epoch 21/50), acc = 0.82, loss = 0.76 (132.6 examples/sec; 0.754 sec/batch), lr: 0.005987\n",
      "2018-11-29 17:03:00.081625: step 6050/14650 (epoch 21/50), acc = 0.81, loss = 0.85 (96.3 examples/sec; 1.038 sec/batch), lr: 0.005987\n",
      "2018-11-29 17:03:40.409415: step 6060/14650 (epoch 21/50), acc = 0.80, loss = 0.85 (134.0 examples/sec; 0.746 sec/batch), lr: 0.005987\n",
      "2018-11-29 17:04:20.966287: step 6070/14650 (epoch 21/50), acc = 0.81, loss = 0.81 (136.9 examples/sec; 0.730 sec/batch), lr: 0.005987\n",
      "2018-11-29 17:05:00.996531: step 6080/14650 (epoch 21/50), acc = 0.78, loss = 0.72 (131.0 examples/sec; 0.763 sec/batch), lr: 0.005987\n",
      "2018-11-29 17:05:41.357696: step 6090/14650 (epoch 21/50), acc = 0.72, loss = 1.06 (135.8 examples/sec; 0.736 sec/batch), lr: 0.005987\n",
      "2018-11-29 17:06:21.972494: step 6100/14650 (epoch 21/50), acc = 0.81, loss = 0.78 (75.9 examples/sec; 1.318 sec/batch), lr: 0.005987\n",
      "2018-11-29 17:07:02.144840: step 6110/14650 (epoch 21/50), acc = 0.86, loss = 0.68 (132.2 examples/sec; 0.756 sec/batch), lr: 0.005987\n",
      "2018-11-29 17:07:42.892877: step 6120/14650 (epoch 21/50), acc = 0.81, loss = 1.12 (131.9 examples/sec; 0.758 sec/batch), lr: 0.005987\n",
      "2018-11-29 17:08:23.158209: step 6130/14650 (epoch 21/50), acc = 0.88, loss = 0.87 (128.7 examples/sec; 0.777 sec/batch), lr: 0.005987\n",
      "2018-11-29 17:09:04.511644: step 6140/14650 (epoch 21/50), acc = 0.82, loss = 0.92 (134.6 examples/sec; 0.743 sec/batch), lr: 0.005987\n",
      "2018-11-29 17:09:45.497976: step 6150/14650 (epoch 21/50), acc = 0.89, loss = 0.52 (131.5 examples/sec; 0.761 sec/batch), lr: 0.005987\n",
      "2018-11-29 17:10:26.807860: step 6160/14650 (epoch 22/50), acc = 0.88, loss = 0.55 (135.1 examples/sec; 0.740 sec/batch), lr: 0.005987\n",
      "2018-11-29 17:11:08.135016: step 6170/14650 (epoch 22/50), acc = 0.81, loss = 0.90 (131.1 examples/sec; 0.763 sec/batch), lr: 0.005987\n",
      "2018-11-29 17:11:50.442628: step 6180/14650 (epoch 22/50), acc = 0.82, loss = 0.81 (85.1 examples/sec; 1.175 sec/batch), lr: 0.005987\n",
      "2018-11-29 17:12:32.124286: step 6190/14650 (epoch 22/50), acc = 0.84, loss = 0.65 (131.0 examples/sec; 0.763 sec/batch), lr: 0.005987\n",
      "2018-11-29 17:13:13.560887: step 6200/14650 (epoch 22/50), acc = 0.79, loss = 0.84 (134.1 examples/sec; 0.746 sec/batch), lr: 0.005987\n",
      "\n",
      "Step 6200: train_loss = 0.807190, train_accuracy = 0.822\n",
      "Step 6200:  test_loss = 0.992007,  test_accuracy = 0.848\n",
      "\n",
      "2018-11-29 17:13:59.207461: step 6210/14650 (epoch 22/50), acc = 0.84, loss = 0.74 (85.4 examples/sec; 1.171 sec/batch), lr: 0.005987\n",
      "2018-11-29 17:14:41.932526: step 6220/14650 (epoch 22/50), acc = 0.79, loss = 0.87 (130.2 examples/sec; 0.768 sec/batch), lr: 0.005987\n",
      "2018-11-29 17:15:24.188452: step 6230/14650 (epoch 22/50), acc = 0.73, loss = 0.91 (78.1 examples/sec; 1.281 sec/batch), lr: 0.005987\n",
      "2018-11-29 17:16:06.196356: step 6240/14650 (epoch 22/50), acc = 0.81, loss = 0.78 (130.1 examples/sec; 0.768 sec/batch), lr: 0.005987\n",
      "2018-11-29 17:16:48.350541: step 6250/14650 (epoch 22/50), acc = 0.80, loss = 0.89 (132.5 examples/sec; 0.754 sec/batch), lr: 0.005987\n",
      "2018-11-29 17:17:31.043158: step 6260/14650 (epoch 22/50), acc = 0.83, loss = 0.92 (132.2 examples/sec; 0.757 sec/batch), lr: 0.005987\n",
      "2018-11-29 17:18:13.827260: step 6270/14650 (epoch 22/50), acc = 0.74, loss = 0.91 (132.1 examples/sec; 0.757 sec/batch), lr: 0.005987\n",
      "2018-11-29 17:18:57.071391: step 6280/14650 (epoch 22/50), acc = 0.86, loss = 0.57 (71.3 examples/sec; 1.403 sec/batch), lr: 0.005987\n",
      "2018-11-29 17:19:39.995989: step 6290/14650 (epoch 22/50), acc = 0.83, loss = 0.68 (127.8 examples/sec; 0.782 sec/batch), lr: 0.005987\n",
      "2018-11-29 17:20:22.705926: step 6300/14650 (epoch 22/50), acc = 0.88, loss = 0.67 (127.4 examples/sec; 0.785 sec/batch), lr: 0.005987\n",
      "2018-11-29 17:21:06.279009: step 6310/14650 (epoch 22/50), acc = 0.75, loss = 1.02 (77.4 examples/sec; 1.292 sec/batch), lr: 0.005987\n",
      "2018-11-29 17:21:40.614621: step 6318/14650 (epoch 22/50), Learning rate decays to 0.00569\n",
      "2018-11-29 17:21:49.323505: step 6320/14650 (epoch 22/50), acc = 0.79, loss = 0.81 (127.8 examples/sec; 0.783 sec/batch), lr: 0.005688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-29 17:22:32.668511: step 6330/14650 (epoch 22/50), acc = 0.81, loss = 0.74 (130.9 examples/sec; 0.764 sec/batch), lr: 0.005688\n",
      "2018-11-29 17:23:16.710112: step 6340/14650 (epoch 22/50), acc = 0.81, loss = 0.71 (96.2 examples/sec; 1.040 sec/batch), lr: 0.005688\n",
      "2018-11-29 17:24:00.535524: step 6350/14650 (epoch 22/50), acc = 0.82, loss = 2.12 (126.8 examples/sec; 0.788 sec/batch), lr: 0.005688\n",
      "2018-11-29 17:24:44.382250: step 6360/14650 (epoch 22/50), acc = 0.89, loss = 0.68 (123.2 examples/sec; 0.812 sec/batch), lr: 0.005688\n",
      "2018-11-29 17:25:28.734415: step 6370/14650 (epoch 22/50), acc = 0.83, loss = 0.65 (93.0 examples/sec; 1.075 sec/batch), lr: 0.005688\n",
      "2018-11-29 17:26:12.436842: step 6380/14650 (epoch 22/50), acc = 0.78, loss = 0.92 (128.3 examples/sec; 0.780 sec/batch), lr: 0.005688\n",
      "2018-11-29 17:26:56.271849: step 6390/14650 (epoch 22/50), acc = 0.77, loss = 1.02 (129.3 examples/sec; 0.773 sec/batch), lr: 0.005688\n",
      "2018-11-29 17:27:41.202890: step 6400/14650 (epoch 22/50), acc = 0.73, loss = 1.05 (129.4 examples/sec; 0.773 sec/batch), lr: 0.005688\n",
      "\n",
      "Step 6400: train_loss = 0.814569, train_accuracy = 0.820\n",
      "Step 6400:  test_loss = 1.126433,  test_accuracy = 0.848\n",
      "\n",
      "2018-11-29 17:28:29.072689: step 6410/14650 (epoch 22/50), acc = 0.75, loss = 1.01 (88.2 examples/sec; 1.133 sec/batch), lr: 0.005688\n",
      "2018-11-29 17:29:13.240627: step 6420/14650 (epoch 22/50), acc = 0.87, loss = 0.65 (129.6 examples/sec; 0.772 sec/batch), lr: 0.005688\n",
      "2018-11-29 17:29:58.088279: step 6430/14650 (epoch 22/50), acc = 0.84, loss = 0.81 (111.0 examples/sec; 0.901 sec/batch), lr: 0.005688\n",
      "2018-11-29 17:30:42.224633: step 6440/14650 (epoch 22/50), acc = 0.80, loss = 0.87 (125.6 examples/sec; 0.796 sec/batch), lr: 0.005688\n",
      "2018-11-29 17:31:27.302586: step 6450/14650 (epoch 23/50), acc = 0.79, loss = 0.81 (126.3 examples/sec; 0.792 sec/batch), lr: 0.005688\n",
      "2018-11-29 17:32:11.922953: step 6460/14650 (epoch 23/50), acc = 0.88, loss = 0.62 (125.1 examples/sec; 0.799 sec/batch), lr: 0.005688\n",
      "2018-11-29 17:32:58.195775: step 6470/14650 (epoch 23/50), acc = 0.84, loss = 0.80 (124.5 examples/sec; 0.803 sec/batch), lr: 0.005688\n",
      "2018-11-29 17:33:43.587057: step 6480/14650 (epoch 23/50), acc = 0.81, loss = 0.96 (124.1 examples/sec; 0.806 sec/batch), lr: 0.005688\n",
      "2018-11-29 17:34:29.079350: step 6490/14650 (epoch 23/50), acc = 0.82, loss = 0.84 (128.2 examples/sec; 0.780 sec/batch), lr: 0.005688\n",
      "2018-11-29 17:35:15.781236: step 6500/14650 (epoch 23/50), acc = 0.82, loss = 0.78 (75.9 examples/sec; 1.318 sec/batch), lr: 0.005688\n",
      "2018-11-29 17:36:01.696063: step 6510/14650 (epoch 23/50), acc = 0.89, loss = 0.61 (124.3 examples/sec; 0.805 sec/batch), lr: 0.005688\n",
      "2018-11-29 17:36:48.489778: step 6520/14650 (epoch 23/50), acc = 0.82, loss = 0.76 (72.2 examples/sec; 1.386 sec/batch), lr: 0.005688\n",
      "2018-11-29 17:37:34.591586: step 6530/14650 (epoch 23/50), acc = 0.88, loss = 0.57 (123.5 examples/sec; 0.810 sec/batch), lr: 0.005688\n",
      "2018-11-29 17:38:20.355251: step 6540/14650 (epoch 23/50), acc = 0.82, loss = 0.75 (123.6 examples/sec; 0.809 sec/batch), lr: 0.005688\n",
      "2018-11-29 17:39:07.022191: step 6550/14650 (epoch 23/50), acc = 0.87, loss = 0.62 (70.6 examples/sec; 1.416 sec/batch), lr: 0.005688\n",
      "2018-11-29 17:39:53.602904: step 6560/14650 (epoch 23/50), acc = 0.86, loss = 0.68 (124.6 examples/sec; 0.803 sec/batch), lr: 0.005688\n",
      "2018-11-29 17:40:40.803743: step 6570/14650 (epoch 23/50), acc = 0.83, loss = 1.04 (123.2 examples/sec; 0.812 sec/batch), lr: 0.005688\n",
      "2018-11-29 17:41:27.715746: step 6580/14650 (epoch 23/50), acc = 0.77, loss = 0.90 (122.7 examples/sec; 0.815 sec/batch), lr: 0.005688\n",
      "2018-11-29 17:42:15.039042: step 6590/14650 (epoch 23/50), acc = 0.87, loss = 0.64 (122.5 examples/sec; 0.816 sec/batch), lr: 0.005688\n",
      "2018-11-29 17:43:02.418655: step 6600/14650 (epoch 23/50), acc = 0.88, loss = 0.51 (122.3 examples/sec; 0.818 sec/batch), lr: 0.005688\n",
      "\n",
      "Step 6600: train_loss = 0.790879, train_accuracy = 0.828\n",
      "Step 6600:  test_loss = 0.972885,  test_accuracy = 0.851\n",
      "\n",
      "2018-11-29 17:43:53.038008: step 6610/14650 (epoch 23/50), acc = 0.83, loss = 0.61 (122.0 examples/sec; 0.819 sec/batch), lr: 0.005688\n",
      "2018-11-29 17:44:40.540935: step 6620/14650 (epoch 23/50), acc = 0.81, loss = 0.74 (121.5 examples/sec; 0.823 sec/batch), lr: 0.005688\n",
      "2018-11-29 17:45:27.945296: step 6630/14650 (epoch 23/50), acc = 0.85, loss = 0.60 (122.7 examples/sec; 0.815 sec/batch), lr: 0.005688\n",
      "2018-11-29 17:46:15.670548: step 6640/14650 (epoch 23/50), acc = 0.87, loss = 0.56 (122.1 examples/sec; 0.819 sec/batch), lr: 0.005688\n",
      "2018-11-29 17:47:03.678726: step 6650/14650 (epoch 23/50), acc = 0.87, loss = 0.69 (121.4 examples/sec; 0.823 sec/batch), lr: 0.005688\n",
      "2018-11-29 17:47:51.581787: step 6660/14650 (epoch 23/50), acc = 0.78, loss = 0.94 (67.7 examples/sec; 1.477 sec/batch), lr: 0.005688\n",
      "2018-11-29 17:48:40.175470: step 6670/14650 (epoch 23/50), acc = 0.81, loss = 0.76 (80.9 examples/sec; 1.237 sec/batch), lr: 0.005688\n",
      "2018-11-29 17:49:28.821135: step 6680/14650 (epoch 23/50), acc = 0.82, loss = 0.67 (72.6 examples/sec; 1.378 sec/batch), lr: 0.005688\n",
      "2018-11-29 17:50:17.438204: step 6690/14650 (epoch 23/50), acc = 0.85, loss = 0.78 (76.5 examples/sec; 1.307 sec/batch), lr: 0.005688\n",
      "2018-11-29 17:51:05.816481: step 6700/14650 (epoch 23/50), acc = 0.79, loss = 0.89 (75.0 examples/sec; 1.333 sec/batch), lr: 0.005688\n",
      "2018-11-29 17:51:54.967017: step 6710/14650 (epoch 23/50), acc = 0.83, loss = 0.69 (78.1 examples/sec; 1.280 sec/batch), lr: 0.005688\n",
      "2018-11-29 17:52:44.025991: step 6720/14650 (epoch 23/50), acc = 0.82, loss = 0.81 (80.2 examples/sec; 1.246 sec/batch), lr: 0.005688\n",
      "2018-11-29 17:53:33.082180: step 6730/14650 (epoch 23/50), acc = 0.88, loss = 0.59 (77.0 examples/sec; 1.299 sec/batch), lr: 0.005688\n",
      "2018-11-29 17:54:22.904134: step 6740/14650 (epoch 24/50), acc = 0.83, loss = 0.81 (97.2 examples/sec; 1.029 sec/batch), lr: 0.005688\n",
      "2018-11-29 17:55:12.507627: step 6750/14650 (epoch 24/50), acc = 0.90, loss = 0.96 (81.2 examples/sec; 1.232 sec/batch), lr: 0.005688\n",
      "2018-11-29 17:56:02.960786: step 6760/14650 (epoch 24/50), acc = 0.82, loss = 0.86 (102.7 examples/sec; 0.973 sec/batch), lr: 0.005688\n",
      "2018-11-29 17:56:52.572882: step 6770/14650 (epoch 24/50), acc = 0.85, loss = 0.68 (86.1 examples/sec; 1.161 sec/batch), lr: 0.005688\n",
      "2018-11-29 17:57:42.308253: step 6780/14650 (epoch 24/50), acc = 0.86, loss = 0.76 (85.8 examples/sec; 1.165 sec/batch), lr: 0.005688\n",
      "2018-11-29 17:58:32.596686: step 6790/14650 (epoch 24/50), acc = 0.84, loss = 0.87 (103.2 examples/sec; 0.969 sec/batch), lr: 0.005688\n",
      "2018-11-29 17:59:22.682360: step 6800/14650 (epoch 24/50), acc = 0.80, loss = 0.95 (101.8 examples/sec; 0.983 sec/batch), lr: 0.005688\n",
      "\n",
      "Step 6800: train_loss = 0.785052, train_accuracy = 0.827\n",
      "Step 6800:  test_loss = 0.979919,  test_accuracy = 0.847\n",
      "\n",
      "2018-11-29 18:00:16.210341: step 6810/14650 (epoch 24/50), acc = 0.81, loss = 0.80 (100.6 examples/sec; 0.994 sec/batch), lr: 0.005688\n",
      "2018-11-29 18:00:56.338711: step 6818/14650 (epoch 24/50), Learning rate decays to 0.00540\n",
      "2018-11-29 18:01:06.301868: step 6820/14650 (epoch 24/50), acc = 0.89, loss = 0.67 (89.4 examples/sec; 1.118 sec/batch), lr: 0.005404\n",
      "2018-11-29 18:01:56.128357: step 6830/14650 (epoch 24/50), acc = 0.85, loss = 0.73 (84.7 examples/sec; 1.181 sec/batch), lr: 0.005404\n",
      "2018-11-29 18:02:46.209205: step 6840/14650 (epoch 24/50), acc = 0.85, loss = 0.71 (107.9 examples/sec; 0.927 sec/batch), lr: 0.005404\n",
      "2018-11-29 18:03:36.231220: step 6850/14650 (epoch 24/50), acc = 0.81, loss = 0.84 (90.6 examples/sec; 1.104 sec/batch), lr: 0.005404\n",
      "2018-11-29 18:04:26.284896: step 6860/14650 (epoch 24/50), acc = 0.86, loss = 0.72 (92.5 examples/sec; 1.081 sec/batch), lr: 0.005404\n",
      "2018-11-29 18:05:16.642749: step 6870/14650 (epoch 24/50), acc = 0.84, loss = 0.71 (94.1 examples/sec; 1.063 sec/batch), lr: 0.005404\n",
      "2018-11-29 18:06:07.388033: step 6880/14650 (epoch 24/50), acc = 0.83, loss = 0.78 (91.2 examples/sec; 1.096 sec/batch), lr: 0.005404\n",
      "2018-11-29 18:06:57.835153: step 6890/14650 (epoch 24/50), acc = 0.90, loss = 0.44 (90.6 examples/sec; 1.104 sec/batch), lr: 0.005404\n",
      "2018-11-29 18:07:48.330068: step 6900/14650 (epoch 24/50), acc = 0.82, loss = 0.91 (92.1 examples/sec; 1.086 sec/batch), lr: 0.005404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-29 18:08:39.383901: step 6910/14650 (epoch 24/50), acc = 0.76, loss = 1.09 (119.7 examples/sec; 0.836 sec/batch), lr: 0.005404\n",
      "2018-11-29 18:09:30.602429: step 6920/14650 (epoch 24/50), acc = 0.73, loss = 0.91 (118.3 examples/sec; 0.845 sec/batch), lr: 0.005404\n",
      "2018-11-29 18:10:21.694196: step 6930/14650 (epoch 24/50), acc = 0.84, loss = 0.86 (114.4 examples/sec; 0.874 sec/batch), lr: 0.005404\n",
      "2018-11-29 18:11:12.783024: step 6940/14650 (epoch 24/50), acc = 0.89, loss = 0.60 (103.6 examples/sec; 0.965 sec/batch), lr: 0.005404\n",
      "2018-11-29 18:12:04.395901: step 6950/14650 (epoch 24/50), acc = 0.76, loss = 0.96 (110.8 examples/sec; 0.902 sec/batch), lr: 0.005404\n",
      "2018-11-29 18:13:00.348471: step 6960/14650 (epoch 24/50), acc = 0.83, loss = 0.71 (66.0 examples/sec; 1.514 sec/batch), lr: 0.005404\n",
      "2018-11-29 18:13:54.497403: step 6970/14650 (epoch 24/50), acc = 0.78, loss = 0.89 (117.1 examples/sec; 0.854 sec/batch), lr: 0.005404\n",
      "2018-11-29 18:14:48.145267: step 6980/14650 (epoch 24/50), acc = 0.87, loss = 0.89 (116.6 examples/sec; 0.857 sec/batch), lr: 0.005404\n",
      "2018-11-29 18:15:40.503024: step 6990/14650 (epoch 24/50), acc = 0.79, loss = 0.91 (99.6 examples/sec; 1.004 sec/batch), lr: 0.005404\n",
      "2018-11-29 18:16:32.558119: step 7000/14650 (epoch 24/50), acc = 0.81, loss = 0.74 (116.1 examples/sec; 0.861 sec/batch), lr: 0.005404\n",
      "\n",
      "Step 7000: train_loss = 0.782624, train_accuracy = 0.825\n",
      "Step 7000:  test_loss = 0.943331,  test_accuracy = 0.848\n",
      "\n",
      "2018-11-29 18:17:28.846940: step 7010/14650 (epoch 24/50), acc = 0.81, loss = 0.85 (116.6 examples/sec; 0.858 sec/batch), lr: 0.005404\n",
      "2018-11-29 18:18:21.076510: step 7020/14650 (epoch 24/50), acc = 0.76, loss = 1.31 (115.0 examples/sec; 0.870 sec/batch), lr: 0.005404\n",
      "2018-11-29 18:19:14.068422: step 7030/14650 (epoch 24/50), acc = 0.79, loss = 0.74 (104.0 examples/sec; 0.961 sec/batch), lr: 0.005404\n",
      "2018-11-29 18:20:06.254765: step 7040/14650 (epoch 25/50), acc = 0.74, loss = 1.06 (107.0 examples/sec; 0.934 sec/batch), lr: 0.005404\n",
      "2018-11-29 18:20:58.403391: step 7050/14650 (epoch 25/50), acc = 0.77, loss = 0.76 (116.8 examples/sec; 0.856 sec/batch), lr: 0.005404\n",
      "2018-11-29 18:21:51.451642: step 7060/14650 (epoch 25/50), acc = 0.80, loss = 0.73 (115.5 examples/sec; 0.866 sec/batch), lr: 0.005404\n",
      "2018-11-29 18:22:44.303363: step 7070/14650 (epoch 25/50), acc = 0.89, loss = 0.57 (115.6 examples/sec; 0.865 sec/batch), lr: 0.005404\n",
      "2018-11-29 18:23:37.063446: step 7080/14650 (epoch 25/50), acc = 0.82, loss = 0.78 (117.0 examples/sec; 0.855 sec/batch), lr: 0.005404\n",
      "2018-11-29 18:24:31.206193: step 7090/14650 (epoch 25/50), acc = 0.81, loss = 0.81 (116.2 examples/sec; 0.861 sec/batch), lr: 0.005404\n",
      "2018-11-29 18:25:24.213979: step 7100/14650 (epoch 25/50), acc = 0.80, loss = 0.94 (114.8 examples/sec; 0.871 sec/batch), lr: 0.005404\n",
      "2018-11-29 18:26:18.266901: step 7110/14650 (epoch 25/50), acc = 0.85, loss = 0.78 (114.6 examples/sec; 0.873 sec/batch), lr: 0.005404\n",
      "2018-11-29 18:27:11.657751: step 7120/14650 (epoch 25/50), acc = 0.76, loss = 0.95 (115.1 examples/sec; 0.869 sec/batch), lr: 0.005404\n",
      "2018-11-29 18:28:06.002374: step 7130/14650 (epoch 25/50), acc = 0.85, loss = 0.70 (114.5 examples/sec; 0.874 sec/batch), lr: 0.005404\n",
      "2018-11-29 18:29:00.626649: step 7140/14650 (epoch 25/50), acc = 0.83, loss = 0.81 (115.8 examples/sec; 0.863 sec/batch), lr: 0.005404\n",
      "2018-11-29 18:29:55.234395: step 7150/14650 (epoch 25/50), acc = 0.78, loss = 0.67 (114.3 examples/sec; 0.875 sec/batch), lr: 0.005404\n",
      "2018-11-29 18:30:50.191165: step 7160/14650 (epoch 25/50), acc = 0.82, loss = 0.82 (113.6 examples/sec; 0.880 sec/batch), lr: 0.005404\n",
      "2018-11-29 18:31:44.671116: step 7170/14650 (epoch 25/50), acc = 0.85, loss = 0.70 (113.6 examples/sec; 0.880 sec/batch), lr: 0.005404\n",
      "2018-11-29 18:32:39.165432: step 7180/14650 (epoch 25/50), acc = 0.79, loss = 1.00 (63.3 examples/sec; 1.580 sec/batch), lr: 0.005404\n",
      "2018-11-29 18:33:32.837209: step 7190/14650 (epoch 25/50), acc = 0.84, loss = 0.65 (112.9 examples/sec; 0.885 sec/batch), lr: 0.005404\n",
      "2018-11-29 18:34:27.325875: step 7200/14650 (epoch 25/50), acc = 0.80, loss = 0.75 (116.0 examples/sec; 0.862 sec/batch), lr: 0.005404\n",
      "\n",
      "Step 7200: train_loss = 0.793378, train_accuracy = 0.822\n",
      "Step 7200:  test_loss = 0.986904,  test_accuracy = 0.848\n",
      "\n",
      "2018-11-29 18:35:25.930733: step 7210/14650 (epoch 25/50), acc = 0.84, loss = 0.71 (114.2 examples/sec; 0.876 sec/batch), lr: 0.005404\n",
      "2018-11-29 18:36:20.904588: step 7220/14650 (epoch 25/50), acc = 0.79, loss = 0.82 (115.8 examples/sec; 0.863 sec/batch), lr: 0.005404\n",
      "2018-11-29 18:37:17.149314: step 7230/14650 (epoch 25/50), acc = 0.78, loss = 0.79 (112.0 examples/sec; 0.893 sec/batch), lr: 0.005404\n",
      "2018-11-29 18:38:12.088125: step 7240/14650 (epoch 25/50), acc = 0.86, loss = 0.57 (112.2 examples/sec; 0.891 sec/batch), lr: 0.005404\n",
      "2018-11-29 18:39:07.243909: step 7250/14650 (epoch 25/50), acc = 0.92, loss = 0.63 (112.4 examples/sec; 0.889 sec/batch), lr: 0.005404\n",
      "2018-11-29 18:40:03.119277: step 7260/14650 (epoch 25/50), acc = 0.83, loss = 0.74 (110.2 examples/sec; 0.907 sec/batch), lr: 0.005404\n",
      "2018-11-29 18:40:58.298986: step 7270/14650 (epoch 25/50), acc = 0.87, loss = 0.74 (111.8 examples/sec; 0.895 sec/batch), lr: 0.005404\n",
      "2018-11-29 18:41:54.597605: step 7280/14650 (epoch 25/50), acc = 0.87, loss = 0.59 (111.0 examples/sec; 0.901 sec/batch), lr: 0.005404\n",
      "2018-11-29 18:42:51.119563: step 7290/14650 (epoch 25/50), acc = 0.81, loss = 0.81 (71.2 examples/sec; 1.404 sec/batch), lr: 0.005404\n",
      "2018-11-29 18:43:46.894020: step 7300/14650 (epoch 25/50), acc = 0.87, loss = 0.63 (111.4 examples/sec; 0.898 sec/batch), lr: 0.005404\n",
      "2018-11-29 18:44:43.487502: step 7310/14650 (epoch 25/50), acc = 0.87, loss = 0.65 (112.0 examples/sec; 0.893 sec/batch), lr: 0.005404\n",
      "2018-11-29 18:45:40.700502: step 7320/14650 (epoch 25/50), acc = 0.84, loss = 0.80 (111.3 examples/sec; 0.899 sec/batch), lr: 0.005404\n",
      "2018-11-29 18:46:37.298624: step 7330/14650 (epoch 26/50), acc = 0.81, loss = 0.79 (68.7 examples/sec; 1.455 sec/batch), lr: 0.005404\n",
      "2018-11-29 18:47:34.814138: step 7340/14650 (epoch 26/50), acc = 0.80, loss = 0.87 (81.3 examples/sec; 1.230 sec/batch), lr: 0.005404\n",
      "2018-11-29 18:48:32.105831: step 7350/14650 (epoch 26/50), acc = 0.85, loss = 0.62 (63.7 examples/sec; 1.570 sec/batch), lr: 0.005404\n",
      "2018-11-29 18:49:28.796440: step 7360/14650 (epoch 26/50), acc = 0.89, loss = 0.60 (67.2 examples/sec; 1.489 sec/batch), lr: 0.005404\n",
      "2018-11-29 18:50:02.478580: step 7366/14650 (epoch 26/50), Learning rate decays to 0.00513\n",
      "2018-11-29 18:50:25.467496: step 7370/14650 (epoch 26/50), acc = 0.83, loss = 0.77 (80.5 examples/sec; 1.243 sec/batch), lr: 0.005133\n",
      "2018-11-29 18:51:21.878792: step 7380/14650 (epoch 26/50), acc = 0.82, loss = 0.78 (111.0 examples/sec; 0.901 sec/batch), lr: 0.005133\n",
      "2018-11-29 18:52:20.424297: step 7390/14650 (epoch 26/50), acc = 0.86, loss = 0.81 (61.4 examples/sec; 1.629 sec/batch), lr: 0.005133\n",
      "2018-11-29 18:53:18.463909: step 7400/14650 (epoch 26/50), acc = 0.81, loss = 0.82 (110.8 examples/sec; 0.902 sec/batch), lr: 0.005133\n",
      "\n",
      "Step 7400: train_loss = 0.770024, train_accuracy = 0.829\n",
      "Step 7400:  test_loss = 0.958819,  test_accuracy = 0.851\n",
      "\n",
      "2018-11-29 18:54:20.008274: step 7410/14650 (epoch 26/50), acc = 0.85, loss = 0.67 (86.9 examples/sec; 1.150 sec/batch), lr: 0.005133\n",
      "2018-11-29 18:55:18.708441: step 7420/14650 (epoch 26/50), acc = 0.78, loss = 0.78 (108.7 examples/sec; 0.920 sec/batch), lr: 0.005133\n",
      "2018-11-29 18:56:16.822963: step 7430/14650 (epoch 26/50), acc = 0.85, loss = 0.63 (70.9 examples/sec; 1.410 sec/batch), lr: 0.005133\n",
      "2018-11-29 18:57:15.560836: step 7440/14650 (epoch 26/50), acc = 0.83, loss = 0.78 (110.9 examples/sec; 0.902 sec/batch), lr: 0.005133\n",
      "2018-11-29 18:58:14.208833: step 7450/14650 (epoch 26/50), acc = 0.78, loss = 0.71 (109.9 examples/sec; 0.910 sec/batch), lr: 0.005133\n",
      "2018-11-29 18:59:12.995654: step 7460/14650 (epoch 26/50), acc = 0.86, loss = 0.62 (110.2 examples/sec; 0.908 sec/batch), lr: 0.005133\n",
      "2018-11-29 19:00:14.168278: step 7470/14650 (epoch 26/50), acc = 0.85, loss = 0.64 (61.2 examples/sec; 1.633 sec/batch), lr: 0.005133\n",
      "2018-11-29 19:01:15.027515: step 7480/14650 (epoch 26/50), acc = 0.82, loss = 0.86 (61.0 examples/sec; 1.639 sec/batch), lr: 0.005133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-29 19:02:14.791218: step 7490/14650 (epoch 26/50), acc = 0.84, loss = 0.67 (109.3 examples/sec; 0.915 sec/batch), lr: 0.005133\n",
      "2018-11-29 19:03:17.599619: step 7500/14650 (epoch 26/50), acc = 0.87, loss = 0.64 (61.3 examples/sec; 1.633 sec/batch), lr: 0.005133\n",
      "2018-11-29 19:04:18.583100: step 7510/14650 (epoch 26/50), acc = 0.89, loss = 0.65 (65.3 examples/sec; 1.532 sec/batch), lr: 0.005133\n",
      "2018-11-29 19:05:20.418720: step 7520/14650 (epoch 26/50), acc = 0.86, loss = 0.75 (60.4 examples/sec; 1.655 sec/batch), lr: 0.005133\n",
      "2018-11-29 19:06:24.189471: step 7530/14650 (epoch 26/50), acc = 0.90, loss = 0.48 (60.6 examples/sec; 1.651 sec/batch), lr: 0.005133\n",
      "2018-11-29 19:07:27.403612: step 7540/14650 (epoch 26/50), acc = 0.73, loss = 1.48 (61.1 examples/sec; 1.637 sec/batch), lr: 0.005133\n",
      "2018-11-29 19:08:30.595137: step 7550/14650 (epoch 26/50), acc = 0.81, loss = 0.82 (61.4 examples/sec; 1.627 sec/batch), lr: 0.005133\n",
      "2018-11-29 19:09:34.041768: step 7560/14650 (epoch 26/50), acc = 0.84, loss = 0.66 (62.9 examples/sec; 1.591 sec/batch), lr: 0.005133\n",
      "2018-11-29 19:10:36.378319: step 7570/14650 (epoch 26/50), acc = 0.88, loss = 0.54 (60.7 examples/sec; 1.647 sec/batch), lr: 0.005133\n",
      "2018-11-29 19:11:38.850649: step 7580/14650 (epoch 26/50), acc = 0.84, loss = 0.70 (108.4 examples/sec; 0.923 sec/batch), lr: 0.005133\n",
      "2018-11-29 19:12:41.931161: step 7590/14650 (epoch 26/50), acc = 0.85, loss = 0.60 (60.6 examples/sec; 1.651 sec/batch), lr: 0.005133\n",
      "2018-11-29 19:13:43.058235: step 7600/14650 (epoch 26/50), acc = 0.84, loss = 0.59 (109.3 examples/sec; 0.915 sec/batch), lr: 0.005133\n",
      "\n",
      "Step 7600: train_loss = 0.749364, train_accuracy = 0.831\n",
      "Step 7600:  test_loss = 0.941595,  test_accuracy = 0.857\n",
      "\n",
      "2018-11-29 19:14:50.170204: step 7610/14650 (epoch 26/50), acc = 0.87, loss = 0.61 (62.8 examples/sec; 1.591 sec/batch), lr: 0.005133\n",
      "2018-11-29 19:15:53.962577: step 7620/14650 (epoch 27/50), acc = 0.86, loss = 0.60 (70.7 examples/sec; 1.414 sec/batch), lr: 0.005133\n",
      "2018-11-29 19:16:57.388575: step 7630/14650 (epoch 27/50), acc = 0.83, loss = 0.71 (64.6 examples/sec; 1.549 sec/batch), lr: 0.005133\n",
      "2018-11-29 19:18:00.491263: step 7640/14650 (epoch 27/50), acc = 0.84, loss = 0.93 (65.6 examples/sec; 1.523 sec/batch), lr: 0.005133\n",
      "2018-11-29 19:19:04.194424: step 7650/14650 (epoch 27/50), acc = 0.76, loss = 1.08 (60.4 examples/sec; 1.657 sec/batch), lr: 0.005133\n",
      "2018-11-29 19:20:07.846694: step 7660/14650 (epoch 27/50), acc = 0.82, loss = 0.79 (63.2 examples/sec; 1.583 sec/batch), lr: 0.005133\n",
      "2018-11-29 19:21:12.162424: step 7670/14650 (epoch 27/50), acc = 0.89, loss = 0.62 (72.5 examples/sec; 1.379 sec/batch), lr: 0.005133\n",
      "2018-11-29 19:22:16.330963: step 7680/14650 (epoch 27/50), acc = 0.80, loss = 0.87 (63.8 examples/sec; 1.568 sec/batch), lr: 0.005133\n",
      "2018-11-29 19:23:20.372124: step 7690/14650 (epoch 27/50), acc = 0.81, loss = 0.63 (69.0 examples/sec; 1.449 sec/batch), lr: 0.005133\n",
      "2018-11-29 19:24:24.856688: step 7700/14650 (epoch 27/50), acc = 0.83, loss = 0.57 (65.7 examples/sec; 1.522 sec/batch), lr: 0.005133\n",
      "2018-11-29 19:25:29.444844: step 7710/14650 (epoch 27/50), acc = 0.84, loss = 0.64 (65.5 examples/sec; 1.526 sec/batch), lr: 0.005133\n",
      "2018-11-29 19:26:34.803688: step 7720/14650 (epoch 27/50), acc = 0.83, loss = 0.66 (71.5 examples/sec; 1.400 sec/batch), lr: 0.005133\n",
      "2018-11-29 19:27:40.367526: step 7730/14650 (epoch 27/50), acc = 0.83, loss = 0.64 (65.1 examples/sec; 1.536 sec/batch), lr: 0.005133\n",
      "2018-11-29 19:28:45.794417: step 7740/14650 (epoch 27/50), acc = 0.85, loss = 0.62 (69.9 examples/sec; 1.431 sec/batch), lr: 0.005133\n",
      "2018-11-29 19:29:50.615617: step 7750/14650 (epoch 27/50), acc = 0.82, loss = 0.75 (72.5 examples/sec; 1.380 sec/batch), lr: 0.005133\n",
      "2018-11-29 19:30:55.821985: step 7760/14650 (epoch 27/50), acc = 0.79, loss = 0.77 (75.1 examples/sec; 1.332 sec/batch), lr: 0.005133\n",
      "2018-11-29 19:32:01.462430: step 7770/14650 (epoch 27/50), acc = 0.88, loss = 0.54 (75.4 examples/sec; 1.327 sec/batch), lr: 0.005133\n",
      "2018-11-29 19:33:07.089290: step 7780/14650 (epoch 27/50), acc = 0.83, loss = 0.72 (65.9 examples/sec; 1.517 sec/batch), lr: 0.005133\n",
      "2018-11-29 19:34:13.512290: step 7790/14650 (epoch 27/50), acc = 0.86, loss = 0.57 (74.8 examples/sec; 1.337 sec/batch), lr: 0.005133\n",
      "2018-11-29 19:35:19.596241: step 7800/14650 (epoch 27/50), acc = 0.89, loss = 0.62 (71.2 examples/sec; 1.405 sec/batch), lr: 0.005133\n",
      "\n",
      "Step 7800: train_loss = 0.731700, train_accuracy = 0.830\n",
      "Step 7800:  test_loss = 0.964650,  test_accuracy = 0.860\n",
      "\n",
      "2018-11-29 19:36:30.305946: step 7810/14650 (epoch 27/50), acc = 0.78, loss = 0.78 (63.8 examples/sec; 1.567 sec/batch), lr: 0.005133\n",
      "2018-11-29 19:37:37.629692: step 7820/14650 (epoch 27/50), acc = 0.88, loss = 0.59 (71.7 examples/sec; 1.395 sec/batch), lr: 0.005133\n",
      "2018-11-29 19:38:44.864621: step 7830/14650 (epoch 27/50), acc = 0.82, loss = 0.69 (71.6 examples/sec; 1.397 sec/batch), lr: 0.005133\n",
      "2018-11-29 19:39:52.118268: step 7840/14650 (epoch 27/50), acc = 0.82, loss = 0.73 (69.1 examples/sec; 1.447 sec/batch), lr: 0.005133\n",
      "2018-11-29 19:40:59.176734: step 7850/14650 (epoch 27/50), acc = 0.83, loss = 0.74 (84.1 examples/sec; 1.189 sec/batch), lr: 0.005133\n",
      "2018-11-29 19:42:06.672086: step 7860/14650 (epoch 27/50), acc = 0.85, loss = 0.64 (73.3 examples/sec; 1.364 sec/batch), lr: 0.005133\n",
      "2018-11-29 19:42:46.807084: step 7866/14650 (epoch 27/50), Learning rate decays to 0.00488\n",
      "2018-11-29 19:43:13.849077: step 7870/14650 (epoch 27/50), acc = 0.78, loss = 0.76 (69.5 examples/sec; 1.438 sec/batch), lr: 0.004877\n",
      "2018-11-29 19:44:21.000307: step 7880/14650 (epoch 27/50), acc = 0.76, loss = 0.98 (67.9 examples/sec; 1.473 sec/batch), lr: 0.004877\n",
      "2018-11-29 19:45:28.265649: step 7890/14650 (epoch 27/50), acc = 0.80, loss = 0.78 (72.2 examples/sec; 1.386 sec/batch), lr: 0.004877\n",
      "2018-11-29 19:46:35.664577: step 7900/14650 (epoch 27/50), acc = 0.83, loss = 0.68 (71.0 examples/sec; 1.409 sec/batch), lr: 0.004877\n",
      "2018-11-29 19:47:42.591463: step 7910/14650 (epoch 27/50), acc = 0.88, loss = 0.63 (67.6 examples/sec; 1.479 sec/batch), lr: 0.004877\n",
      "2018-11-29 19:48:49.800890: step 7920/14650 (epoch 28/50), acc = 0.79, loss = 0.81 (73.0 examples/sec; 1.369 sec/batch), lr: 0.004877\n",
      "2018-11-29 19:49:57.134570: step 7930/14650 (epoch 28/50), acc = 0.90, loss = 0.49 (73.7 examples/sec; 1.357 sec/batch), lr: 0.004877\n",
      "2018-11-29 19:51:04.687221: step 7940/14650 (epoch 28/50), acc = 0.79, loss = 0.92 (73.4 examples/sec; 1.363 sec/batch), lr: 0.004877\n",
      "2018-11-29 19:52:12.717312: step 7950/14650 (epoch 28/50), acc = 0.84, loss = 0.63 (77.5 examples/sec; 1.290 sec/batch), lr: 0.004877\n",
      "2018-11-29 19:53:20.576507: step 7960/14650 (epoch 28/50), acc = 0.93, loss = 0.47 (78.3 examples/sec; 1.276 sec/batch), lr: 0.004877\n",
      "2018-11-29 19:54:29.049683: step 7970/14650 (epoch 28/50), acc = 0.79, loss = 0.72 (81.6 examples/sec; 1.226 sec/batch), lr: 0.004877\n",
      "2018-11-29 19:55:37.118748: step 7980/14650 (epoch 28/50), acc = 0.79, loss = 0.80 (79.2 examples/sec; 1.263 sec/batch), lr: 0.004877\n",
      "2018-11-29 19:56:45.013143: step 7990/14650 (epoch 28/50), acc = 0.76, loss = 0.97 (73.4 examples/sec; 1.362 sec/batch), lr: 0.004877\n",
      "2018-11-29 19:57:52.934741: step 8000/14650 (epoch 28/50), acc = 0.80, loss = 0.72 (71.4 examples/sec; 1.401 sec/batch), lr: 0.004877\n",
      "\n",
      "Step 8000: train_loss = 0.740724, train_accuracy = 0.831\n",
      "Step 8000:  test_loss = 0.923487,  test_accuracy = 0.853\n",
      "\n",
      "2018-11-29 19:59:05.532783: step 8010/14650 (epoch 28/50), acc = 0.84, loss = 0.66 (76.6 examples/sec; 1.305 sec/batch), lr: 0.004877\n",
      "2018-11-29 20:00:14.188381: step 8020/14650 (epoch 28/50), acc = 0.84, loss = 0.66 (73.2 examples/sec; 1.365 sec/batch), lr: 0.004877\n",
      "2018-11-29 20:01:22.433926: step 8030/14650 (epoch 28/50), acc = 0.85, loss = 0.66 (73.3 examples/sec; 1.364 sec/batch), lr: 0.004877\n",
      "2018-11-29 20:02:30.997556: step 8040/14650 (epoch 28/50), acc = 0.86, loss = 0.58 (67.9 examples/sec; 1.474 sec/batch), lr: 0.004877\n",
      "2018-11-29 20:03:39.736330: step 8050/14650 (epoch 28/50), acc = 0.87, loss = 0.77 (70.6 examples/sec; 1.416 sec/batch), lr: 0.004877\n",
      "2018-11-29 20:04:49.418588: step 8060/14650 (epoch 28/50), acc = 0.91, loss = 0.48 (101.4 examples/sec; 0.986 sec/batch), lr: 0.004877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-29 20:05:58.375114: step 8070/14650 (epoch 28/50), acc = 0.87, loss = 0.50 (77.0 examples/sec; 1.299 sec/batch), lr: 0.004877\n",
      "2018-11-29 20:07:07.907363: step 8080/14650 (epoch 28/50), acc = 0.80, loss = 0.76 (79.6 examples/sec; 1.256 sec/batch), lr: 0.004877\n",
      "2018-11-29 20:08:17.876788: step 8090/14650 (epoch 28/50), acc = 0.83, loss = 0.71 (91.2 examples/sec; 1.097 sec/batch), lr: 0.004877\n",
      "2018-11-29 20:09:27.903307: step 8100/14650 (epoch 28/50), acc = 0.81, loss = 0.99 (87.1 examples/sec; 1.148 sec/batch), lr: 0.004877\n",
      "2018-11-29 20:10:38.591299: step 8110/14650 (epoch 28/50), acc = 0.78, loss = 0.94 (83.1 examples/sec; 1.203 sec/batch), lr: 0.004877\n",
      "2018-11-29 20:11:48.512812: step 8120/14650 (epoch 28/50), acc = 0.83, loss = 0.75 (82.5 examples/sec; 1.212 sec/batch), lr: 0.004877\n",
      "2018-11-29 20:12:58.362641: step 8130/14650 (epoch 28/50), acc = 0.90, loss = 0.55 (86.8 examples/sec; 1.152 sec/batch), lr: 0.004877\n",
      "2018-11-29 20:14:08.375983: step 8140/14650 (epoch 28/50), acc = 0.82, loss = 0.76 (82.8 examples/sec; 1.208 sec/batch), lr: 0.004877\n",
      "2018-11-29 20:15:19.291445: step 8150/14650 (epoch 28/50), acc = 0.90, loss = 0.50 (78.6 examples/sec; 1.272 sec/batch), lr: 0.004877\n",
      "2018-11-29 20:16:29.844924: step 8160/14650 (epoch 28/50), acc = 0.84, loss = 0.73 (94.2 examples/sec; 1.061 sec/batch), lr: 0.004877\n",
      "2018-11-29 20:17:40.347888: step 8170/14650 (epoch 28/50), acc = 0.82, loss = 0.65 (87.4 examples/sec; 1.144 sec/batch), lr: 0.004877\n",
      "2018-11-29 20:18:50.708253: step 8180/14650 (epoch 28/50), acc = 0.87, loss = 0.55 (86.7 examples/sec; 1.153 sec/batch), lr: 0.004877\n",
      "2018-11-29 20:20:01.502954: step 8190/14650 (epoch 28/50), acc = 0.76, loss = 0.83 (87.5 examples/sec; 1.143 sec/batch), lr: 0.004877\n",
      "2018-11-29 20:21:13.227401: step 8200/14650 (epoch 28/50), acc = 0.76, loss = 0.91 (82.0 examples/sec; 1.220 sec/batch), lr: 0.004877\n",
      "\n",
      "Step 8200: train_loss = 0.725238, train_accuracy = 0.831\n",
      "Step 8200:  test_loss = 0.882592,  test_accuracy = 0.854\n",
      "\n",
      "2018-11-29 20:22:29.398629: step 8210/14650 (epoch 29/50), acc = 0.80, loss = 0.79 (85.7 examples/sec; 1.167 sec/batch), lr: 0.004877\n",
      "2018-11-29 20:23:41.534907: step 8220/14650 (epoch 29/50), acc = 0.90, loss = 0.48 (74.6 examples/sec; 1.340 sec/batch), lr: 0.004877\n",
      "2018-11-29 20:24:53.484699: step 8230/14650 (epoch 29/50), acc = 0.79, loss = 0.72 (83.1 examples/sec; 1.204 sec/batch), lr: 0.004877\n",
      "2018-11-29 20:26:05.922285: step 8240/14650 (epoch 29/50), acc = 0.81, loss = 0.85 (82.9 examples/sec; 1.206 sec/batch), lr: 0.004877\n",
      "2018-11-29 20:27:18.946467: step 8250/14650 (epoch 29/50), acc = 0.77, loss = 0.75 (81.9 examples/sec; 1.222 sec/batch), lr: 0.004877\n",
      "2018-11-29 20:28:31.712897: step 8260/14650 (epoch 29/50), acc = 0.80, loss = 0.73 (91.9 examples/sec; 1.089 sec/batch), lr: 0.004877\n",
      "2018-11-29 20:29:44.812398: step 8270/14650 (epoch 29/50), acc = 0.82, loss = 0.69 (101.6 examples/sec; 0.984 sec/batch), lr: 0.004877\n",
      "2018-11-29 20:30:57.640867: step 8280/14650 (epoch 29/50), acc = 0.80, loss = 0.77 (89.8 examples/sec; 1.113 sec/batch), lr: 0.004877\n",
      "2018-11-29 20:32:11.094117: step 8290/14650 (epoch 29/50), acc = 0.82, loss = 0.76 (100.0 examples/sec; 1.000 sec/batch), lr: 0.004877\n",
      "2018-11-29 20:33:24.759300: step 8300/14650 (epoch 29/50), acc = 0.83, loss = 0.67 (88.7 examples/sec; 1.128 sec/batch), lr: 0.004877\n",
      "2018-11-29 20:34:38.465712: step 8310/14650 (epoch 29/50), acc = 0.86, loss = 0.53 (92.1 examples/sec; 1.086 sec/batch), lr: 0.004877\n",
      "2018-11-29 20:35:52.593780: step 8320/14650 (epoch 29/50), acc = 0.86, loss = 0.69 (99.6 examples/sec; 1.004 sec/batch), lr: 0.004877\n",
      "2018-11-29 20:37:06.411408: step 8330/14650 (epoch 29/50), acc = 0.86, loss = 0.58 (99.4 examples/sec; 1.006 sec/batch), lr: 0.004877\n",
      "2018-11-29 20:38:20.188961: step 8340/14650 (epoch 29/50), acc = 0.82, loss = 0.64 (90.8 examples/sec; 1.101 sec/batch), lr: 0.004877\n",
      "2018-11-29 20:39:34.417167: step 8350/14650 (epoch 29/50), acc = 0.82, loss = 0.61 (100.0 examples/sec; 1.000 sec/batch), lr: 0.004877\n",
      "2018-11-29 20:40:48.278897: step 8360/14650 (epoch 29/50), acc = 0.86, loss = 0.67 (98.1 examples/sec; 1.019 sec/batch), lr: 0.004877\n",
      "2018-11-29 20:42:02.258353: step 8370/14650 (epoch 29/50), acc = 0.80, loss = 0.61 (99.1 examples/sec; 1.009 sec/batch), lr: 0.004877\n",
      "2018-11-29 20:43:16.179668: step 8380/14650 (epoch 29/50), acc = 0.85, loss = 0.64 (98.5 examples/sec; 1.016 sec/batch), lr: 0.004877\n",
      "2018-11-29 20:44:30.350026: step 8390/14650 (epoch 29/50), acc = 0.89, loss = 0.67 (98.7 examples/sec; 1.014 sec/batch), lr: 0.004877\n",
      "2018-11-29 20:45:44.973425: step 8400/14650 (epoch 29/50), acc = 0.83, loss = 0.74 (98.9 examples/sec; 1.011 sec/batch), lr: 0.004877\n",
      "\n",
      "Step 8400: train_loss = 0.720087, train_accuracy = 0.828\n",
      "Step 8400:  test_loss = 0.844091,  test_accuracy = 0.862\n",
      "\n",
      "2018-11-29 20:47:03.581020: step 8410/14650 (epoch 29/50), acc = 0.82, loss = 0.68 (99.6 examples/sec; 1.004 sec/batch), lr: 0.004877\n",
      "2018-11-29 20:48:17.525986: step 8420/14650 (epoch 29/50), acc = 0.83, loss = 0.75 (98.6 examples/sec; 1.015 sec/batch), lr: 0.004877\n",
      "2018-11-29 20:49:31.274537: step 8430/14650 (epoch 29/50), acc = 0.79, loss = 0.77 (93.5 examples/sec; 1.070 sec/batch), lr: 0.004877\n",
      "2018-11-29 20:50:45.530575: step 8440/14650 (epoch 29/50), acc = 0.87, loss = 0.59 (99.6 examples/sec; 1.004 sec/batch), lr: 0.004877\n",
      "2018-11-29 20:52:00.389178: step 8450/14650 (epoch 29/50), acc = 0.83, loss = 0.69 (98.0 examples/sec; 1.020 sec/batch), lr: 0.004877\n",
      "2018-11-29 20:53:15.368539: step 8460/14650 (epoch 29/50), acc = 0.79, loss = 0.83 (97.9 examples/sec; 1.021 sec/batch), lr: 0.004877\n",
      "2018-11-29 20:54:29.752368: step 8470/14650 (epoch 29/50), acc = 0.84, loss = 0.69 (98.6 examples/sec; 1.015 sec/batch), lr: 0.004877\n",
      "2018-11-29 20:55:44.826848: step 8480/14650 (epoch 29/50), acc = 0.86, loss = 0.59 (97.4 examples/sec; 1.027 sec/batch), lr: 0.004877\n",
      "2018-11-29 20:56:59.799365: step 8490/14650 (epoch 29/50), acc = 0.86, loss = 0.59 (98.7 examples/sec; 1.014 sec/batch), lr: 0.004877\n",
      "2018-11-29 20:58:16.406882: step 8500/14650 (epoch 30/50), acc = 0.85, loss = 0.65 (97.4 examples/sec; 1.027 sec/batch), lr: 0.004877\n",
      "2018-11-29 20:59:32.915760: step 8510/14650 (epoch 30/50), acc = 0.90, loss = 0.54 (97.1 examples/sec; 1.029 sec/batch), lr: 0.004877\n",
      "2018-11-29 21:00:48.021249: step 8520/14650 (epoch 30/50), acc = 0.87, loss = 0.59 (97.0 examples/sec; 1.031 sec/batch), lr: 0.004877\n",
      "2018-11-29 21:02:05.227885: step 8530/14650 (epoch 30/50), acc = 0.83, loss = 0.68 (97.4 examples/sec; 1.027 sec/batch), lr: 0.004877\n",
      "2018-11-29 21:03:21.742912: step 8540/14650 (epoch 30/50), acc = 0.82, loss = 0.76 (97.2 examples/sec; 1.029 sec/batch), lr: 0.004877\n",
      "2018-11-29 21:04:37.182708: step 8550/14650 (epoch 30/50), acc = 0.81, loss = 0.71 (98.3 examples/sec; 1.017 sec/batch), lr: 0.004877\n",
      "2018-11-29 21:05:54.034288: step 8560/14650 (epoch 30/50), acc = 0.82, loss = 0.82 (96.7 examples/sec; 1.034 sec/batch), lr: 0.004877\n",
      "2018-11-29 21:07:11.049826: step 8570/14650 (epoch 30/50), acc = 0.85, loss = 0.78 (96.9 examples/sec; 1.032 sec/batch), lr: 0.004877\n",
      "2018-11-29 21:08:27.948051: step 8580/14650 (epoch 30/50), acc = 0.85, loss = 0.61 (98.1 examples/sec; 1.020 sec/batch), lr: 0.004877\n",
      "2018-11-29 21:09:45.544138: step 8590/14650 (epoch 30/50), acc = 0.85, loss = 0.61 (52.8 examples/sec; 1.892 sec/batch), lr: 0.004877\n",
      "2018-11-29 21:11:02.515985: step 8600/14650 (epoch 30/50), acc = 0.89, loss = 0.48 (95.9 examples/sec; 1.043 sec/batch), lr: 0.004877\n",
      "\n",
      "Step 8600: train_loss = 0.696362, train_accuracy = 0.836\n",
      "Step 8600:  test_loss = 0.841459,  test_accuracy = 0.856\n",
      "\n",
      "2018-11-29 21:12:25.325648: step 8610/14650 (epoch 30/50), acc = 0.83, loss = 0.90 (96.4 examples/sec; 1.037 sec/batch), lr: 0.004877\n",
      "2018-11-29 21:13:43.730832: step 8620/14650 (epoch 30/50), acc = 0.86, loss = 0.56 (96.3 examples/sec; 1.038 sec/batch), lr: 0.004877\n",
      "2018-11-29 21:15:00.858138: step 8630/14650 (epoch 30/50), acc = 0.75, loss = 1.00 (96.6 examples/sec; 1.035 sec/batch), lr: 0.004877\n",
      "2018-11-29 21:16:17.544511: step 8640/14650 (epoch 30/50), acc = 0.79, loss = 0.82 (96.7 examples/sec; 1.034 sec/batch), lr: 0.004877\n",
      "2018-11-29 21:17:35.364743: step 8650/14650 (epoch 30/50), acc = 0.88, loss = 0.64 (96.2 examples/sec; 1.039 sec/batch), lr: 0.004877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-29 21:18:52.283074: step 8660/14650 (epoch 30/50), acc = 0.83, loss = 0.84 (96.6 examples/sec; 1.035 sec/batch), lr: 0.004877\n",
      "2018-11-29 21:20:10.400395: step 8670/14650 (epoch 30/50), acc = 0.81, loss = 0.66 (96.0 examples/sec; 1.042 sec/batch), lr: 0.004877\n",
      "2018-11-29 21:21:27.564975: step 8680/14650 (epoch 30/50), acc = 0.88, loss = 0.56 (96.2 examples/sec; 1.039 sec/batch), lr: 0.004877\n",
      "2018-11-29 21:22:46.488763: step 8690/14650 (epoch 30/50), acc = 0.84, loss = 0.62 (97.5 examples/sec; 1.025 sec/batch), lr: 0.004877\n",
      "2018-11-29 21:24:05.148486: step 8700/14650 (epoch 30/50), acc = 0.77, loss = 0.77 (96.2 examples/sec; 1.039 sec/batch), lr: 0.004877\n",
      "2018-11-29 21:25:24.244222: step 8710/14650 (epoch 30/50), acc = 0.82, loss = 0.75 (95.2 examples/sec; 1.051 sec/batch), lr: 0.004877\n",
      "2018-11-29 21:26:42.575527: step 8720/14650 (epoch 30/50), acc = 0.79, loss = 0.91 (95.5 examples/sec; 1.048 sec/batch), lr: 0.004877\n",
      "2018-11-29 21:28:00.201832: step 8730/14650 (epoch 30/50), acc = 0.82, loss = 0.62 (95.1 examples/sec; 1.052 sec/batch), lr: 0.004877\n",
      "2018-11-29 21:29:19.350228: step 8740/14650 (epoch 30/50), acc = 0.87, loss = 0.61 (95.5 examples/sec; 1.048 sec/batch), lr: 0.004877\n",
      "2018-11-29 21:30:39.433629: step 8750/14650 (epoch 30/50), acc = 0.83, loss = 0.67 (52.4 examples/sec; 1.909 sec/batch), lr: 0.004877\n",
      "2018-11-29 21:31:58.594274: step 8760/14650 (epoch 30/50), acc = 0.81, loss = 0.80 (95.5 examples/sec; 1.048 sec/batch), lr: 0.004877\n",
      "2018-11-29 21:33:17.968793: step 8770/14650 (epoch 30/50), acc = 0.81, loss = 0.76 (95.0 examples/sec; 1.053 sec/batch), lr: 0.004877\n",
      "2018-11-29 21:34:37.675041: step 8780/14650 (epoch 30/50), acc = 0.80, loss = 0.82 (95.4 examples/sec; 1.048 sec/batch), lr: 0.004877\n",
      "2018-11-29 21:35:58.288932: step 8790/14650 (epoch 30/50), acc = 0.40, loss = 0.87 (99.0 examples/sec; 1.010 sec/batch), lr: 0.004877\n",
      "2018-11-29 21:37:18.202778: step 8800/14650 (epoch 31/50), acc = 0.89, loss = 0.47 (94.7 examples/sec; 1.056 sec/batch), lr: 0.004877\n",
      "\n",
      "Step 8800: train_loss = 0.718848, train_accuracy = 0.841\n",
      "Step 8800:  test_loss = 0.893979,  test_accuracy = 0.854\n",
      "\n",
      "2018-11-29 21:38:44.949860: step 8810/14650 (epoch 31/50), acc = 0.82, loss = 0.73 (56.8 examples/sec; 1.762 sec/batch), lr: 0.004877\n",
      "2018-11-29 21:40:05.041689: step 8820/14650 (epoch 31/50), acc = 0.84, loss = 0.64 (94.9 examples/sec; 1.054 sec/batch), lr: 0.004877\n",
      "2018-11-29 21:41:26.135539: step 8830/14650 (epoch 31/50), acc = 0.82, loss = 0.68 (94.3 examples/sec; 1.060 sec/batch), lr: 0.004877\n",
      "2018-11-29 21:42:47.604404: step 8840/14650 (epoch 31/50), acc = 0.82, loss = 0.75 (93.7 examples/sec; 1.067 sec/batch), lr: 0.004877\n",
      "2018-11-29 21:44:09.545017: step 8850/14650 (epoch 31/50), acc = 0.84, loss = 0.63 (93.7 examples/sec; 1.068 sec/batch), lr: 0.004877\n",
      "2018-11-29 21:45:31.299751: step 8860/14650 (epoch 31/50), acc = 0.85, loss = 0.72 (94.3 examples/sec; 1.060 sec/batch), lr: 0.004877\n",
      "2018-11-29 21:46:54.077652: step 8870/14650 (epoch 31/50), acc = 0.81, loss = 0.81 (51.7 examples/sec; 1.935 sec/batch), lr: 0.004877\n",
      "2018-11-29 21:48:14.865040: step 8880/14650 (epoch 31/50), acc = 0.84, loss = 0.72 (92.3 examples/sec; 1.084 sec/batch), lr: 0.004877\n",
      "2018-11-29 21:49:36.072340: step 8890/14650 (epoch 31/50), acc = 0.87, loss = 1.43 (93.5 examples/sec; 1.069 sec/batch), lr: 0.004877\n",
      "2018-11-29 21:50:57.713957: step 8900/14650 (epoch 31/50), acc = 0.80, loss = 0.75 (93.0 examples/sec; 1.075 sec/batch), lr: 0.004877\n",
      "2018-11-29 21:52:20.121674: step 8910/14650 (epoch 31/50), acc = 0.81, loss = 0.74 (93.3 examples/sec; 1.072 sec/batch), lr: 0.004877\n",
      "2018-11-29 21:53:41.990734: step 8920/14650 (epoch 31/50), acc = 0.88, loss = 0.55 (93.7 examples/sec; 1.068 sec/batch), lr: 0.004877\n",
      "2018-11-29 21:55:05.363847: step 8930/14650 (epoch 31/50), acc = 0.83, loss = 0.69 (93.6 examples/sec; 1.068 sec/batch), lr: 0.004877\n",
      "2018-11-29 21:56:27.959381: step 8940/14650 (epoch 31/50), acc = 0.83, loss = 0.76 (93.5 examples/sec; 1.070 sec/batch), lr: 0.004877\n",
      "2018-11-29 21:57:51.733657: step 8950/14650 (epoch 31/50), acc = 0.90, loss = 0.44 (93.1 examples/sec; 1.074 sec/batch), lr: 0.004877\n",
      "2018-11-29 21:58:33.528730: step 8955/14650 (epoch 31/50), Learning rate decays to 0.00463\n",
      "2018-11-29 21:59:16.016536: step 8960/14650 (epoch 31/50), acc = 0.79, loss = 0.74 (58.1 examples/sec; 1.720 sec/batch), lr: 0.004633\n",
      "2018-11-29 22:00:39.087470: step 8970/14650 (epoch 31/50), acc = 0.86, loss = 0.57 (92.6 examples/sec; 1.080 sec/batch), lr: 0.004633\n",
      "2018-11-29 22:02:01.735812: step 8980/14650 (epoch 31/50), acc = 0.82, loss = 0.64 (93.2 examples/sec; 1.073 sec/batch), lr: 0.004633\n",
      "2018-11-29 22:03:24.920150: step 8990/14650 (epoch 31/50), acc = 0.88, loss = 0.66 (93.3 examples/sec; 1.072 sec/batch), lr: 0.004633\n",
      "2018-11-29 22:04:49.212878: step 9000/14650 (epoch 31/50), acc = 0.86, loss = 0.60 (94.0 examples/sec; 1.064 sec/batch), lr: 0.004633\n",
      "\n",
      "Step 9000: train_loss = 0.698752, train_accuracy = 0.834\n",
      "Step 9000:  test_loss = 0.899782,  test_accuracy = 0.863\n",
      "\n",
      "2018-11-29 22:06:18.637082: step 9010/14650 (epoch 31/50), acc = 0.84, loss = 0.66 (92.7 examples/sec; 1.078 sec/batch), lr: 0.004633\n",
      "2018-11-29 22:07:42.650657: step 9020/14650 (epoch 31/50), acc = 0.89, loss = 0.75 (92.3 examples/sec; 1.084 sec/batch), lr: 0.004633\n",
      "2018-11-29 22:09:08.765757: step 9030/14650 (epoch 31/50), acc = 0.80, loss = 0.71 (91.4 examples/sec; 1.094 sec/batch), lr: 0.004633\n",
      "2018-11-29 22:10:33.440548: step 9040/14650 (epoch 31/50), acc = 0.82, loss = 0.78 (93.1 examples/sec; 1.074 sec/batch), lr: 0.004633\n",
      "2018-11-29 22:11:59.269315: step 9050/14650 (epoch 31/50), acc = 0.80, loss = 0.72 (92.7 examples/sec; 1.079 sec/batch), lr: 0.004633\n",
      "2018-11-29 22:13:26.027678: step 9060/14650 (epoch 31/50), acc = 0.83, loss = 0.75 (92.5 examples/sec; 1.081 sec/batch), lr: 0.004633\n",
      "2018-11-29 22:14:54.023394: step 9070/14650 (epoch 31/50), acc = 0.73, loss = 0.82 (54.3 examples/sec; 1.840 sec/batch), lr: 0.004633\n",
      "2018-11-29 22:16:20.662850: step 9080/14650 (epoch 31/50), acc = 0.81, loss = 0.75 (92.5 examples/sec; 1.081 sec/batch), lr: 0.004633\n",
      "2018-11-29 22:17:47.200930: step 9090/14650 (epoch 32/50), acc = 0.89, loss = 0.52 (92.4 examples/sec; 1.082 sec/batch), lr: 0.004633\n",
      "2018-11-29 22:19:13.767158: step 9100/14650 (epoch 32/50), acc = 0.90, loss = 0.46 (91.1 examples/sec; 1.097 sec/batch), lr: 0.004633\n",
      "2018-11-29 22:20:40.741315: step 9110/14650 (epoch 32/50), acc = 0.85, loss = 0.55 (91.5 examples/sec; 1.093 sec/batch), lr: 0.004633\n",
      "2018-11-29 22:22:07.360943: step 9120/14650 (epoch 32/50), acc = 0.83, loss = 0.74 (91.6 examples/sec; 1.092 sec/batch), lr: 0.004633\n",
      "2018-11-29 22:23:33.961113: step 9130/14650 (epoch 32/50), acc = 0.79, loss = 0.80 (91.6 examples/sec; 1.092 sec/batch), lr: 0.004633\n",
      "2018-11-29 22:25:02.172806: step 9140/14650 (epoch 32/50), acc = 0.85, loss = 0.65 (50.6 examples/sec; 1.977 sec/batch), lr: 0.004633\n",
      "2018-11-29 22:26:28.984187: step 9150/14650 (epoch 32/50), acc = 0.84, loss = 0.71 (93.2 examples/sec; 1.073 sec/batch), lr: 0.004633\n",
      "2018-11-29 22:27:55.796115: step 9160/14650 (epoch 32/50), acc = 0.84, loss = 0.58 (91.4 examples/sec; 1.094 sec/batch), lr: 0.004633\n",
      "2018-11-29 22:29:24.509348: step 9170/14650 (epoch 32/50), acc = 0.84, loss = 0.68 (63.4 examples/sec; 1.578 sec/batch), lr: 0.004633\n",
      "2018-11-29 22:30:51.872300: step 9180/14650 (epoch 32/50), acc = 0.82, loss = 0.66 (91.2 examples/sec; 1.097 sec/batch), lr: 0.004633\n",
      "2018-11-29 22:32:19.242027: step 9190/14650 (epoch 32/50), acc = 0.81, loss = 0.74 (91.1 examples/sec; 1.098 sec/batch), lr: 0.004633\n",
      "2018-11-29 22:33:48.430879: step 9200/14650 (epoch 32/50), acc = 0.90, loss = 0.55 (69.9 examples/sec; 1.431 sec/batch), lr: 0.004633\n",
      "\n",
      "Step 9200: train_loss = 0.684590, train_accuracy = 0.839\n",
      "Step 9200:  test_loss = 0.921219,  test_accuracy = 0.859\n",
      "\n",
      "2018-11-29 22:35:22.599734: step 9210/14650 (epoch 32/50), acc = 0.80, loss = 0.92 (89.7 examples/sec; 1.115 sec/batch), lr: 0.004633\n",
      "2018-11-29 22:36:51.814720: step 9220/14650 (epoch 32/50), acc = 0.83, loss = 0.71 (60.6 examples/sec; 1.651 sec/batch), lr: 0.004633\n",
      "2018-11-29 22:38:19.979246: step 9230/14650 (epoch 32/50), acc = 0.86, loss = 0.72 (91.2 examples/sec; 1.097 sec/batch), lr: 0.004633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-29 22:39:49.773038: step 9240/14650 (epoch 32/50), acc = 0.82, loss = 0.68 (90.0 examples/sec; 1.111 sec/batch), lr: 0.004633\n",
      "2018-11-29 22:41:19.985445: step 9250/14650 (epoch 32/50), acc = 0.84, loss = 1.79 (90.1 examples/sec; 1.110 sec/batch), lr: 0.004633\n",
      "2018-11-29 22:42:49.236828: step 9260/14650 (epoch 32/50), acc = 0.82, loss = 0.77 (60.9 examples/sec; 1.641 sec/batch), lr: 0.004633\n",
      "2018-11-29 22:44:18.292821: step 9270/14650 (epoch 32/50), acc = 0.81, loss = 0.80 (89.7 examples/sec; 1.115 sec/batch), lr: 0.004633\n",
      "2018-11-29 22:45:48.854564: step 9280/14650 (epoch 32/50), acc = 0.79, loss = 0.67 (89.8 examples/sec; 1.113 sec/batch), lr: 0.004633\n",
      "2018-11-29 22:47:18.872351: step 9290/14650 (epoch 32/50), acc = 0.76, loss = 0.80 (89.5 examples/sec; 1.117 sec/batch), lr: 0.004633\n",
      "2018-11-29 22:48:49.143212: step 9300/14650 (epoch 32/50), acc = 0.88, loss = 0.63 (90.4 examples/sec; 1.106 sec/batch), lr: 0.004633\n",
      "2018-11-29 22:50:18.788257: step 9310/14650 (epoch 32/50), acc = 0.81, loss = 0.73 (49.5 examples/sec; 2.020 sec/batch), lr: 0.004633\n",
      "2018-11-29 22:51:48.695216: step 9320/14650 (epoch 32/50), acc = 0.86, loss = 0.64 (64.0 examples/sec; 1.564 sec/batch), lr: 0.004633\n",
      "2018-11-29 22:53:17.164553: step 9330/14650 (epoch 32/50), acc = 0.85, loss = 0.68 (88.9 examples/sec; 1.125 sec/batch), lr: 0.004633\n",
      "2018-11-29 22:54:46.653644: step 9340/14650 (epoch 32/50), acc = 0.83, loss = 0.72 (50.0 examples/sec; 1.998 sec/batch), lr: 0.004633\n",
      "2018-11-29 22:56:15.235722: step 9350/14650 (epoch 32/50), acc = 0.85, loss = 0.80 (89.6 examples/sec; 1.116 sec/batch), lr: 0.004633\n",
      "2018-11-29 22:57:44.619111: step 9360/14650 (epoch 32/50), acc = 0.83, loss = 0.72 (63.3 examples/sec; 1.580 sec/batch), lr: 0.004633\n",
      "2018-11-29 22:59:15.552764: step 9370/14650 (epoch 32/50), acc = 0.83, loss = 0.66 (49.2 examples/sec; 2.033 sec/batch), lr: 0.004633\n",
      "2018-11-29 23:00:46.559446: step 9380/14650 (epoch 33/50), acc = 0.83, loss = 0.66 (88.8 examples/sec; 1.126 sec/batch), lr: 0.004633\n",
      "2018-11-29 23:02:18.099651: step 9390/14650 (epoch 33/50), acc = 0.83, loss = 0.79 (88.1 examples/sec; 1.135 sec/batch), lr: 0.004633\n",
      "2018-11-29 23:03:49.755445: step 9400/14650 (epoch 33/50), acc = 0.86, loss = 0.59 (89.4 examples/sec; 1.119 sec/batch), lr: 0.004633\n",
      "\n",
      "Step 9400: train_loss = 0.754466, train_accuracy = 0.823\n",
      "Step 9400:  test_loss = 0.880999,  test_accuracy = 0.856\n",
      "\n",
      "2018-11-29 23:05:27.673472: step 9410/14650 (epoch 33/50), acc = 0.88, loss = 0.67 (89.2 examples/sec; 1.121 sec/batch), lr: 0.004633\n",
      "2018-11-29 23:07:01.227641: step 9420/14650 (epoch 33/50), acc = 0.79, loss = 1.03 (51.5 examples/sec; 1.940 sec/batch), lr: 0.004633\n",
      "2018-11-29 23:08:33.454106: step 9430/14650 (epoch 33/50), acc = 0.82, loss = 0.70 (88.9 examples/sec; 1.124 sec/batch), lr: 0.004633\n",
      "2018-11-29 23:10:05.561939: step 9440/14650 (epoch 33/50), acc = 0.85, loss = 0.69 (88.5 examples/sec; 1.130 sec/batch), lr: 0.004633\n",
      "2018-11-29 23:11:37.574840: step 9450/14650 (epoch 33/50), acc = 0.85, loss = 0.61 (65.3 examples/sec; 1.532 sec/batch), lr: 0.004633\n",
      "2018-11-29 23:12:22.690530: step 9455/14650 (epoch 33/50), Learning rate decays to 0.00440\n",
      "2018-11-29 23:13:09.955758: step 9460/14650 (epoch 33/50), acc = 0.83, loss = 0.77 (48.2 examples/sec; 2.076 sec/batch), lr: 0.004401\n",
      "2018-11-29 23:14:42.754111: step 9470/14650 (epoch 33/50), acc = 0.84, loss = 0.63 (85.9 examples/sec; 1.164 sec/batch), lr: 0.004401\n",
      "2018-11-29 23:16:16.663084: step 9480/14650 (epoch 33/50), acc = 0.81, loss = 0.80 (52.4 examples/sec; 1.909 sec/batch), lr: 0.004401\n",
      "2018-11-29 23:17:51.256720: step 9490/14650 (epoch 33/50), acc = 0.78, loss = 0.88 (56.8 examples/sec; 1.760 sec/batch), lr: 0.004401\n",
      "2018-11-29 23:19:26.266504: step 9500/14650 (epoch 33/50), acc = 0.78, loss = 0.85 (56.1 examples/sec; 1.783 sec/batch), lr: 0.004401\n",
      "2018-11-29 23:21:00.920097: step 9510/14650 (epoch 33/50), acc = 0.86, loss = 0.70 (51.8 examples/sec; 1.929 sec/batch), lr: 0.004401\n",
      "2018-11-29 23:22:35.825231: step 9520/14650 (epoch 33/50), acc = 0.79, loss = 0.66 (52.9 examples/sec; 1.890 sec/batch), lr: 0.004401\n",
      "2018-11-29 23:24:10.370795: step 9530/14650 (epoch 33/50), acc = 0.78, loss = 0.76 (48.3 examples/sec; 2.071 sec/batch), lr: 0.004401\n",
      "2018-11-29 23:25:45.885203: step 9540/14650 (epoch 33/50), acc = 0.84, loss = 0.99 (58.9 examples/sec; 1.698 sec/batch), lr: 0.004401\n",
      "2018-11-29 23:27:21.355301: step 9550/14650 (epoch 33/50), acc = 0.80, loss = 0.74 (59.1 examples/sec; 1.692 sec/batch), lr: 0.004401\n",
      "2018-11-29 23:28:56.235590: step 9560/14650 (epoch 33/50), acc = 0.79, loss = 0.67 (55.5 examples/sec; 1.803 sec/batch), lr: 0.004401\n",
      "2018-11-29 23:30:31.956897: step 9570/14650 (epoch 33/50), acc = 0.86, loss = 0.47 (68.8 examples/sec; 1.454 sec/batch), lr: 0.004401\n",
      "2018-11-29 23:32:07.832808: step 9580/14650 (epoch 33/50), acc = 0.86, loss = 0.49 (56.2 examples/sec; 1.781 sec/batch), lr: 0.004401\n",
      "2018-11-29 23:33:42.430025: step 9590/14650 (epoch 33/50), acc = 0.92, loss = 0.62 (47.9 examples/sec; 2.086 sec/batch), lr: 0.004401\n",
      "2018-11-29 23:35:17.716159: step 9600/14650 (epoch 33/50), acc = 0.84, loss = 0.63 (58.7 examples/sec; 1.704 sec/batch), lr: 0.004401\n",
      "\n",
      "Step 9600: train_loss = 0.701277, train_accuracy = 0.834\n",
      "Step 9600:  test_loss = 1.027438,  test_accuracy = 0.862\n",
      "\n",
      "2018-11-29 23:36:57.625209: step 9610/14650 (epoch 33/50), acc = 0.78, loss = 0.77 (87.1 examples/sec; 1.149 sec/batch), lr: 0.004401\n",
      "2018-11-29 23:38:33.419069: step 9620/14650 (epoch 33/50), acc = 0.82, loss = 0.80 (86.2 examples/sec; 1.160 sec/batch), lr: 0.004401\n",
      "2018-11-29 23:40:08.734082: step 9630/14650 (epoch 33/50), acc = 0.86, loss = 0.60 (87.2 examples/sec; 1.146 sec/batch), lr: 0.004401\n",
      "2018-11-29 23:41:44.681467: step 9640/14650 (epoch 33/50), acc = 0.80, loss = 0.71 (86.3 examples/sec; 1.159 sec/batch), lr: 0.004401\n",
      "2018-11-29 23:43:20.589387: step 9650/14650 (epoch 33/50), acc = 0.82, loss = 0.67 (87.0 examples/sec; 1.150 sec/batch), lr: 0.004401\n",
      "2018-11-29 23:44:56.282873: step 9660/14650 (epoch 33/50), acc = 0.85, loss = 0.64 (87.4 examples/sec; 1.144 sec/batch), lr: 0.004401\n",
      "2018-11-29 23:46:32.672337: step 9670/14650 (epoch 34/50), acc = 0.84, loss = 0.73 (85.7 examples/sec; 1.166 sec/batch), lr: 0.004401\n",
      "2018-11-29 23:48:09.614867: step 9680/14650 (epoch 34/50), acc = 0.80, loss = 0.83 (86.9 examples/sec; 1.150 sec/batch), lr: 0.004401\n",
      "2018-11-29 23:49:46.677582: step 9690/14650 (epoch 34/50), acc = 0.82, loss = 0.66 (86.2 examples/sec; 1.161 sec/batch), lr: 0.004401\n",
      "2018-11-29 23:51:23.676143: step 9700/14650 (epoch 34/50), acc = 0.89, loss = 0.49 (87.1 examples/sec; 1.148 sec/batch), lr: 0.004401\n",
      "2018-11-29 23:53:00.192972: step 9710/14650 (epoch 34/50), acc = 0.70, loss = 0.89 (86.5 examples/sec; 1.156 sec/batch), lr: 0.004401\n",
      "2018-11-29 23:54:36.790649: step 9720/14650 (epoch 34/50), acc = 0.83, loss = 0.73 (85.6 examples/sec; 1.168 sec/batch), lr: 0.004401\n",
      "2018-11-29 23:56:14.567872: step 9730/14650 (epoch 34/50), acc = 0.88, loss = 0.55 (86.2 examples/sec; 1.160 sec/batch), lr: 0.004401\n",
      "2018-11-29 23:57:52.844031: step 9740/14650 (epoch 34/50), acc = 0.86, loss = 0.55 (86.1 examples/sec; 1.161 sec/batch), lr: 0.004401\n",
      "2018-11-29 23:59:30.217958: step 9750/14650 (epoch 34/50), acc = 0.83, loss = 0.65 (87.3 examples/sec; 1.146 sec/batch), lr: 0.004401\n",
      "2018-11-30 00:01:09.090652: step 9760/14650 (epoch 34/50), acc = 0.85, loss = 0.59 (85.5 examples/sec; 1.169 sec/batch), lr: 0.004401\n",
      "2018-11-30 00:02:48.358899: step 9770/14650 (epoch 34/50), acc = 0.90, loss = 0.61 (84.6 examples/sec; 1.182 sec/batch), lr: 0.004401\n",
      "2018-11-30 00:04:27.015638: step 9780/14650 (epoch 34/50), acc = 0.80, loss = 0.86 (85.3 examples/sec; 1.172 sec/batch), lr: 0.004401\n",
      "2018-11-30 00:06:06.550883: step 9790/14650 (epoch 34/50), acc = 0.82, loss = 0.58 (86.0 examples/sec; 1.163 sec/batch), lr: 0.004401\n",
      "2018-11-30 00:07:45.565756: step 9800/14650 (epoch 34/50), acc = 0.86, loss = 0.68 (85.5 examples/sec; 1.170 sec/batch), lr: 0.004401\n",
      "\n",
      "Step 9800: train_loss = 0.663132, train_accuracy = 0.838\n",
      "Step 9800:  test_loss = 0.911532,  test_accuracy = 0.858\n",
      "\n",
      "2018-11-30 00:09:30.261110: step 9810/14650 (epoch 34/50), acc = 0.88, loss = 0.54 (86.0 examples/sec; 1.163 sec/batch), lr: 0.004401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-30 00:11:08.614894: step 9820/14650 (epoch 34/50), acc = 0.87, loss = 0.67 (86.0 examples/sec; 1.163 sec/batch), lr: 0.004401\n",
      "2018-11-30 00:12:46.964203: step 9830/14650 (epoch 34/50), acc = 0.88, loss = 0.45 (85.1 examples/sec; 1.175 sec/batch), lr: 0.004401\n",
      "2018-11-30 00:14:26.446772: step 9840/14650 (epoch 34/50), acc = 0.88, loss = 0.63 (84.8 examples/sec; 1.180 sec/batch), lr: 0.004401\n",
      "2018-11-30 00:16:06.415555: step 9850/14650 (epoch 34/50), acc = 0.85, loss = 0.64 (84.9 examples/sec; 1.177 sec/batch), lr: 0.004401\n",
      "2018-11-30 00:17:46.217260: step 9860/14650 (epoch 34/50), acc = 0.80, loss = 0.82 (85.8 examples/sec; 1.166 sec/batch), lr: 0.004401\n",
      "2018-11-30 00:19:25.594895: step 9870/14650 (epoch 34/50), acc = 0.85, loss = 0.73 (85.2 examples/sec; 1.173 sec/batch), lr: 0.004401\n",
      "2018-11-30 00:21:05.264135: step 9880/14650 (epoch 34/50), acc = 0.86, loss = 0.69 (85.3 examples/sec; 1.172 sec/batch), lr: 0.004401\n",
      "2018-11-30 00:22:45.261787: step 9890/14650 (epoch 34/50), acc = 0.80, loss = 0.77 (85.6 examples/sec; 1.169 sec/batch), lr: 0.004401\n",
      "2018-11-30 00:24:27.007019: step 9900/14650 (epoch 34/50), acc = 0.82, loss = 0.79 (46.2 examples/sec; 2.163 sec/batch), lr: 0.004401\n",
      "2018-11-30 00:26:07.289053: step 9910/14650 (epoch 34/50), acc = 0.84, loss = 0.68 (84.7 examples/sec; 1.180 sec/batch), lr: 0.004401\n",
      "2018-11-30 00:27:47.811748: step 9920/14650 (epoch 34/50), acc = 0.83, loss = 0.64 (84.4 examples/sec; 1.185 sec/batch), lr: 0.004401\n",
      "2018-11-30 00:29:29.216960: step 9930/14650 (epoch 34/50), acc = 0.79, loss = 0.91 (85.3 examples/sec; 1.173 sec/batch), lr: 0.004401\n",
      "2018-11-30 00:31:11.155811: step 9940/14650 (epoch 34/50), acc = 0.88, loss = 0.64 (84.5 examples/sec; 1.183 sec/batch), lr: 0.004401\n",
      "2018-11-30 00:32:53.854343: step 9950/14650 (epoch 34/50), acc = 0.81, loss = 0.80 (76.7 examples/sec; 1.303 sec/batch), lr: 0.004401\n",
      "2018-11-30 00:33:45.622568: step 9955/14650 (epoch 34/50), Learning rate decays to 0.00418\n",
      "2018-11-30 00:34:37.399988: step 9960/14650 (epoch 34/50), acc = 0.89, loss = 0.60 (84.2 examples/sec; 1.187 sec/batch), lr: 0.004181\n",
      "2018-11-30 00:36:21.777918: step 9970/14650 (epoch 35/50), acc = 0.86, loss = 0.62 (49.0 examples/sec; 2.042 sec/batch), lr: 0.004181\n",
      "2018-11-30 00:38:05.233204: step 9980/14650 (epoch 35/50), acc = 0.81, loss = 0.85 (85.0 examples/sec; 1.177 sec/batch), lr: 0.004181\n",
      "2018-11-30 00:39:51.805269: step 9990/14650 (epoch 35/50), acc = 0.91, loss = 0.51 (46.4 examples/sec; 2.156 sec/batch), lr: 0.004181\n",
      "2018-11-30 00:41:36.057316: step 10000/14650 (epoch 35/50), acc = 0.84, loss = 0.68 (45.5 examples/sec; 2.199 sec/batch), lr: 0.004181\n",
      "\n",
      "Step 10000: train_loss = 0.656648, train_accuracy = 0.850\n",
      "Step 10000:  test_loss = 0.855471,  test_accuracy = 0.861\n",
      "\n",
      "2018-11-30 00:43:27.329056: step 10010/14650 (epoch 35/50), acc = 0.85, loss = 0.54 (46.0 examples/sec; 2.176 sec/batch), lr: 0.004181\n",
      "2018-11-30 00:45:10.532170: step 10020/14650 (epoch 35/50), acc = 0.81, loss = 0.74 (84.5 examples/sec; 1.184 sec/batch), lr: 0.004181\n",
      "2018-11-30 00:46:55.916928: step 10030/14650 (epoch 35/50), acc = 0.86, loss = 0.74 (83.3 examples/sec; 1.200 sec/batch), lr: 0.004181\n",
      "2018-11-30 00:48:41.817606: step 10040/14650 (epoch 35/50), acc = 0.83, loss = 0.62 (46.1 examples/sec; 2.170 sec/batch), lr: 0.004181\n",
      "2018-11-30 00:50:28.590620: step 10050/14650 (epoch 35/50), acc = 0.87, loss = 0.54 (45.9 examples/sec; 2.180 sec/batch), lr: 0.004181\n",
      "2018-11-30 00:52:14.449823: step 10060/14650 (epoch 35/50), acc = 0.80, loss = 0.71 (83.7 examples/sec; 1.194 sec/batch), lr: 0.004181\n",
      "2018-11-30 00:53:59.475134: step 10070/14650 (epoch 35/50), acc = 0.87, loss = 0.55 (82.3 examples/sec; 1.215 sec/batch), lr: 0.004181\n",
      "2018-11-30 00:55:47.603414: step 10080/14650 (epoch 35/50), acc = 0.89, loss = 0.65 (49.4 examples/sec; 2.024 sec/batch), lr: 0.004181\n",
      "2018-11-30 00:57:34.133906: step 10090/14650 (epoch 35/50), acc = 0.81, loss = 0.69 (45.7 examples/sec; 2.187 sec/batch), lr: 0.004181\n",
      "2018-11-30 00:59:21.098047: step 10100/14650 (epoch 35/50), acc = 0.82, loss = 0.66 (46.1 examples/sec; 2.169 sec/batch), lr: 0.004181\n",
      "2018-11-30 01:01:09.812399: step 10110/14650 (epoch 35/50), acc = 0.87, loss = 0.51 (46.0 examples/sec; 2.173 sec/batch), lr: 0.004181\n",
      "2018-11-30 01:02:57.120018: step 10120/14650 (epoch 35/50), acc = 0.78, loss = 0.67 (82.6 examples/sec; 1.210 sec/batch), lr: 0.004181\n",
      "2018-11-30 01:04:45.285148: step 10130/14650 (epoch 35/50), acc = 0.84, loss = 0.59 (53.4 examples/sec; 1.873 sec/batch), lr: 0.004181\n",
      "2018-11-30 01:06:32.810470: step 10140/14650 (epoch 35/50), acc = 0.84, loss = 0.99 (83.4 examples/sec; 1.198 sec/batch), lr: 0.004181\n",
      "2018-11-30 01:08:21.226588: step 10150/14650 (epoch 35/50), acc = 0.89, loss = 0.40 (46.2 examples/sec; 2.167 sec/batch), lr: 0.004181\n",
      "2018-11-30 01:10:10.386306: step 10160/14650 (epoch 35/50), acc = 0.83, loss = 0.80 (44.9 examples/sec; 2.228 sec/batch), lr: 0.004181\n",
      "2018-11-30 01:12:00.787815: step 10170/14650 (epoch 35/50), acc = 0.84, loss = 0.59 (45.0 examples/sec; 2.224 sec/batch), lr: 0.004181\n",
      "2018-11-30 01:13:49.793883: step 10180/14650 (epoch 35/50), acc = 0.85, loss = 0.64 (55.7 examples/sec; 1.795 sec/batch), lr: 0.004181\n",
      "2018-11-30 01:15:38.982244: step 10190/14650 (epoch 35/50), acc = 0.79, loss = 0.68 (83.3 examples/sec; 1.201 sec/batch), lr: 0.004181\n",
      "2018-11-30 01:17:27.913804: step 10200/14650 (epoch 35/50), acc = 0.82, loss = 0.75 (45.1 examples/sec; 2.218 sec/batch), lr: 0.004181\n",
      "\n",
      "Step 10200: train_loss = 0.662979, train_accuracy = 0.838\n",
      "Step 10200:  test_loss = 0.808606,  test_accuracy = 0.865\n",
      "\n",
      "2018-11-30 01:19:24.545398: step 10210/14650 (epoch 35/50), acc = 0.80, loss = 0.74 (45.4 examples/sec; 2.202 sec/batch), lr: 0.004181\n",
      "2018-11-30 01:21:16.774036: step 10220/14650 (epoch 35/50), acc = 0.90, loss = 0.47 (44.4 examples/sec; 2.253 sec/batch), lr: 0.004181\n",
      "2018-11-30 01:23:08.986928: step 10230/14650 (epoch 35/50), acc = 0.84, loss = 0.58 (45.2 examples/sec; 2.214 sec/batch), lr: 0.004181\n",
      "2018-11-30 01:25:01.375245: step 10240/14650 (epoch 35/50), acc = 0.88, loss = 0.48 (53.3 examples/sec; 1.875 sec/batch), lr: 0.004181\n",
      "2018-11-30 01:26:54.324061: step 10250/14650 (epoch 35/50), acc = 0.84, loss = 0.58 (53.3 examples/sec; 1.876 sec/batch), lr: 0.004181\n",
      "2018-11-30 01:28:43.470649: step 10260/14650 (epoch 36/50), acc = 0.83, loss = 0.57 (81.8 examples/sec; 1.222 sec/batch), lr: 0.004181\n",
      "2018-11-30 01:30:34.707547: step 10270/14650 (epoch 36/50), acc = 0.81, loss = 0.71 (45.1 examples/sec; 2.216 sec/batch), lr: 0.004181\n",
      "2018-11-30 01:32:27.212288: step 10280/14650 (epoch 36/50), acc = 0.79, loss = 0.77 (46.1 examples/sec; 2.168 sec/batch), lr: 0.004181\n",
      "2018-11-30 01:34:19.802864: step 10290/14650 (epoch 36/50), acc = 0.79, loss = 0.70 (48.0 examples/sec; 2.083 sec/batch), lr: 0.004181\n",
      "2018-11-30 01:36:09.038948: step 10300/14650 (epoch 36/50), acc = 0.87, loss = 0.60 (46.3 examples/sec; 2.159 sec/batch), lr: 0.004181\n",
      "2018-11-30 01:38:00.897959: step 10310/14650 (epoch 36/50), acc = 0.87, loss = 0.53 (45.2 examples/sec; 2.214 sec/batch), lr: 0.004181\n",
      "2018-11-30 01:39:54.149403: step 10320/14650 (epoch 36/50), acc = 0.88, loss = 0.55 (56.2 examples/sec; 1.781 sec/batch), lr: 0.004181\n",
      "2018-11-30 01:41:47.443499: step 10330/14650 (epoch 36/50), acc = 0.84, loss = 0.52 (53.0 examples/sec; 1.887 sec/batch), lr: 0.004181\n",
      "2018-11-30 01:43:41.323740: step 10340/14650 (epoch 36/50), acc = 0.88, loss = 0.50 (52.6 examples/sec; 1.900 sec/batch), lr: 0.004181\n",
      "2018-11-30 01:45:35.751935: step 10350/14650 (epoch 36/50), acc = 0.86, loss = 0.57 (44.7 examples/sec; 2.236 sec/batch), lr: 0.004181\n",
      "2018-11-30 01:47:30.015262: step 10360/14650 (epoch 36/50), acc = 0.89, loss = 0.50 (48.6 examples/sec; 2.058 sec/batch), lr: 0.004181\n",
      "2018-11-30 01:49:23.904072: step 10370/14650 (epoch 36/50), acc = 0.83, loss = 0.63 (49.6 examples/sec; 2.016 sec/batch), lr: 0.004181\n",
      "2018-11-30 01:51:18.653404: step 10380/14650 (epoch 36/50), acc = 0.84, loss = 0.64 (54.6 examples/sec; 1.831 sec/batch), lr: 0.004181\n",
      "2018-11-30 01:53:13.221095: step 10390/14650 (epoch 36/50), acc = 0.79, loss = 0.59 (54.3 examples/sec; 1.841 sec/batch), lr: 0.004181\n",
      "2018-11-30 01:55:08.565125: step 10400/14650 (epoch 36/50), acc = 0.89, loss = 0.48 (56.8 examples/sec; 1.761 sec/batch), lr: 0.004181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 10400: train_loss = 0.651340, train_accuracy = 0.838\n",
      "Step 10400:  test_loss = 0.809552,  test_accuracy = 0.866\n",
      "\n",
      "2018-11-30 01:57:08.942804: step 10410/14650 (epoch 36/50), acc = 0.84, loss = 0.69 (59.6 examples/sec; 1.678 sec/batch), lr: 0.004181\n",
      "2018-11-30 01:59:04.617150: step 10420/14650 (epoch 36/50), acc = 0.81, loss = 0.80 (55.9 examples/sec; 1.789 sec/batch), lr: 0.004181\n",
      "2018-11-30 02:01:02.971851: step 10430/14650 (epoch 36/50), acc = 0.79, loss = 0.75 (81.9 examples/sec; 1.221 sec/batch), lr: 0.004181\n",
      "2018-11-30 02:02:58.626060: step 10440/14650 (epoch 36/50), acc = 0.79, loss = 0.76 (55.1 examples/sec; 1.816 sec/batch), lr: 0.004181\n",
      "2018-11-30 02:04:53.293196: step 10450/14650 (epoch 36/50), acc = 0.76, loss = 0.99 (57.6 examples/sec; 1.737 sec/batch), lr: 0.004181\n",
      "2018-11-30 02:05:50.705863: step 10455/14650 (epoch 36/50), Learning rate decays to 0.00397\n",
      "2018-11-30 02:06:47.818432: step 10460/14650 (epoch 36/50), acc = 0.90, loss = 0.52 (52.8 examples/sec; 1.895 sec/batch), lr: 0.003972\n",
      "2018-11-30 02:08:42.664535: step 10470/14650 (epoch 36/50), acc = 0.83, loss = 0.81 (55.4 examples/sec; 1.805 sec/batch), lr: 0.003972\n",
      "2018-11-30 02:10:36.970356: step 10480/14650 (epoch 36/50), acc = 0.91, loss = 0.58 (48.1 examples/sec; 2.079 sec/batch), lr: 0.003972\n",
      "2018-11-30 02:12:32.254785: step 10490/14650 (epoch 36/50), acc = 0.87, loss = 0.60 (48.1 examples/sec; 2.081 sec/batch), lr: 0.003972\n",
      "2018-11-30 02:14:27.849634: step 10500/14650 (epoch 36/50), acc = 0.84, loss = 0.67 (55.5 examples/sec; 1.801 sec/batch), lr: 0.003972\n",
      "2018-11-30 02:16:23.403817: step 10510/14650 (epoch 36/50), acc = 0.83, loss = 0.67 (61.4 examples/sec; 1.628 sec/batch), lr: 0.003972\n",
      "2018-11-30 02:18:21.074881: step 10520/14650 (epoch 36/50), acc = 0.86, loss = 0.72 (59.7 examples/sec; 1.675 sec/batch), lr: 0.003972\n",
      "2018-11-30 02:20:19.568392: step 10530/14650 (epoch 36/50), acc = 0.86, loss = 0.62 (67.7 examples/sec; 1.477 sec/batch), lr: 0.003972\n",
      "2018-11-30 02:22:17.251479: step 10540/14650 (epoch 36/50), acc = 0.90, loss = 0.50 (57.7 examples/sec; 1.732 sec/batch), lr: 0.003972\n",
      "2018-11-30 02:24:15.031726: step 10550/14650 (epoch 37/50), acc = 0.85, loss = 0.66 (52.2 examples/sec; 1.914 sec/batch), lr: 0.003972\n",
      "2018-11-30 02:26:12.729619: step 10560/14650 (epoch 37/50), acc = 0.84, loss = 0.64 (47.3 examples/sec; 2.116 sec/batch), lr: 0.003972\n",
      "2018-11-30 02:28:09.808775: step 10570/14650 (epoch 37/50), acc = 0.82, loss = 0.70 (46.3 examples/sec; 2.161 sec/batch), lr: 0.003972\n",
      "2018-11-30 02:30:06.232972: step 10580/14650 (epoch 37/50), acc = 0.80, loss = 0.80 (44.4 examples/sec; 2.252 sec/batch), lr: 0.003972\n",
      "2018-11-30 02:32:03.737096: step 10590/14650 (epoch 37/50), acc = 0.83, loss = 0.68 (50.5 examples/sec; 1.982 sec/batch), lr: 0.003972\n",
      "2018-11-30 02:34:01.785908: step 10600/14650 (epoch 37/50), acc = 0.81, loss = 0.68 (48.7 examples/sec; 2.053 sec/batch), lr: 0.003972\n",
      "\n",
      "Step 10600: train_loss = 0.639742, train_accuracy = 0.846\n",
      "Step 10600:  test_loss = 0.803670,  test_accuracy = 0.860\n",
      "\n",
      "2018-11-30 02:36:04.708582: step 10610/14650 (epoch 37/50), acc = 0.87, loss = 0.60 (56.8 examples/sec; 1.761 sec/batch), lr: 0.003972\n",
      "2018-11-30 02:38:02.032046: step 10620/14650 (epoch 37/50), acc = 0.80, loss = 0.66 (52.5 examples/sec; 1.903 sec/batch), lr: 0.003972\n",
      "2018-11-30 02:40:00.815193: step 10630/14650 (epoch 37/50), acc = 0.87, loss = 0.58 (50.6 examples/sec; 1.975 sec/batch), lr: 0.003972\n",
      "2018-11-30 02:42:00.046708: step 10640/14650 (epoch 37/50), acc = 0.85, loss = 0.58 (56.9 examples/sec; 1.758 sec/batch), lr: 0.003972\n",
      "2018-11-30 02:43:59.320878: step 10650/14650 (epoch 37/50), acc = 0.77, loss = 0.91 (51.4 examples/sec; 1.945 sec/batch), lr: 0.003972\n",
      "2018-11-30 02:45:58.760182: step 10660/14650 (epoch 37/50), acc = 0.86, loss = 0.57 (57.2 examples/sec; 1.748 sec/batch), lr: 0.003972\n",
      "2018-11-30 02:47:57.117030: step 10670/14650 (epoch 37/50), acc = 0.85, loss = 0.60 (53.6 examples/sec; 1.866 sec/batch), lr: 0.003972\n",
      "2018-11-30 02:49:56.406262: step 10680/14650 (epoch 37/50), acc = 0.87, loss = 0.63 (63.0 examples/sec; 1.587 sec/batch), lr: 0.003972\n",
      "2018-11-30 02:51:55.369149: step 10690/14650 (epoch 37/50), acc = 0.80, loss = 0.70 (56.1 examples/sec; 1.782 sec/batch), lr: 0.003972\n",
      "2018-11-30 02:53:55.568598: step 10700/14650 (epoch 37/50), acc = 0.88, loss = 0.76 (53.4 examples/sec; 1.872 sec/batch), lr: 0.003972\n",
      "2018-11-30 02:55:56.263328: step 10710/14650 (epoch 37/50), acc = 0.80, loss = 0.69 (57.5 examples/sec; 1.739 sec/batch), lr: 0.003972\n",
      "2018-11-30 02:57:56.079742: step 10720/14650 (epoch 37/50), acc = 0.89, loss = 0.54 (53.4 examples/sec; 1.874 sec/batch), lr: 0.003972\n",
      "2018-11-30 02:59:56.553685: step 10730/14650 (epoch 37/50), acc = 0.87, loss = 0.54 (60.7 examples/sec; 1.647 sec/batch), lr: 0.003972\n",
      "2018-11-30 03:01:57.222616: step 10740/14650 (epoch 37/50), acc = 0.82, loss = 0.56 (57.4 examples/sec; 1.742 sec/batch), lr: 0.003972\n",
      "2018-11-30 03:03:57.638009: step 10750/14650 (epoch 37/50), acc = 0.82, loss = 0.67 (66.9 examples/sec; 1.495 sec/batch), lr: 0.003972\n",
      "2018-11-30 03:06:00.312251: step 10760/14650 (epoch 37/50), acc = 0.79, loss = 0.91 (65.9 examples/sec; 1.516 sec/batch), lr: 0.003972\n",
      "2018-11-30 03:08:02.627302: step 10770/14650 (epoch 37/50), acc = 0.79, loss = 0.68 (68.0 examples/sec; 1.471 sec/batch), lr: 0.003972\n",
      "2018-11-30 03:10:03.867727: step 10780/14650 (epoch 37/50), acc = 0.84, loss = 0.81 (61.4 examples/sec; 1.628 sec/batch), lr: 0.003972\n",
      "2018-11-30 03:12:05.533457: step 10790/14650 (epoch 37/50), acc = 0.84, loss = 0.54 (64.9 examples/sec; 1.542 sec/batch), lr: 0.003972\n",
      "2018-11-30 03:14:06.987285: step 10800/14650 (epoch 37/50), acc = 0.84, loss = 0.57 (63.4 examples/sec; 1.577 sec/batch), lr: 0.003972\n",
      "\n",
      "Step 10800: train_loss = 0.654450, train_accuracy = 0.841\n",
      "Step 10800:  test_loss = 1.009095,  test_accuracy = 0.862\n",
      "\n",
      "2018-11-30 03:16:13.801986: step 10810/14650 (epoch 37/50), acc = 0.87, loss = 0.53 (67.9 examples/sec; 1.474 sec/batch), lr: 0.003972\n",
      "2018-11-30 03:18:14.712475: step 10820/14650 (epoch 37/50), acc = 0.83, loss = 0.61 (61.1 examples/sec; 1.637 sec/batch), lr: 0.003972\n",
      "2018-11-30 03:20:16.605285: step 10830/14650 (epoch 37/50), acc = 0.88, loss = 0.54 (67.3 examples/sec; 1.485 sec/batch), lr: 0.003972\n",
      "2018-11-30 03:22:17.993032: step 10840/14650 (epoch 37/50), acc = 0.89, loss = 0.45 (62.0 examples/sec; 1.613 sec/batch), lr: 0.003972\n",
      "2018-11-30 03:24:20.679376: step 10850/14650 (epoch 38/50), acc = 0.90, loss = 0.55 (78.7 examples/sec; 1.270 sec/batch), lr: 0.003972\n",
      "2018-11-30 03:26:22.662519: step 10860/14650 (epoch 38/50), acc = 0.80, loss = 0.63 (65.5 examples/sec; 1.526 sec/batch), lr: 0.003972\n",
      "2018-11-30 03:28:25.319891: step 10870/14650 (epoch 38/50), acc = 0.79, loss = 0.96 (72.0 examples/sec; 1.388 sec/batch), lr: 0.003972\n",
      "2018-11-30 03:30:28.157680: step 10880/14650 (epoch 38/50), acc = 0.83, loss = 0.79 (69.4 examples/sec; 1.440 sec/batch), lr: 0.003972\n",
      "2018-11-30 03:32:29.277594: step 10890/14650 (epoch 38/50), acc = 0.93, loss = 0.41 (72.8 examples/sec; 1.374 sec/batch), lr: 0.003972\n",
      "2018-11-30 03:34:31.397882: step 10900/14650 (epoch 38/50), acc = 0.86, loss = 0.60 (64.5 examples/sec; 1.550 sec/batch), lr: 0.003972\n",
      "2018-11-30 03:36:34.426799: step 10910/14650 (epoch 38/50), acc = 0.86, loss = 0.68 (71.3 examples/sec; 1.403 sec/batch), lr: 0.003972\n",
      "2018-11-30 03:38:37.318289: step 10920/14650 (epoch 38/50), acc = 0.83, loss = 0.63 (74.3 examples/sec; 1.346 sec/batch), lr: 0.003972\n",
      "2018-11-30 03:40:42.148671: step 10930/14650 (epoch 38/50), acc = 0.79, loss = 0.81 (75.6 examples/sec; 1.323 sec/batch), lr: 0.003972\n",
      "2018-11-30 03:42:47.058692: step 10940/14650 (epoch 38/50), acc = 0.83, loss = 0.68 (74.5 examples/sec; 1.343 sec/batch), lr: 0.003972\n",
      "2018-11-30 03:44:52.408282: step 10950/14650 (epoch 38/50), acc = 0.81, loss = 0.60 (67.5 examples/sec; 1.482 sec/batch), lr: 0.003972\n",
      "2018-11-30 03:45:55.463200: step 10955/14650 (epoch 38/50), Learning rate decays to 0.00377\n",
      "2018-11-30 03:46:58.064799: step 10960/14650 (epoch 38/50), acc = 0.89, loss = 0.62 (78.5 examples/sec; 1.274 sec/batch), lr: 0.003774\n",
      "2018-11-30 03:49:03.439354: step 10970/14650 (epoch 38/50), acc = 0.86, loss = 0.53 (78.6 examples/sec; 1.272 sec/batch), lr: 0.003774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-30 03:51:09.727649: step 10980/14650 (epoch 38/50), acc = 0.81, loss = 0.72 (78.1 examples/sec; 1.281 sec/batch), lr: 0.003774\n",
      "2018-11-30 03:53:14.842607: step 10990/14650 (epoch 38/50), acc = 0.82, loss = 0.67 (72.8 examples/sec; 1.373 sec/batch), lr: 0.003774\n",
      "2018-11-30 03:55:21.015764: step 11000/14650 (epoch 38/50), acc = 0.91, loss = 0.43 (78.1 examples/sec; 1.280 sec/batch), lr: 0.003774\n",
      "\n",
      "Step 11000: train_loss = 0.642130, train_accuracy = 0.838\n",
      "Step 11000:  test_loss = 0.960140,  test_accuracy = 0.864\n",
      "\n",
      "2018-11-30 03:57:32.094117: step 11010/14650 (epoch 38/50), acc = 0.87, loss = 0.57 (77.5 examples/sec; 1.291 sec/batch), lr: 0.003774\n",
      "2018-11-30 03:59:38.028181: step 11020/14650 (epoch 38/50), acc = 0.86, loss = 0.49 (69.6 examples/sec; 1.436 sec/batch), lr: 0.003774\n",
      "2018-11-30 04:01:43.168212: step 11030/14650 (epoch 38/50), acc = 0.84, loss = 0.64 (63.4 examples/sec; 1.576 sec/batch), lr: 0.003774\n",
      "2018-11-30 04:03:49.081710: step 11040/14650 (epoch 38/50), acc = 0.81, loss = 0.81 (62.9 examples/sec; 1.590 sec/batch), lr: 0.003774\n",
      "2018-11-30 04:05:54.894076: step 11050/14650 (epoch 38/50), acc = 0.85, loss = 0.53 (76.8 examples/sec; 1.303 sec/batch), lr: 0.003774\n",
      "2018-11-30 04:08:00.932018: step 11060/14650 (epoch 38/50), acc = 0.91, loss = 0.53 (62.3 examples/sec; 1.605 sec/batch), lr: 0.003774\n",
      "2018-11-30 04:10:06.546040: step 11070/14650 (epoch 38/50), acc = 0.84, loss = 0.79 (72.4 examples/sec; 1.382 sec/batch), lr: 0.003774\n",
      "2018-11-30 04:12:13.078492: step 11080/14650 (epoch 38/50), acc = 0.77, loss = 0.76 (77.2 examples/sec; 1.295 sec/batch), lr: 0.003774\n",
      "2018-11-30 04:14:19.682119: step 11090/14650 (epoch 38/50), acc = 0.81, loss = 0.69 (77.6 examples/sec; 1.289 sec/batch), lr: 0.003774\n",
      "2018-11-30 04:16:26.820551: step 11100/14650 (epoch 38/50), acc = 0.83, loss = 0.74 (77.2 examples/sec; 1.296 sec/batch), lr: 0.003774\n",
      "2018-11-30 04:18:34.162903: step 11110/14650 (epoch 38/50), acc = 0.89, loss = 0.54 (77.2 examples/sec; 1.296 sec/batch), lr: 0.003774\n",
      "2018-11-30 04:20:41.596887: step 11120/14650 (epoch 38/50), acc = 0.81, loss = 0.79 (77.6 examples/sec; 1.289 sec/batch), lr: 0.003774\n",
      "2018-11-30 04:22:49.806141: step 11130/14650 (epoch 38/50), acc = 0.85, loss = 0.48 (76.7 examples/sec; 1.304 sec/batch), lr: 0.003774\n",
      "2018-11-30 04:24:58.818294: step 11140/14650 (epoch 39/50), acc = 0.85, loss = 0.82 (76.8 examples/sec; 1.302 sec/batch), lr: 0.003774\n",
      "2018-11-30 04:27:08.825341: step 11150/14650 (epoch 39/50), acc = 0.87, loss = 0.76 (75.6 examples/sec; 1.323 sec/batch), lr: 0.003774\n",
      "2018-11-30 04:29:15.968215: step 11160/14650 (epoch 39/50), acc = 0.87, loss = 0.55 (76.9 examples/sec; 1.301 sec/batch), lr: 0.003774\n",
      "2018-11-30 04:31:32.478587: step 11170/14650 (epoch 39/50), acc = 0.82, loss = 0.67 (76.4 examples/sec; 1.309 sec/batch), lr: 0.003774\n",
      "2018-11-30 04:33:52.196873: step 11180/14650 (epoch 39/50), acc = 0.84, loss = 0.59 (41.1 examples/sec; 2.434 sec/batch), lr: 0.003774\n",
      "2018-11-30 04:36:05.646459: step 11190/14650 (epoch 39/50), acc = 0.85, loss = 0.63 (76.6 examples/sec; 1.306 sec/batch), lr: 0.003774\n",
      "2018-11-30 04:38:14.582480: step 11200/14650 (epoch 39/50), acc = 0.82, loss = 0.70 (76.8 examples/sec; 1.302 sec/batch), lr: 0.003774\n",
      "\n",
      "Step 11200: train_loss = 0.609802, train_accuracy = 0.850\n",
      "Step 11200:  test_loss = 0.805422,  test_accuracy = 0.862\n",
      "\n",
      "2018-11-30 04:40:28.633434: step 11210/14650 (epoch 39/50), acc = 0.89, loss = 0.44 (76.7 examples/sec; 1.304 sec/batch), lr: 0.003774\n",
      "2018-11-30 04:42:39.282267: step 11220/14650 (epoch 39/50), acc = 0.82, loss = 0.63 (76.3 examples/sec; 1.311 sec/batch), lr: 0.003774\n",
      "2018-11-30 04:44:48.619538: step 11230/14650 (epoch 39/50), acc = 0.87, loss = 0.52 (76.7 examples/sec; 1.304 sec/batch), lr: 0.003774\n",
      "2018-11-30 04:46:56.633785: step 11240/14650 (epoch 39/50), acc = 0.79, loss = 0.85 (75.8 examples/sec; 1.319 sec/batch), lr: 0.003774\n",
      "2018-11-30 04:49:07.047317: step 11250/14650 (epoch 39/50), acc = 0.85, loss = 0.52 (76.3 examples/sec; 1.310 sec/batch), lr: 0.003774\n",
      "2018-11-30 04:51:17.112623: step 11260/14650 (epoch 39/50), acc = 0.90, loss = 0.55 (76.5 examples/sec; 1.308 sec/batch), lr: 0.003774\n",
      "2018-11-30 04:53:29.839553: step 11270/14650 (epoch 39/50), acc = 0.87, loss = 0.65 (76.5 examples/sec; 1.307 sec/batch), lr: 0.003774\n",
      "2018-11-30 04:55:40.490891: step 11280/14650 (epoch 39/50), acc = 0.84, loss = 0.62 (76.0 examples/sec; 1.315 sec/batch), lr: 0.003774\n",
      "2018-11-30 04:57:55.774999: step 11290/14650 (epoch 39/50), acc = 0.78, loss = 0.80 (74.4 examples/sec; 1.345 sec/batch), lr: 0.003774\n",
      "2018-11-30 05:00:09.231810: step 11300/14650 (epoch 39/50), acc = 0.88, loss = 0.66 (75.4 examples/sec; 1.326 sec/batch), lr: 0.003774\n",
      "2018-11-30 05:02:21.923015: step 11310/14650 (epoch 39/50), acc = 0.84, loss = 0.61 (76.3 examples/sec; 1.311 sec/batch), lr: 0.003774\n",
      "2018-11-30 05:04:36.666261: step 11320/14650 (epoch 39/50), acc = 0.78, loss = 0.79 (76.0 examples/sec; 1.316 sec/batch), lr: 0.003774\n",
      "2018-11-30 05:06:50.705355: step 11330/14650 (epoch 39/50), acc = 0.82, loss = 0.63 (74.5 examples/sec; 1.343 sec/batch), lr: 0.003774\n",
      "2018-11-30 05:09:05.858540: step 11340/14650 (epoch 39/50), acc = 0.83, loss = 0.60 (76.1 examples/sec; 1.315 sec/batch), lr: 0.003774\n",
      "2018-11-30 05:11:19.728233: step 11350/14650 (epoch 39/50), acc = 0.84, loss = 0.67 (76.1 examples/sec; 1.313 sec/batch), lr: 0.003774\n",
      "2018-11-30 05:13:32.469394: step 11360/14650 (epoch 39/50), acc = 0.77, loss = 0.71 (74.5 examples/sec; 1.343 sec/batch), lr: 0.003774\n",
      "2018-11-30 05:15:45.569192: step 11370/14650 (epoch 39/50), acc = 0.78, loss = 0.70 (73.9 examples/sec; 1.353 sec/batch), lr: 0.003774\n",
      "2018-11-30 05:17:58.265796: step 11380/14650 (epoch 39/50), acc = 0.86, loss = 0.59 (74.7 examples/sec; 1.339 sec/batch), lr: 0.003774\n",
      "2018-11-30 05:20:13.218029: step 11390/14650 (epoch 39/50), acc = 0.81, loss = 0.67 (74.0 examples/sec; 1.351 sec/batch), lr: 0.003774\n",
      "2018-11-30 05:22:28.676189: step 11400/14650 (epoch 39/50), acc = 0.84, loss = 0.65 (75.7 examples/sec; 1.322 sec/batch), lr: 0.003774\n",
      "\n",
      "Step 11400: train_loss = 0.621811, train_accuracy = 0.845\n",
      "Step 11400:  test_loss = 0.779718,  test_accuracy = 0.867\n",
      "\n",
      "2018-11-30 05:24:50.718585: step 11410/14650 (epoch 39/50), acc = 0.91, loss = 0.54 (40.4 examples/sec; 2.476 sec/batch), lr: 0.003774\n",
      "2018-11-30 05:27:05.351068: step 11420/14650 (epoch 39/50), acc = 0.82, loss = 0.71 (74.8 examples/sec; 1.337 sec/batch), lr: 0.003774\n",
      "2018-11-30 05:29:21.192425: step 11430/14650 (epoch 40/50), acc = 0.84, loss = 0.67 (75.5 examples/sec; 1.324 sec/batch), lr: 0.003774\n",
      "2018-11-30 05:31:36.820496: step 11440/14650 (epoch 40/50), acc = 0.88, loss = 0.52 (75.5 examples/sec; 1.324 sec/batch), lr: 0.003774\n",
      "2018-11-30 05:33:52.640005: step 11450/14650 (epoch 40/50), acc = 0.77, loss = 0.75 (74.4 examples/sec; 1.344 sec/batch), lr: 0.003774\n",
      "2018-11-30 05:36:08.218937: step 11460/14650 (epoch 40/50), acc = 0.80, loss = 0.70 (74.8 examples/sec; 1.338 sec/batch), lr: 0.003774\n",
      "2018-11-30 05:38:26.381454: step 11470/14650 (epoch 40/50), acc = 0.84, loss = 0.56 (75.3 examples/sec; 1.329 sec/batch), lr: 0.003774\n",
      "2018-11-30 05:40:44.658602: step 11480/14650 (epoch 40/50), acc = 0.80, loss = 0.75 (43.7 examples/sec; 2.290 sec/batch), lr: 0.003774\n",
      "2018-11-30 05:43:03.260948: step 11490/14650 (epoch 40/50), acc = 0.84, loss = 0.66 (43.3 examples/sec; 2.309 sec/batch), lr: 0.003774\n",
      "2018-11-30 05:45:19.903941: step 11500/14650 (epoch 40/50), acc = 0.80, loss = 0.69 (73.8 examples/sec; 1.355 sec/batch), lr: 0.003774\n",
      "2018-11-30 05:47:38.006680: step 11510/14650 (epoch 40/50), acc = 0.87, loss = 0.58 (75.1 examples/sec; 1.331 sec/batch), lr: 0.003774\n",
      "2018-11-30 05:49:56.961650: step 11520/14650 (epoch 40/50), acc = 0.89, loss = 0.46 (41.9 examples/sec; 2.385 sec/batch), lr: 0.003774\n",
      "2018-11-30 05:52:15.823267: step 11530/14650 (epoch 40/50), acc = 0.89, loss = 0.46 (73.0 examples/sec; 1.370 sec/batch), lr: 0.003774\n",
      "2018-11-30 05:54:35.129882: step 11540/14650 (epoch 40/50), acc = 0.88, loss = 0.50 (72.8 examples/sec; 1.373 sec/batch), lr: 0.003774\n",
      "2018-11-30 05:56:52.629135: step 11550/14650 (epoch 40/50), acc = 0.84, loss = 0.59 (74.5 examples/sec; 1.342 sec/batch), lr: 0.003774\n",
      "2018-11-30 05:59:11.718316: step 11560/14650 (epoch 40/50), acc = 0.82, loss = 0.56 (74.1 examples/sec; 1.350 sec/batch), lr: 0.003774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-30 06:01:30.912117: step 11570/14650 (epoch 40/50), acc = 0.81, loss = 0.69 (74.4 examples/sec; 1.344 sec/batch), lr: 0.003774\n",
      "2018-11-30 06:04:03.078942: step 11580/14650 (epoch 40/50), acc = 0.82, loss = 0.77 (43.7 examples/sec; 2.286 sec/batch), lr: 0.003774\n",
      "2018-11-30 06:06:23.791632: step 11590/14650 (epoch 40/50), acc = 0.82, loss = 0.64 (74.2 examples/sec; 1.347 sec/batch), lr: 0.003774\n",
      "2018-11-30 06:08:46.289713: step 11600/14650 (epoch 40/50), acc = 0.84, loss = 0.70 (74.3 examples/sec; 1.345 sec/batch), lr: 0.003774\n",
      "\n",
      "Step 11600: train_loss = 0.627267, train_accuracy = 0.840\n",
      "Step 11600:  test_loss = 0.760367,  test_accuracy = 0.861\n",
      "\n",
      "2018-11-30 06:11:12.927282: step 11610/14650 (epoch 40/50), acc = 0.82, loss = 0.65 (41.3 examples/sec; 2.419 sec/batch), lr: 0.003774\n",
      "2018-11-30 06:13:33.105257: step 11620/14650 (epoch 40/50), acc = 0.83, loss = 0.70 (72.0 examples/sec; 1.389 sec/batch), lr: 0.003774\n",
      "2018-11-30 06:15:52.655878: step 11630/14650 (epoch 40/50), acc = 0.84, loss = 0.51 (73.8 examples/sec; 1.354 sec/batch), lr: 0.003774\n",
      "2018-11-30 06:18:11.884265: step 11640/14650 (epoch 40/50), acc = 0.84, loss = 0.73 (74.0 examples/sec; 1.351 sec/batch), lr: 0.003774\n",
      "2018-11-30 06:20:33.919994: step 11650/14650 (epoch 40/50), acc = 0.85, loss = 0.70 (74.1 examples/sec; 1.349 sec/batch), lr: 0.003774\n",
      "2018-11-30 06:22:55.498146: step 11660/14650 (epoch 40/50), acc = 0.84, loss = 0.59 (39.2 examples/sec; 2.550 sec/batch), lr: 0.003774\n",
      "2018-11-30 06:25:15.168512: step 11670/14650 (epoch 40/50), acc = 0.85, loss = 0.72 (48.3 examples/sec; 2.068 sec/batch), lr: 0.003774\n",
      "2018-11-30 06:27:36.253588: step 11680/14650 (epoch 40/50), acc = 0.87, loss = 0.52 (73.6 examples/sec; 1.359 sec/batch), lr: 0.003774\n",
      "2018-11-30 06:29:58.158250: step 11690/14650 (epoch 40/50), acc = 0.82, loss = 0.69 (72.6 examples/sec; 1.378 sec/batch), lr: 0.003774\n",
      "2018-11-30 06:32:19.802764: step 11700/14650 (epoch 40/50), acc = 0.84, loss = 0.62 (39.8 examples/sec; 2.511 sec/batch), lr: 0.003774\n",
      "2018-11-30 06:34:40.948001: step 11710/14650 (epoch 40/50), acc = 0.86, loss = 0.56 (41.1 examples/sec; 2.433 sec/batch), lr: 0.003774\n",
      "2018-11-30 06:37:02.558361: step 11720/14650 (epoch 40/50), acc = 0.45, loss = 0.38 (39.6 examples/sec; 2.526 sec/batch), lr: 0.003774\n",
      "2018-11-30 06:39:24.724646: step 11730/14650 (epoch 41/50), acc = 0.78, loss = 0.70 (72.3 examples/sec; 1.383 sec/batch), lr: 0.003774\n",
      "2018-11-30 06:41:47.908079: step 11740/14650 (epoch 41/50), acc = 0.86, loss = 0.65 (72.0 examples/sec; 1.388 sec/batch), lr: 0.003774\n",
      "2018-11-30 06:44:12.298341: step 11750/14650 (epoch 41/50), acc = 0.83, loss = 0.59 (45.6 examples/sec; 2.195 sec/batch), lr: 0.003774\n",
      "2018-11-30 06:46:36.233927: step 11760/14650 (epoch 41/50), acc = 0.84, loss = 0.53 (73.5 examples/sec; 1.360 sec/batch), lr: 0.003774\n",
      "2018-11-30 06:49:01.606719: step 11770/14650 (epoch 41/50), acc = 0.81, loss = 0.67 (56.3 examples/sec; 1.777 sec/batch), lr: 0.003774\n",
      "2018-11-30 06:51:27.581350: step 11780/14650 (epoch 41/50), acc = 0.86, loss = 0.52 (71.3 examples/sec; 1.402 sec/batch), lr: 0.003774\n",
      "2018-11-30 06:53:53.625807: step 11790/14650 (epoch 41/50), acc = 0.86, loss = 0.54 (72.9 examples/sec; 1.371 sec/batch), lr: 0.003774\n",
      "2018-11-30 06:56:20.762982: step 11800/14650 (epoch 41/50), acc = 0.87, loss = 0.57 (44.5 examples/sec; 2.247 sec/batch), lr: 0.003774\n",
      "\n",
      "Step 11800: train_loss = 0.602852, train_accuracy = 0.844\n",
      "Step 11800:  test_loss = 0.775188,  test_accuracy = 0.860\n",
      "\n",
      "2018-11-30 06:58:53.693143: step 11810/14650 (epoch 41/50), acc = 0.82, loss = 0.67 (42.3 examples/sec; 2.366 sec/batch), lr: 0.003774\n",
      "2018-11-30 07:01:19.221992: step 11820/14650 (epoch 41/50), acc = 0.87, loss = 0.55 (71.6 examples/sec; 1.397 sec/batch), lr: 0.003774\n",
      "2018-11-30 07:03:47.238712: step 11830/14650 (epoch 41/50), acc = 0.82, loss = 0.67 (72.9 examples/sec; 1.372 sec/batch), lr: 0.003774\n",
      "2018-11-30 07:06:14.778629: step 11840/14650 (epoch 41/50), acc = 0.80, loss = 0.64 (49.4 examples/sec; 2.026 sec/batch), lr: 0.003774\n",
      "2018-11-30 07:08:40.944942: step 11850/14650 (epoch 41/50), acc = 0.85, loss = 0.55 (71.6 examples/sec; 1.396 sec/batch), lr: 0.003774\n",
      "2018-11-30 07:11:05.421079: step 11860/14650 (epoch 41/50), acc = 0.83, loss = 0.73 (40.4 examples/sec; 2.477 sec/batch), lr: 0.003774\n",
      "2018-11-30 07:13:30.747004: step 11870/14650 (epoch 41/50), acc = 0.86, loss = 0.62 (41.8 examples/sec; 2.391 sec/batch), lr: 0.003774\n",
      "2018-11-30 07:15:58.913796: step 11880/14650 (epoch 41/50), acc = 0.84, loss = 0.55 (42.3 examples/sec; 2.365 sec/batch), lr: 0.003774\n",
      "2018-11-30 07:18:27.342792: step 11890/14650 (epoch 41/50), acc = 0.90, loss = 0.60 (45.2 examples/sec; 2.214 sec/batch), lr: 0.003774\n",
      "2018-11-30 07:20:55.687004: step 11900/14650 (epoch 41/50), acc = 0.92, loss = 0.42 (45.4 examples/sec; 2.204 sec/batch), lr: 0.003774\n",
      "2018-11-30 07:23:24.995464: step 11910/14650 (epoch 41/50), acc = 0.79, loss = 0.68 (45.5 examples/sec; 2.200 sec/batch), lr: 0.003774\n",
      "2018-11-30 07:25:53.911023: step 11920/14650 (epoch 41/50), acc = 0.85, loss = 0.58 (52.1 examples/sec; 1.919 sec/batch), lr: 0.003774\n",
      "2018-11-30 07:28:22.850353: step 11930/14650 (epoch 41/50), acc = 0.83, loss = 0.61 (47.0 examples/sec; 2.126 sec/batch), lr: 0.003774\n",
      "2018-11-30 07:30:49.402990: step 11940/14650 (epoch 41/50), acc = 0.87, loss = 0.72 (71.0 examples/sec; 1.408 sec/batch), lr: 0.003774\n",
      "2018-11-30 07:33:17.825902: step 11950/14650 (epoch 41/50), acc = 0.89, loss = 0.54 (71.2 examples/sec; 1.405 sec/batch), lr: 0.003774\n",
      "2018-11-30 07:35:46.709698: step 11960/14650 (epoch 41/50), acc = 0.84, loss = 0.56 (72.3 examples/sec; 1.383 sec/batch), lr: 0.003774\n",
      "2018-11-30 07:38:15.927595: step 11970/14650 (epoch 41/50), acc = 0.84, loss = 0.62 (71.2 examples/sec; 1.405 sec/batch), lr: 0.003774\n",
      "2018-11-30 07:40:45.728939: step 11980/14650 (epoch 41/50), acc = 0.80, loss = 0.67 (72.2 examples/sec; 1.385 sec/batch), lr: 0.003774\n",
      "2018-11-30 07:43:14.167511: step 11990/14650 (epoch 41/50), acc = 0.81, loss = 0.85 (70.1 examples/sec; 1.426 sec/batch), lr: 0.003774\n",
      "2018-11-30 07:43:29.769785: step 11991/14650 (epoch 41/50), Learning rate decays to 0.00358\n",
      "2018-11-30 07:45:42.424472: step 12000/14650 (epoch 41/50), acc = 0.82, loss = 0.58 (69.8 examples/sec; 1.432 sec/batch), lr: 0.003585\n",
      "\n",
      "Step 12000: train_loss = 0.605291, train_accuracy = 0.842\n",
      "Step 12000:  test_loss = 0.764693,  test_accuracy = 0.868\n",
      "\n",
      "2018-11-30 07:48:16.973465: step 12010/14650 (epoch 41/50), acc = 0.83, loss = 0.58 (70.8 examples/sec; 1.413 sec/batch), lr: 0.003585\n",
      "2018-11-30 07:50:47.417757: step 12020/14650 (epoch 42/50), acc = 0.84, loss = 0.55 (71.7 examples/sec; 1.394 sec/batch), lr: 0.003585\n",
      "2018-11-30 07:53:17.152392: step 12030/14650 (epoch 42/50), acc = 0.84, loss = 0.65 (70.8 examples/sec; 1.412 sec/batch), lr: 0.003585\n",
      "2018-11-30 07:55:47.514720: step 12040/14650 (epoch 42/50), acc = 0.84, loss = 0.67 (70.9 examples/sec; 1.410 sec/batch), lr: 0.003585\n",
      "2018-11-30 07:58:16.568440: step 12050/14650 (epoch 42/50), acc = 0.86, loss = 0.62 (72.0 examples/sec; 1.390 sec/batch), lr: 0.003585\n",
      "2018-11-30 08:00:47.500083: step 12060/14650 (epoch 42/50), acc = 0.81, loss = 0.59 (71.1 examples/sec; 1.407 sec/batch), lr: 0.003585\n",
      "2018-11-30 08:03:19.000707: step 12070/14650 (epoch 42/50), acc = 0.83, loss = 0.81 (71.6 examples/sec; 1.397 sec/batch), lr: 0.003585\n",
      "2018-11-30 08:05:49.897657: step 12080/14650 (epoch 42/50), acc = 0.85, loss = 0.61 (71.7 examples/sec; 1.395 sec/batch), lr: 0.003585\n",
      "2018-11-30 08:08:21.873288: step 12090/14650 (epoch 42/50), acc = 0.86, loss = 0.68 (70.6 examples/sec; 1.417 sec/batch), lr: 0.003585\n",
      "2018-11-30 08:10:55.221530: step 12100/14650 (epoch 42/50), acc = 0.83, loss = 0.59 (71.2 examples/sec; 1.405 sec/batch), lr: 0.003585\n",
      "2018-11-30 08:13:28.272468: step 12110/14650 (epoch 42/50), acc = 0.80, loss = 0.68 (70.4 examples/sec; 1.420 sec/batch), lr: 0.003585\n",
      "2018-11-30 08:16:02.124738: step 12120/14650 (epoch 42/50), acc = 0.87, loss = 0.58 (71.3 examples/sec; 1.403 sec/batch), lr: 0.003585\n",
      "2018-11-30 08:18:35.397583: step 12130/14650 (epoch 42/50), acc = 0.80, loss = 0.61 (71.1 examples/sec; 1.406 sec/batch), lr: 0.003585\n",
      "2018-11-30 08:21:08.762593: step 12140/14650 (epoch 42/50), acc = 0.80, loss = 0.88 (71.0 examples/sec; 1.409 sec/batch), lr: 0.003585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-30 08:23:42.896211: step 12150/14650 (epoch 42/50), acc = 0.87, loss = 0.52 (71.1 examples/sec; 1.407 sec/batch), lr: 0.003585\n",
      "2018-11-30 08:26:18.561002: step 12160/14650 (epoch 42/50), acc = 0.80, loss = 0.69 (46.1 examples/sec; 2.168 sec/batch), lr: 0.003585\n",
      "2018-11-30 08:28:52.806669: step 12170/14650 (epoch 42/50), acc = 0.88, loss = 0.51 (71.0 examples/sec; 1.409 sec/batch), lr: 0.003585\n",
      "2018-11-30 08:31:26.057111: step 12180/14650 (epoch 42/50), acc = 0.90, loss = 0.43 (71.4 examples/sec; 1.401 sec/batch), lr: 0.003585\n",
      "2018-11-30 08:34:01.374778: step 12190/14650 (epoch 42/50), acc = 0.88, loss = 0.40 (70.7 examples/sec; 1.414 sec/batch), lr: 0.003585\n",
      "2018-11-30 08:36:39.095175: step 12200/14650 (epoch 42/50), acc = 0.90, loss = 0.56 (70.0 examples/sec; 1.428 sec/batch), lr: 0.003585\n",
      "\n",
      "Step 12200: train_loss = 0.598472, train_accuracy = 0.845\n",
      "Step 12200:  test_loss = 0.760731,  test_accuracy = 0.865\n",
      "\n",
      "2018-11-30 08:39:20.302807: step 12210/14650 (epoch 42/50), acc = 0.85, loss = 0.55 (64.9 examples/sec; 1.541 sec/batch), lr: 0.003585\n",
      "2018-11-30 08:41:54.500811: step 12220/14650 (epoch 42/50), acc = 0.81, loss = 0.69 (59.3 examples/sec; 1.685 sec/batch), lr: 0.003585\n",
      "2018-11-30 08:44:29.291374: step 12230/14650 (epoch 42/50), acc = 0.80, loss = 0.70 (65.2 examples/sec; 1.534 sec/batch), lr: 0.003585\n",
      "2018-11-30 08:47:05.160349: step 12240/14650 (epoch 42/50), acc = 0.83, loss = 0.66 (64.6 examples/sec; 1.547 sec/batch), lr: 0.003585\n",
      "2018-11-30 08:49:41.222484: step 12250/14650 (epoch 42/50), acc = 0.87, loss = 0.67 (69.0 examples/sec; 1.449 sec/batch), lr: 0.003585\n",
      "2018-11-30 08:52:20.912798: step 12260/14650 (epoch 42/50), acc = 0.87, loss = 0.50 (52.2 examples/sec; 1.917 sec/batch), lr: 0.003585\n",
      "2018-11-30 08:55:00.241073: step 12270/14650 (epoch 42/50), acc = 0.85, loss = 0.60 (69.9 examples/sec; 1.432 sec/batch), lr: 0.003585\n",
      "2018-11-30 08:57:43.621762: step 12280/14650 (epoch 42/50), acc = 0.85, loss = 0.58 (38.1 examples/sec; 2.627 sec/batch), lr: 0.003585\n",
      "2018-11-30 09:00:26.880041: step 12290/14650 (epoch 42/50), acc = 0.87, loss = 0.46 (37.7 examples/sec; 2.654 sec/batch), lr: 0.003585\n",
      "2018-11-30 09:03:12.068313: step 12300/14650 (epoch 42/50), acc = 0.85, loss = 0.69 (47.1 examples/sec; 2.123 sec/batch), lr: 0.003585\n",
      "2018-11-30 09:05:57.278891: step 12310/14650 (epoch 43/50), acc = 0.84, loss = 0.58 (43.8 examples/sec; 2.282 sec/batch), lr: 0.003585\n",
      "2018-11-30 09:08:36.034415: step 12320/14650 (epoch 43/50), acc = 0.86, loss = 0.49 (70.4 examples/sec; 1.421 sec/batch), lr: 0.003585\n",
      "2018-11-30 09:11:11.922738: step 12330/14650 (epoch 43/50), acc = 0.90, loss = 0.44 (69.4 examples/sec; 1.440 sec/batch), lr: 0.003585\n",
      "2018-11-30 09:13:45.570097: step 12340/14650 (epoch 43/50), acc = 0.89, loss = 0.57 (71.3 examples/sec; 1.403 sec/batch), lr: 0.003585\n",
      "2018-11-30 09:16:19.577966: step 12350/14650 (epoch 43/50), acc = 0.82, loss = 0.76 (70.1 examples/sec; 1.427 sec/batch), lr: 0.003585\n",
      "2018-11-30 09:18:52.951994: step 12360/14650 (epoch 43/50), acc = 0.83, loss = 0.60 (71.2 examples/sec; 1.404 sec/batch), lr: 0.003585\n",
      "2018-11-30 09:21:25.575384: step 12370/14650 (epoch 43/50), acc = 0.84, loss = 0.64 (71.2 examples/sec; 1.405 sec/batch), lr: 0.003585\n",
      "2018-11-30 09:23:59.961966: step 12380/14650 (epoch 43/50), acc = 0.81, loss = 0.80 (51.8 examples/sec; 1.930 sec/batch), lr: 0.003585\n",
      "2018-11-30 09:26:34.686330: step 12390/14650 (epoch 43/50), acc = 0.86, loss = 0.57 (60.5 examples/sec; 1.654 sec/batch), lr: 0.003585\n",
      "2018-11-30 09:29:08.019443: step 12400/14650 (epoch 43/50), acc = 0.84, loss = 0.61 (58.4 examples/sec; 1.713 sec/batch), lr: 0.003585\n",
      "\n",
      "Step 12400: train_loss = 0.569539, train_accuracy = 0.852\n",
      "Step 12400:  test_loss = 0.822064,  test_accuracy = 0.866\n",
      "\n",
      "2018-11-30 09:31:53.506404: step 12410/14650 (epoch 43/50), acc = 0.82, loss = 0.74 (70.0 examples/sec; 1.429 sec/batch), lr: 0.003585\n",
      "2018-11-30 09:34:29.457598: step 12420/14650 (epoch 43/50), acc = 0.86, loss = 0.44 (71.0 examples/sec; 1.409 sec/batch), lr: 0.003585\n",
      "2018-11-30 09:37:04.177824: step 12430/14650 (epoch 43/50), acc = 0.84, loss = 0.54 (70.2 examples/sec; 1.424 sec/batch), lr: 0.003585\n",
      "2018-11-30 09:39:37.995336: step 12440/14650 (epoch 43/50), acc = 0.90, loss = 0.41 (71.0 examples/sec; 1.409 sec/batch), lr: 0.003585\n",
      "2018-11-30 09:42:13.645907: step 12450/14650 (epoch 43/50), acc = 0.82, loss = 0.65 (69.5 examples/sec; 1.439 sec/batch), lr: 0.003585\n",
      "2018-11-30 09:44:49.996870: step 12460/14650 (epoch 43/50), acc = 0.77, loss = 0.76 (70.6 examples/sec; 1.416 sec/batch), lr: 0.003585\n",
      "2018-11-30 09:47:27.846132: step 12470/14650 (epoch 43/50), acc = 0.79, loss = 0.66 (39.3 examples/sec; 2.542 sec/batch), lr: 0.003585\n",
      "2018-11-30 09:50:04.138341: step 12480/14650 (epoch 43/50), acc = 0.83, loss = 0.74 (70.5 examples/sec; 1.419 sec/batch), lr: 0.003585\n",
      "2018-11-30 09:52:42.343564: step 12490/14650 (epoch 43/50), acc = 0.88, loss = 0.46 (70.7 examples/sec; 1.415 sec/batch), lr: 0.003585\n",
      "2018-11-30 09:55:19.322695: step 12500/14650 (epoch 43/50), acc = 0.85, loss = 0.81 (70.6 examples/sec; 1.417 sec/batch), lr: 0.003585\n",
      "2018-11-30 09:58:00.198844: step 12510/14650 (epoch 43/50), acc = 0.83, loss = 0.52 (70.5 examples/sec; 1.419 sec/batch), lr: 0.003585\n",
      "2018-11-30 10:00:38.838404: step 12520/14650 (epoch 43/50), acc = 0.87, loss = 0.51 (39.0 examples/sec; 2.563 sec/batch), lr: 0.003585\n",
      "2018-11-30 10:03:16.523382: step 12530/14650 (epoch 43/50), acc = 0.87, loss = 0.58 (70.2 examples/sec; 1.425 sec/batch), lr: 0.003585\n",
      "2018-11-30 10:05:55.188500: step 12540/14650 (epoch 43/50), acc = 0.83, loss = 0.66 (70.2 examples/sec; 1.424 sec/batch), lr: 0.003585\n",
      "2018-11-30 10:08:34.658286: step 12550/14650 (epoch 43/50), acc = 0.87, loss = 0.45 (69.9 examples/sec; 1.430 sec/batch), lr: 0.003585\n",
      "2018-11-30 10:11:15.398758: step 12560/14650 (epoch 43/50), acc = 0.86, loss = 0.55 (66.7 examples/sec; 1.498 sec/batch), lr: 0.003585\n",
      "2018-11-30 10:13:56.756316: step 12570/14650 (epoch 43/50), acc = 0.83, loss = 0.60 (37.3 examples/sec; 2.681 sec/batch), lr: 0.003585\n",
      "2018-11-30 10:16:38.887051: step 12580/14650 (epoch 43/50), acc = 0.86, loss = 0.57 (70.1 examples/sec; 1.427 sec/batch), lr: 0.003585\n",
      "2018-11-30 10:19:23.371302: step 12590/14650 (epoch 43/50), acc = 0.81, loss = 0.62 (69.7 examples/sec; 1.434 sec/batch), lr: 0.003585\n",
      "2018-11-30 10:22:03.250591: step 12600/14650 (epoch 44/50), acc = 0.85, loss = 0.54 (64.9 examples/sec; 1.541 sec/batch), lr: 0.003585\n",
      "\n",
      "Step 12600: train_loss = 0.535252, train_accuracy = 0.850\n",
      "Step 12600:  test_loss = 0.779987,  test_accuracy = 0.862\n",
      "\n",
      "2018-11-30 10:24:55.683011: step 12610/14650 (epoch 44/50), acc = 0.92, loss = 0.46 (39.3 examples/sec; 2.546 sec/batch), lr: 0.003585\n",
      "2018-11-30 10:27:48.995280: step 12620/14650 (epoch 44/50), acc = 0.85, loss = 0.52 (68.8 examples/sec; 1.453 sec/batch), lr: 0.003585\n",
      "2018-11-30 10:30:49.571761: step 12630/14650 (epoch 44/50), acc = 0.84, loss = 0.65 (68.6 examples/sec; 1.457 sec/batch), lr: 0.003585\n",
      "2018-11-30 10:33:43.070307: step 12640/14650 (epoch 44/50), acc = 0.88, loss = 0.46 (42.2 examples/sec; 2.368 sec/batch), lr: 0.003585\n",
      "2018-11-30 10:36:31.796900: step 12650/14650 (epoch 44/50), acc = 0.92, loss = 0.37 (38.8 examples/sec; 2.577 sec/batch), lr: 0.003585\n",
      "2018-11-30 10:39:20.086870: step 12660/14650 (epoch 44/50), acc = 0.85, loss = 0.73 (69.6 examples/sec; 1.438 sec/batch), lr: 0.003585\n",
      "2018-11-30 10:42:08.836469: step 12670/14650 (epoch 44/50), acc = 0.83, loss = 0.60 (44.1 examples/sec; 2.269 sec/batch), lr: 0.003585\n",
      "2018-11-30 10:45:01.587613: step 12680/14650 (epoch 44/50), acc = 0.80, loss = 0.71 (48.4 examples/sec; 2.067 sec/batch), lr: 0.003585\n",
      "2018-11-30 10:47:51.781512: step 12690/14650 (epoch 44/50), acc = 0.82, loss = 1.17 (53.0 examples/sec; 1.888 sec/batch), lr: 0.003585\n",
      "2018-11-30 10:50:39.901986: step 12700/14650 (epoch 44/50), acc = 0.83, loss = 0.59 (39.0 examples/sec; 2.567 sec/batch), lr: 0.003585\n",
      "2018-11-30 10:53:27.740705: step 12710/14650 (epoch 44/50), acc = 0.88, loss = 0.62 (38.9 examples/sec; 2.572 sec/batch), lr: 0.003585\n",
      "2018-11-30 10:56:17.070821: step 12720/14650 (epoch 44/50), acc = 0.78, loss = 0.77 (45.6 examples/sec; 2.195 sec/batch), lr: 0.003585\n",
      "2018-11-30 10:59:02.935571: step 12730/14650 (epoch 44/50), acc = 0.89, loss = 0.50 (69.4 examples/sec; 1.441 sec/batch), lr: 0.003585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-30 11:01:51.398290: step 12740/14650 (epoch 44/50), acc = 0.88, loss = 0.51 (40.5 examples/sec; 2.467 sec/batch), lr: 0.003585\n",
      "2018-11-30 11:04:40.185370: step 12750/14650 (epoch 44/50), acc = 0.85, loss = 0.54 (44.0 examples/sec; 2.272 sec/batch), lr: 0.003585\n",
      "2018-11-30 11:07:29.383906: step 12760/14650 (epoch 44/50), acc = 0.86, loss = 0.61 (50.4 examples/sec; 1.984 sec/batch), lr: 0.003585\n",
      "2018-11-30 11:10:19.775767: step 12770/14650 (epoch 44/50), acc = 0.77, loss = 0.77 (59.7 examples/sec; 1.676 sec/batch), lr: 0.003585\n",
      "2018-11-30 11:13:11.285921: step 12780/14650 (epoch 44/50), acc = 0.90, loss = 0.45 (43.3 examples/sec; 2.309 sec/batch), lr: 0.003585\n",
      "2018-11-30 11:16:02.044064: step 12790/14650 (epoch 44/50), acc = 0.80, loss = 0.73 (40.3 examples/sec; 2.484 sec/batch), lr: 0.003585\n",
      "2018-11-30 11:18:55.970280: step 12800/14650 (epoch 44/50), acc = 0.87, loss = 0.63 (52.8 examples/sec; 1.893 sec/batch), lr: 0.003585\n",
      "\n",
      "Step 12800: train_loss = 0.585764, train_accuracy = 0.849\n",
      "Step 12800:  test_loss = 0.685220,  test_accuracy = 0.866\n",
      "\n",
      "2018-11-30 11:21:56.014486: step 12810/14650 (epoch 44/50), acc = 0.82, loss = 0.58 (55.3 examples/sec; 1.808 sec/batch), lr: 0.003585\n",
      "2018-11-30 11:24:51.226788: step 12820/14650 (epoch 44/50), acc = 0.86, loss = 0.60 (65.4 examples/sec; 1.528 sec/batch), lr: 0.003585\n",
      "2018-11-30 11:27:46.858231: step 12830/14650 (epoch 44/50), acc = 0.85, loss = 0.46 (62.0 examples/sec; 1.612 sec/batch), lr: 0.003585\n",
      "2018-11-30 11:30:42.414313: step 12840/14650 (epoch 44/50), acc = 0.89, loss = 0.52 (68.0 examples/sec; 1.471 sec/batch), lr: 0.003585\n",
      "2018-11-30 11:33:39.158586: step 12850/14650 (epoch 44/50), acc = 0.85, loss = 0.62 (67.2 examples/sec; 1.489 sec/batch), lr: 0.003585\n",
      "2018-11-30 11:36:36.354799: step 12860/14650 (epoch 44/50), acc = 0.81, loss = 0.66 (61.3 examples/sec; 1.630 sec/batch), lr: 0.003585\n",
      "2018-11-30 11:39:32.566043: step 12870/14650 (epoch 44/50), acc = 0.89, loss = 0.48 (68.5 examples/sec; 1.461 sec/batch), lr: 0.003585\n",
      "2018-11-30 11:42:30.283363: step 12880/14650 (epoch 44/50), acc = 0.89, loss = 0.58 (54.7 examples/sec; 1.827 sec/batch), lr: 0.003585\n",
      "2018-11-30 11:45:29.805166: step 12890/14650 (epoch 44/50), acc = 0.87, loss = 0.54 (50.9 examples/sec; 1.964 sec/batch), lr: 0.003585\n",
      "2018-11-30 11:48:29.148922: step 12900/14650 (epoch 45/50), acc = 0.89, loss = 0.41 (51.7 examples/sec; 1.935 sec/batch), lr: 0.003585\n",
      "2018-11-30 11:51:28.277441: step 12910/14650 (epoch 45/50), acc = 0.85, loss = 0.57 (55.5 examples/sec; 1.801 sec/batch), lr: 0.003585\n",
      "2018-11-30 11:54:28.608979: step 12920/14650 (epoch 45/50), acc = 0.85, loss = 0.57 (54.4 examples/sec; 1.840 sec/batch), lr: 0.003585\n",
      "2018-11-30 11:57:29.638112: step 12930/14650 (epoch 45/50), acc = 0.86, loss = 0.55 (53.8 examples/sec; 1.857 sec/batch), lr: 0.003585\n",
      "2018-11-30 12:00:28.231842: step 12940/14650 (epoch 45/50), acc = 0.80, loss = 0.69 (48.3 examples/sec; 2.070 sec/batch), lr: 0.003585\n",
      "2018-11-30 12:03:28.151880: step 12950/14650 (epoch 45/50), acc = 0.87, loss = 0.57 (53.2 examples/sec; 1.878 sec/batch), lr: 0.003585\n",
      "2018-11-30 12:06:26.650092: step 12960/14650 (epoch 45/50), acc = 0.84, loss = 0.55 (46.4 examples/sec; 2.157 sec/batch), lr: 0.003585\n",
      "2018-11-30 12:09:27.591702: step 12970/14650 (epoch 45/50), acc = 0.85, loss = 0.51 (67.9 examples/sec; 1.474 sec/batch), lr: 0.003585\n",
      "2018-11-30 12:12:27.730859: step 12980/14650 (epoch 45/50), acc = 0.92, loss = 0.51 (68.0 examples/sec; 1.470 sec/batch), lr: 0.003585\n",
      "2018-11-30 12:15:29.354525: step 12990/14650 (epoch 45/50), acc = 0.85, loss = 0.51 (58.6 examples/sec; 1.707 sec/batch), lr: 0.003585\n",
      "2018-11-30 12:18:31.874267: step 13000/14650 (epoch 45/50), acc = 0.86, loss = 0.52 (67.2 examples/sec; 1.487 sec/batch), lr: 0.003585\n",
      "\n",
      "Step 13000: train_loss = 0.571270, train_accuracy = 0.850\n",
      "Step 13000:  test_loss = 0.845098,  test_accuracy = 0.862\n",
      "\n",
      "2018-11-30 12:21:43.113288: step 13010/14650 (epoch 45/50), acc = 0.83, loss = 0.51 (67.2 examples/sec; 1.487 sec/batch), lr: 0.003585\n",
      "2018-11-30 12:24:44.624605: step 13020/14650 (epoch 45/50), acc = 0.84, loss = 0.67 (64.3 examples/sec; 1.556 sec/batch), lr: 0.003585\n",
      "2018-11-30 12:27:44.743302: step 13030/14650 (epoch 45/50), acc = 0.86, loss = 0.52 (53.9 examples/sec; 1.856 sec/batch), lr: 0.003585\n",
      "2018-11-30 12:30:51.124185: step 13040/14650 (epoch 45/50), acc = 0.80, loss = 0.55 (67.0 examples/sec; 1.492 sec/batch), lr: 0.003585\n",
      "2018-11-30 12:33:55.092567: step 13050/14650 (epoch 45/50), acc = 0.78, loss = 0.70 (67.3 examples/sec; 1.486 sec/batch), lr: 0.003585\n",
      "2018-11-30 12:37:01.121554: step 13060/14650 (epoch 45/50), acc = 0.85, loss = 0.54 (67.3 examples/sec; 1.487 sec/batch), lr: 0.003585\n",
      "2018-11-30 12:40:05.139821: step 13070/14650 (epoch 45/50), acc = 0.86, loss = 0.47 (67.4 examples/sec; 1.484 sec/batch), lr: 0.003585\n",
      "2018-11-30 12:43:11.010093: step 13080/14650 (epoch 45/50), acc = 0.82, loss = 0.59 (67.0 examples/sec; 1.492 sec/batch), lr: 0.003585\n",
      "2018-11-30 12:46:20.425696: step 13090/14650 (epoch 45/50), acc = 0.79, loss = 1.36 (67.0 examples/sec; 1.491 sec/batch), lr: 0.003585\n",
      "2018-11-30 12:49:26.798572: step 13100/14650 (epoch 45/50), acc = 0.82, loss = 0.55 (67.1 examples/sec; 1.491 sec/batch), lr: 0.003585\n",
      "2018-11-30 12:52:33.653735: step 13110/14650 (epoch 45/50), acc = 0.85, loss = 0.55 (67.2 examples/sec; 1.489 sec/batch), lr: 0.003585\n",
      "2018-11-30 12:55:42.751104: step 13120/14650 (epoch 45/50), acc = 0.86, loss = 0.62 (42.0 examples/sec; 2.379 sec/batch), lr: 0.003585\n",
      "2018-11-30 12:58:47.367890: step 13130/14650 (epoch 45/50), acc = 0.84, loss = 0.66 (66.3 examples/sec; 1.508 sec/batch), lr: 0.003585\n",
      "2018-11-30 13:01:52.647716: step 13140/14650 (epoch 45/50), acc = 0.84, loss = 0.73 (66.7 examples/sec; 1.499 sec/batch), lr: 0.003585\n",
      "2018-11-30 13:04:56.122517: step 13150/14650 (epoch 45/50), acc = 0.90, loss = 0.37 (66.9 examples/sec; 1.495 sec/batch), lr: 0.003585\n",
      "2018-11-30 13:07:59.700367: step 13160/14650 (epoch 45/50), acc = 0.83, loss = 0.62 (66.7 examples/sec; 1.500 sec/batch), lr: 0.003585\n",
      "2018-11-30 13:11:08.914637: step 13170/14650 (epoch 45/50), acc = 0.90, loss = 0.40 (66.6 examples/sec; 1.501 sec/batch), lr: 0.003585\n",
      "2018-11-30 13:14:17.454239: step 13180/14650 (epoch 45/50), acc = 0.86, loss = 0.54 (66.5 examples/sec; 1.504 sec/batch), lr: 0.003585\n",
      "2018-11-30 13:17:23.591878: step 13190/14650 (epoch 46/50), acc = 0.89, loss = 0.52 (66.9 examples/sec; 1.494 sec/batch), lr: 0.003585\n",
      "2018-11-30 13:20:27.095031: step 13200/14650 (epoch 46/50), acc = 0.85, loss = 0.61 (66.8 examples/sec; 1.496 sec/batch), lr: 0.003585\n",
      "\n",
      "Step 13200: train_loss = 0.546754, train_accuracy = 0.860\n",
      "Step 13200:  test_loss = 0.763075,  test_accuracy = 0.863\n",
      "\n",
      "2018-11-30 13:23:33.256288: step 13210/14650 (epoch 46/50), acc = 0.87, loss = 0.53 (56.6 examples/sec; 1.767 sec/batch), lr: 0.003585\n",
      "2018-11-30 13:26:31.801102: step 13220/14650 (epoch 46/50), acc = 0.90, loss = 0.45 (61.4 examples/sec; 1.629 sec/batch), lr: 0.003585\n",
      "2018-11-30 13:29:28.772141: step 13230/14650 (epoch 46/50), acc = 0.82, loss = 0.64 (56.9 examples/sec; 1.756 sec/batch), lr: 0.003585\n",
      "2018-11-30 13:32:27.488474: step 13240/14650 (epoch 46/50), acc = 0.82, loss = 0.64 (49.1 examples/sec; 2.036 sec/batch), lr: 0.003585\n",
      "2018-11-30 13:35:26.122587: step 13250/14650 (epoch 46/50), acc = 0.86, loss = 0.53 (52.8 examples/sec; 1.894 sec/batch), lr: 0.003585\n",
      "2018-11-30 13:38:23.671379: step 13260/14650 (epoch 46/50), acc = 0.80, loss = 0.83 (56.6 examples/sec; 1.766 sec/batch), lr: 0.003585\n",
      "2018-11-30 13:41:05.694106: step 13269/14650 (epoch 46/50), Learning rate decays to 0.00341\n",
      "2018-11-30 13:41:23.600989: step 13270/14650 (epoch 46/50), acc = 0.89, loss = 0.40 (47.5 examples/sec; 2.105 sec/batch), lr: 0.003406\n",
      "2018-11-30 13:44:23.308496: step 13280/14650 (epoch 46/50), acc = 0.81, loss = 0.66 (51.4 examples/sec; 1.945 sec/batch), lr: 0.003406\n",
      "2018-11-30 13:47:24.374925: step 13290/14650 (epoch 46/50), acc = 0.82, loss = 0.63 (57.2 examples/sec; 1.749 sec/batch), lr: 0.003406\n",
      "2018-11-30 13:50:24.554958: step 13300/14650 (epoch 46/50), acc = 0.84, loss = 0.69 (47.7 examples/sec; 2.096 sec/batch), lr: 0.003406\n",
      "2018-11-30 13:53:25.229256: step 13310/14650 (epoch 46/50), acc = 0.83, loss = 0.71 (50.7 examples/sec; 1.972 sec/batch), lr: 0.003406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-30 13:56:27.001788: step 13320/14650 (epoch 46/50), acc = 0.82, loss = 0.63 (63.4 examples/sec; 1.577 sec/batch), lr: 0.003406\n",
      "2018-11-30 13:59:28.413458: step 13330/14650 (epoch 46/50), acc = 0.88, loss = 0.63 (55.3 examples/sec; 1.808 sec/batch), lr: 0.003406\n",
      "2018-11-30 14:02:30.240874: step 13340/14650 (epoch 46/50), acc = 0.81, loss = 0.68 (48.0 examples/sec; 2.084 sec/batch), lr: 0.003406\n",
      "2018-11-30 14:05:32.894987: step 13350/14650 (epoch 46/50), acc = 0.83, loss = 0.58 (61.3 examples/sec; 1.630 sec/batch), lr: 0.003406\n",
      "2018-11-30 14:08:36.421647: step 13360/14650 (epoch 46/50), acc = 0.84, loss = 0.79 (63.2 examples/sec; 1.582 sec/batch), lr: 0.003406\n",
      "2018-11-30 14:11:39.377130: step 13370/14650 (epoch 46/50), acc = 0.86, loss = 0.57 (54.7 examples/sec; 1.829 sec/batch), lr: 0.003406\n",
      "2018-11-30 14:14:41.748970: step 13380/14650 (epoch 46/50), acc = 0.80, loss = 0.64 (54.9 examples/sec; 1.821 sec/batch), lr: 0.003406\n",
      "2018-11-30 14:17:44.695981: step 13390/14650 (epoch 46/50), acc = 0.82, loss = 0.66 (50.5 examples/sec; 1.979 sec/batch), lr: 0.003406\n",
      "2018-11-30 14:20:46.394672: step 13400/14650 (epoch 46/50), acc = 0.82, loss = 0.64 (51.7 examples/sec; 1.935 sec/batch), lr: 0.003406\n",
      "\n",
      "Step 13400: train_loss = 0.576216, train_accuracy = 0.849\n",
      "Step 13400:  test_loss = 0.775953,  test_accuracy = 0.863\n",
      "\n",
      "2018-11-30 14:23:56.314225: step 13410/14650 (epoch 46/50), acc = 0.80, loss = 0.76 (66.0 examples/sec; 1.516 sec/batch), lr: 0.003406\n",
      "2018-11-30 14:27:00.425181: step 13420/14650 (epoch 46/50), acc = 0.85, loss = 0.69 (65.9 examples/sec; 1.516 sec/batch), lr: 0.003406\n",
      "2018-11-30 14:30:03.909962: step 13430/14650 (epoch 46/50), acc = 0.79, loss = 0.66 (62.7 examples/sec; 1.594 sec/batch), lr: 0.003406\n",
      "2018-11-30 14:33:08.411285: step 13440/14650 (epoch 46/50), acc = 0.79, loss = 0.73 (66.0 examples/sec; 1.516 sec/batch), lr: 0.003406\n",
      "2018-11-30 14:36:12.409636: step 13450/14650 (epoch 46/50), acc = 0.86, loss = 0.56 (62.7 examples/sec; 1.594 sec/batch), lr: 0.003406\n",
      "2018-11-30 14:39:16.886510: step 13460/14650 (epoch 46/50), acc = 0.87, loss = 0.45 (64.7 examples/sec; 1.546 sec/batch), lr: 0.003406\n",
      "2018-11-30 14:42:22.328301: step 13470/14650 (epoch 46/50), acc = 0.84, loss = 0.52 (54.0 examples/sec; 1.852 sec/batch), lr: 0.003406\n"
     ]
    }
   ],
   "source": [
    "from cnndetector import train\n",
    "import os\n",
    "cr_dir = os.path.abspath(os.path.dirname(\"./\"))\n",
    "data_dir = os.path.join(cr_dir, 'data', 'ted500')\n",
    "train(data_dir=data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
