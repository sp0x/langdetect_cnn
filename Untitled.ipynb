{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:\n",
      "           train_dir 'models'\n",
      "            data_dir 'C:\\\\Users\\\\cyb3r\\\\Ml\\\\Untitled Folder\\\\data\\\\ted500'\n",
      "          batch_size 100\n",
      "           num_epoch 1\n",
      "        use_pretrain False\n",
      "          vocab_size 4090\n",
      "             init_lr 0.01\n",
      "        summary_step 200\n",
      "     checkpoint_step 200\n",
      "            log_step 10\n",
      "      tolerance_step 500\n",
      "            lr_decay 0.95\n",
      "            emb_size 300\n",
      "          num_kernel 100\n",
      "          min_window 3\n",
      "          max_window 5\n",
      "            sent_len 257\n",
      "              l2_reg 1e-05\n",
      "           optimizer 'adam'\n",
      "             dropout 0.5\n",
      "Preparing train data ...\n",
      "Loaded target classes (length 65).\n",
      "Loaded data with 29250 examples. 100 examples per batch will be used.\n",
      "Preparing test data ...\n",
      "Loaded target classes (length 65).\n",
      "Loaded data with 3250 examples. 100 examples per batch will be used.\n",
      "WARNING:tensorflow:From C:\\Users\\cyb3r\\Ml\\Untitled Folder\\cnn.py:113: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "\n",
      "Start training, 293 batches needed, with 100 examples per batch.\n",
      "2018-11-28 09:04:21.250384: step 10/293 (epoch 1/1), acc = 0.30, loss = 3.50 (114.2 examples/sec; 0.876 sec/batch), lr: 0.010000\n",
      "2018-11-28 09:04:30.610383: step 20/293 (epoch 1/1), acc = 0.44, loss = 2.26 (111.9 examples/sec; 0.894 sec/batch), lr: 0.010000\n",
      "2018-11-28 09:04:40.786384: step 30/293 (epoch 1/1), acc = 0.62, loss = 1.44 (100.9 examples/sec; 0.991 sec/batch), lr: 0.010000\n",
      "2018-11-28 09:04:50.699383: step 40/293 (epoch 1/1), acc = 0.66, loss = 1.20 (110.4 examples/sec; 0.906 sec/batch), lr: 0.010000\n",
      "2018-11-28 09:05:01.527385: step 50/293 (epoch 1/1), acc = 0.60, loss = 1.55 (95.7 examples/sec; 1.045 sec/batch), lr: 0.010000\n",
      "2018-11-28 09:05:10.591383: step 60/293 (epoch 1/1), acc = 0.68, loss = 1.09 (130.4 examples/sec; 0.767 sec/batch), lr: 0.010000\n",
      "2018-11-28 09:05:18.564382: step 70/293 (epoch 1/1), acc = 0.72, loss = 0.98 (123.9 examples/sec; 0.807 sec/batch), lr: 0.010000\n",
      "2018-11-28 09:05:26.457384: step 80/293 (epoch 1/1), acc = 0.69, loss = 0.98 (127.4 examples/sec; 0.785 sec/batch), lr: 0.010000\n",
      "2018-11-28 09:05:34.637382: step 90/293 (epoch 1/1), acc = 0.76, loss = 0.96 (129.9 examples/sec; 0.770 sec/batch), lr: 0.010000\n",
      "2018-11-28 09:05:42.894383: step 100/293 (epoch 1/1), acc = 0.61, loss = 1.30 (126.4 examples/sec; 0.791 sec/batch), lr: 0.010000\n",
      "2018-11-28 09:05:52.435383: step 110/293 (epoch 1/1), acc = 0.71, loss = 1.01 (98.9 examples/sec; 1.011 sec/batch), lr: 0.010000\n",
      "2018-11-28 09:06:01.896383: step 120/293 (epoch 1/1), acc = 0.76, loss = 0.82 (114.4 examples/sec; 0.874 sec/batch), lr: 0.010000\n",
      "2018-11-28 09:06:10.093383: step 130/293 (epoch 1/1), acc = 0.68, loss = 1.09 (128.4 examples/sec; 0.779 sec/batch), lr: 0.010000\n",
      "2018-11-28 09:06:18.411384: step 140/293 (epoch 1/1), acc = 0.70, loss = 1.24 (124.4 examples/sec; 0.804 sec/batch), lr: 0.010000\n",
      "2018-11-28 09:06:26.680383: step 150/293 (epoch 1/1), acc = 0.76, loss = 0.88 (122.7 examples/sec; 0.815 sec/batch), lr: 0.010000\n",
      "2018-11-28 09:06:35.798385: step 160/293 (epoch 1/1), acc = 0.74, loss = 0.84 (125.6 examples/sec; 0.796 sec/batch), lr: 0.010000\n",
      "2018-11-28 09:06:44.736385: step 170/293 (epoch 1/1), acc = 0.66, loss = 1.19 (113.1 examples/sec; 0.884 sec/batch), lr: 0.010000\n"
     ]
    }
   ],
   "source": [
    "from cnndetector import train\n",
    "import os\n",
    "cr_dir = os.path.abspath(os.path.dirname(\"./\"))\n",
    "data_dir = os.path.join(cr_dir, 'data', 'ted500')\n",
    "train(data_dir=data_dir, num_epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
